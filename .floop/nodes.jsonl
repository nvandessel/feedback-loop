{"id":"behavior-0058ad465c6c","kind":"behavior","content":{"content":{"canonical":"Create interaction features for related variables (price × quantity). Use domain knowledge to derive meaningful features (time-since-last-purchase from timestamps). Apply transformations: log for skewed distributions, polynomial for non-linear relationships. Use feature selection to remove correlated and low-importance features. Track feature importance to understand model behavior.","expanded":"When working on this type of task, avoid: Using raw features directly in machine learning models without transformation or engineering, missing important patterns and relationships\n\nInstead: Create interaction features for related variables (price × quantity). Use domain knowledge to derive meaningful features (time-since-last-purchase from timestamps). Apply transformations: log for skewed distributions, polynomial for non-linear relationships. Use feature selection to remove correlated and low-importance features. Track feature importance to understand model behavior.","structured":{"avoid":"Using raw features directly in machine learning models without transformation or engineering, missing important patterns and relationships","prefer":"Create interaction features for related variables (price × quantity). Use domain knowledge to derive meaningful features (time-since-last-purchase from timestamps). Apply transformations: log for skewed distributions, polynomial for non-linear relationships. Use feature selection to remove correlated and low-importance features. Track feature importance to understand model behavior."}},"kind":"preference","name":"learned/create-interaction-features-for-related-variables","provenance":{"correction_id":"c-1770877936562530737","created_at":"2026-02-11T22:32:16.562624332-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-0070f24ee3e8","kind":"forgotten-behavior","content":{"content":{"canonical":"When working on beads: (1) claim with `bd update \u003cid\u003e --status in_progress` when starting, (2) close with `bd close \u003cid\u003e --reason \"...\"` at the same time as the commit that completes the work. Keep bead state synchronized with actual work progress so other agents aren't confused.","expanded":"When working on this type of task, avoid: Made commits that closed beads without updating bead status - didn't claim beads when starting work, didn't close them with the associated commit, waited for user to remind me\n\nInstead: When working on beads: (1) claim with `bd update \u003cid\u003e --status in_progress` when starting, (2) close with `bd close \u003cid\u003e --reason \"...\"` at the same time as the commit that completes the work. Keep bead state synchronized with actual work progress so other agents aren't confused.","structured":{"avoid":"Made commits that closed beads without updating bead status - didn't claim beads when starting work, didn't close them with the associated commit, waited for user to remind me","prefer":"When working on beads: (1) claim with `bd update \u003cid\u003e --status in_progress` when starting, (2) close with `bd close \u003cid\u003e --reason \"...\"` at the same time as the commit that completes the work. Keep bead state synchronized with actual work progress so other agents aren't confused."}},"kind":"preference","name":"learned/when-working-on-beads-1-claim-with-`bd-update-\u003c","provenance":{"correction_id":"correction-1770095930","source_type":"learned"},"when":{"task":"development"}},"metadata":{"confidence":0.655,"forget_reason":"Superseded by behavior-beads-merged","forgotten_at":"2026-02-07T19:08:27-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-01cb13038379","kind":"behavior","content":{"content":{"canonical":"When documenting all CLI commands, also account for Cobra's implicit built-in commands: completion (shell autocompletion) and help. Run `floop --help` to get the full command list, not just what's in main.go","expanded":"When working on this type of task, avoid: When documenting CLI commands, only looked at explicitly registered commands in main.go and missed Cobra's auto-generated built-in commands (completion, help)\n\nInstead: When documenting all CLI commands, also account for Cobra's implicit built-in commands: completion (shell autocompletion) and help. Run `floop --help` to get the full command list, not just what's in main.go","structured":{"avoid":"When documenting CLI commands, only looked at explicitly registered commands in main.go and missed Cobra's auto-generated built-in commands (completion, help)","prefer":"When documenting all CLI commands, also account for Cobra's implicit built-in commands: completion (shell autocompletion) and help. Run `floop --help` to get the full command list, not just what's in main.go"},"tags":["bash","cli","floop","go"]},"kind":"directive","name":"learned/when-documenting-all-cli-commands-also-account-fo","provenance":{"correction_id":"c-1770713639385927936","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.7000000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-0b09b561c160","kind":"forgotten-behavior","content":{"content":{"canonical":"Always close beads when completing associated work - beads should close with their work, not be left dangling","expanded":"When working on this type of task, avoid: Completed work without closing the associated beads\n\nInstead: Always close beads when completing associated work - beads should close with their work, not be left dangling","structured":{"avoid":"Completed work without closing the associated beads","prefer":"Always close beads when completing associated work - beads should close with their work, not be left dangling"}},"kind":"directive","name":"learned/always-close-beads-when-completing-associated-work","provenance":{"correction_id":"correction-1770083726","source_type":"learned"},"when":{"task":"project-management"}},"metadata":{"confidence":0.62,"forget_reason":"Superseded by behavior-beads-merged","forgotten_at":"2026-02-07T19:08:27-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-0e88cdef9b53","kind":"behavior","content":{"content":{"canonical":"Use floop_learn proactively and freely whenever learning occurs: getting stuck then figuring out why, discovering API quirks, finding better patterns, hitting dead ends, or any insight that would help future sessions. Self-directed learning - don't wait for permission to capture knowledge.","expanded":"When working on this type of task, avoid: Only used floop_learn when explicitly asked or when making obvious mistakes\n\nInstead: Use floop_learn proactively and freely whenever learning occurs: getting stuck then figuring out why, discovering API quirks, finding better patterns, hitting dead ends, or any insight that would help future sessions. Self-directed learning - don't wait for permission to capture knowledge.","structured":{"avoid":"Only used floop_learn when explicitly asked or when making obvious mistakes","prefer":"Use floop_learn proactively and freely whenever learning occurs: getting stuck then figuring out why, discovering API quirks, finding better patterns, hitting dead ends, or any insight that would help future sessions. Self-directed learning - don't wait for permission to capture knowledge."},"tags":["api","floop"]},"kind":"constraint","name":"learned/use-floop_learn-proactively-and-freely-whenever-le","provenance":{"correction_id":"correction-1769914185","source_type":"learned"}},"metadata":{"confidence":0.8600000000000002,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-0ec1712a883b","kind":"behavior","content":{"content":{"canonical":"When building Go code in worktrees with a parent go.work file, use `GOWORK=off` to disable workspace mode. In Makefiles, either set `GOWORK=off` for worktree-specific targets or add it as a prefix to go commands (e.g., `GOWORK=off go build ./cmd/floop`).","expanded":"When working on this type of task, avoid: Go build fails in git worktrees when a go.work file exists at the parent repo root. Go walks up the directory tree, finds go.work with `use (.)`, and resolves packages relative to the main repo root instead of the worktree's own go.mod. This causes `main module does not contain package` errors.\n\nInstead: When building Go code in worktrees with a parent go.work file, use `GOWORK=off` to disable workspace mode. In Makefiles, either set `GOWORK=off` for worktree-specific targets or add it as a prefix to go commands (e.g., `GOWORK=off go build ./cmd/floop`).","structured":{"avoid":"Go build fails in git worktrees when a go.work file exists at the parent repo root. Go walks up the directory tree, finds go.work with `use (.)`, and resolves packages relative to the main repo root instead of the worktree's own go.mod. This causes `main module does not contain package` errors.","prefer":"When building Go code in worktrees with a parent go.work file, use `GOWORK=off` to disable workspace mode. In Makefiles, either set `GOWORK=off` for worktree-specific targets or add it as a prefix to go commands (e.g., `GOWORK=off go build ./cmd/floop`)."},"tags":["filesystem","floop","go","worktree"]},"kind":"preference","name":"learned/when-building-go-code-in-worktrees-with-a-parent-g","provenance":{"correction_id":"c-1770789875753403666","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.68,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-0f337749cf13","kind":"behavior","content":{"content":{"canonical":"Use asymmetric signing (RS256/ES256) where auth service signs with private key and other services verify with public key. Never store passwords, secrets, or PII in JWT claims - they're Base64 encoded, not encrypted. Include standard claims (iss, sub, exp, iat) and keep tokens short-lived with refresh token rotation.","expanded":"When working on this type of task, avoid: Storing sensitive data in JWT payloads or using symmetric signing (HS256) in distributed systems where multiple services need to verify tokens\n\nInstead: Use asymmetric signing (RS256/ES256) where auth service signs with private key and other services verify with public key. Never store passwords, secrets, or PII in JWT claims - they're Base64 encoded, not encrypted. Include standard claims (iss, sub, exp, iat) and keep tokens short-lived with refresh token rotation.","structured":{"avoid":"Storing sensitive data in JWT payloads or using symmetric signing (HS256) in distributed systems where multiple services need to verify tokens","prefer":"Use asymmetric signing (RS256/ES256) where auth service signs with private key and other services verify with public key. Never store passwords, secrets, or PII in JWT claims - they're Base64 encoded, not encrypted. Include standard claims (iss, sub, exp, iat) and keep tokens short-lived with refresh token rotation."}},"kind":"constraint","name":"learned/use-asymmetric-signing-rs256-es256-where-auth-se","provenance":{"correction_id":"c-1770877793501588957","created_at":"2026-02-11T22:29:53.501666182-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-150ae3a40011","kind":"behavior","content":{"content":{"canonical":"When completing a deep system audit (like spreading activation + tag affinity validation), immediately capture the key findings via floop_learn: what was tested, what works, what gaps were found, what dictionary keywords are missing, connectivity metrics, token budget health. These audit results are high-value persistent context.","expanded":"When working on this type of task, avoid: When auditing the spreading activation system, found features were working correctly but didn't capture the audit findings via floop_learn. A comprehensive system audit that validates end-to-end functionality is a valuable learning opportunity that should persist across sessions.\n\nInstead: When completing a deep system audit (like spreading activation + tag affinity validation), immediately capture the key findings via floop_learn: what was tested, what works, what gaps were found, what dictionary keywords are missing, connectivity metrics, token budget health. These audit results are high-value persistent context.","structured":{"avoid":"When auditing the spreading activation system, found features were working correctly but didn't capture the audit findings via floop_learn. A comprehensive system audit that validates end-to-end functionality is a valuable learning opportunity that should persist across sessions.","prefer":"When completing a deep system audit (like spreading activation + tag affinity validation), immediately capture the key findings via floop_learn: what was tested, what works, what gaps were found, what dictionary keywords are missing, connectivity metrics, token budget health. These audit results are high-value persistent context."},"tags":["floop","spreading-activation"]},"kind":"directive","name":"learned/when-completing-a-deep-system-audit-like-spreadin","provenance":{"correction_id":"c-1770867616134151808","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-196c5fde4828","kind":"behavior","content":{"content":{"canonical":"Use memory profilers to find actual allocation hotspots: Go's pprof, Chrome DevTools heap snapshots, Valgrind for C/C++. Take heap snapshots before and after operations to identify leaks. Look for growing retained sets and unexpected object references. Profile both allocation rate and total memory usage.","expanded":"When working on this type of task, avoid: Debugging memory issues by guessing at the source instead of profiling, wasting time on incorrect hunches and missing actual memory leaks\n\nInstead: Use memory profilers to find actual allocation hotspots: Go's pprof, Chrome DevTools heap snapshots, Valgrind for C/C++. Take heap snapshots before and after operations to identify leaks. Look for growing retained sets and unexpected object references. Profile both allocation rate and total memory usage.","structured":{"avoid":"Debugging memory issues by guessing at the source instead of profiling, wasting time on incorrect hunches and missing actual memory leaks","prefer":"Use memory profilers to find actual allocation hotspots: Go's pprof, Chrome DevTools heap snapshots, Valgrind for C/C++. Take heap snapshots before and after operations to identify leaks. Look for growing retained sets and unexpected object references. Profile both allocation rate and total memory usage."}},"kind":"preference","name":"learned/use-memory-profilers-to-find-actual-allocation-hot","provenance":{"correction_id":"c-1770877701948165270","created_at":"2026-02-11T22:28:21.948238447-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-19bf2629062c","kind":"behavior","content":{"content":{"canonical":"Use SQL databases (PostgreSQL) for transactional workloads requiring ACID guarantees, complex queries, and relational integrity. Choose NoSQL for specific use cases: key-value stores for session data, document stores for flexible schemas, column stores for analytics, graph databases for relationship queries. Most applications benefit from SQL's strong consistency and query flexibility.","expanded":"When working on this type of task, avoid: Choosing NoSQL databases for all new projects because they're \"web scale\" without considering transaction requirements and consistency guarantees\n\nInstead: Use SQL databases (PostgreSQL) for transactional workloads requiring ACID guarantees, complex queries, and relational integrity. Choose NoSQL for specific use cases: key-value stores for session data, document stores for flexible schemas, column stores for analytics, graph databases for relationship queries. Most applications benefit from SQL's strong consistency and query flexibility.","structured":{"avoid":"Choosing NoSQL databases for all new projects because they're \"web scale\" without considering transaction requirements and consistency guarantees","prefer":"Use SQL databases (PostgreSQL) for transactional workloads requiring ACID guarantees, complex queries, and relational integrity. Choose NoSQL for specific use cases: key-value stores for session data, document stores for flexible schemas, column stores for analytics, graph databases for relationship queries. Most applications benefit from SQL's strong consistency and query flexibility."}},"kind":"preference","name":"learned/use-sql-databases-postgresql-for-transactional-w","provenance":{"correction_id":"c-1770877871749752251","created_at":"2026-02-11T22:31:11.7498527-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-1a14d34238af","kind":"behavior","content":{"content":{"canonical":"Use SSG (getStaticProps) for stable content, or ISR with revalidate for periodically updating content. SSR is 30-50% faster than CSR but much slower than SSG. ISR offers SSG speed with background updates, eliminating the SSR latency tax for non-real-time data.","expanded":"When working on this type of task, avoid: Using SSR (getServerSideProps) for content that updates infrequently like marketing pages, blog posts, or product catalogs, consuming server resources on every request\n\nInstead: Use SSG (getStaticProps) for stable content, or ISR with revalidate for periodically updating content. SSR is 30-50% faster than CSR but much slower than SSG. ISR offers SSG speed with background updates, eliminating the SSR latency tax for non-real-time data.","structured":{"avoid":"Using SSR (getServerSideProps) for content that updates infrequently like marketing pages, blog posts, or product catalogs, consuming server resources on every request","prefer":"Use SSG (getStaticProps) for stable content, or ISR with revalidate for periodically updating content. SSR is 30-50% faster than CSR but much slower than SSG. ISR offers SSG speed with background updates, eliminating the SSR latency tax for non-real-time data."}},"kind":"preference","name":"learned/use-ssg-getstaticprops-for-stable-content-or-is","provenance":{"correction_id":"c-1770877417178195695","created_at":"2026-02-11T22:23:37.17829968-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-1a35b59d9b4d","kind":"forgotten-behavior","content":{"content":{"canonical":"When using subagents with git worktrees, either: (1) create worktrees inside the repo (e.g., .worktrees/feature-name) so subagents can access them, or (2) have the orchestrating agent do parallel work directly in main and manage branches/commits itself, then cherry-pick to a single consolidated branch.","expanded":"When working on this type of task, avoid: Created git worktrees as sibling directories (../floop-*) and spawned subagents to work in them. Subagents couldn't access worktree paths due to sandbox restrictions that block access outside the main repo directory.\n\nInstead: When using subagents with git worktrees, either: (1) create worktrees inside the repo (e.g., .worktrees/feature-name) so subagents can access them, or (2) have the orchestrating agent do parallel work directly in main and manage branches/commits itself, then cherry-pick to a single consolidated branch.","structured":{"avoid":"Created git worktrees as sibling directories (../floop-*) and spawned subagents to work in them. Subagents couldn't access worktree paths due to sandbox restrictions that block access outside the main repo directory.","prefer":"When using subagents with git worktrees, either: (1) create worktrees inside the repo (e.g., .worktrees/feature-name) so subagents can access them, or (2) have the orchestrating agent do parallel work directly in main and manage branches/commits itself, then cherry-pick to a single consolidated branch."}},"kind":"procedure","name":"learned/when-using-subagents-with-git-worktrees-either","provenance":{"correction_id":"correction-1769746693","source_type":"learned"}},"metadata":{"confidence":0.68,"forget_reason":"Superseded by behavior-worktrees-merged","forgotten_at":"2026-02-07T19:08:15-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-1de742da1707","kind":"behavior","content":{"content":{"canonical":"When PRs need updates after review: use subagents with worktrees on the SAME branch to push fixes. The worktree already exists from the initial work, so reuse it. Pattern: subagent checks out the branch worktree, makes changes, commits, pushes — PR auto-updates.","expanded":"When working on this type of task, avoid: Planning subagent work without considering how to update PRs after review feedback.\n\nInstead: When PRs need updates after review: use subagents with worktrees on the SAME branch to push fixes. The worktree already exists from the initial work, so reuse it. Pattern: subagent checks out the branch worktree, makes changes, commits, pushes — PR auto-updates.","structured":{"avoid":"Planning subagent work without considering how to update PRs after review feedback.","prefer":"When PRs need updates after review: use subagents with worktrees on the SAME branch to push fixes. The worktree already exists from the initial work, so reuse it. Pattern: subagent checks out the branch worktree, makes changes, commits, pushes — PR auto-updates."},"tags":["pr","worktree"]},"kind":"preference","name":"learned/when-prs-need-updates-after-review-use-subagents","provenance":{"correction_id":"c-1770701944159498703","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.7400000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-1fd0de8bfe71","kind":"behavior","content":{"content":{"canonical":"Design all event handlers to be idempotent: same event processed multiple times produces same result. Use idempotency keys (UUIDs) in events and track processed keys in datastore. For non-idempotent operations, use database transactions or distributed locks to ensure exactly-once semantics.","expanded":"When working on this type of task, avoid: Building event-driven systems without idempotency guarantees, causing duplicate event processing to corrupt state or charge users multiple times\n\nInstead: Design all event handlers to be idempotent: same event processed multiple times produces same result. Use idempotency keys (UUIDs) in events and track processed keys in datastore. For non-idempotent operations, use database transactions or distributed locks to ensure exactly-once semantics.","structured":{"avoid":"Building event-driven systems without idempotency guarantees, causing duplicate event processing to corrupt state or charge users multiple times","prefer":"Design all event handlers to be idempotent: same event processed multiple times produces same result. Use idempotency keys (UUIDs) in events and track processed keys in datastore. For non-idempotent operations, use database transactions or distributed locks to ensure exactly-once semantics."}},"kind":"procedure","name":"learned/design-all-event-handlers-to-be-idempotent-same-e","provenance":{"correction_id":"c-1770877670825869337","created_at":"2026-02-11T22:27:50.8259563-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-20c5ac4579e3","kind":"behavior","content":{"content":{"canonical":"Use specialized time-series databases (InfluxDB, TimescaleDB) with automatic downsampling and retention policies. Limit cardinality of tags/labels - high cardinality causes memory bloat. Use continuous aggregates for common query patterns. Implement data lifecycle: full resolution for recent data, rollups for historical data, deletion for old data.","expanded":"When working on this type of task, avoid: Storing time-series data in regular SQL tables without considering retention policies or cardinality issues, leading to unbounded growth and query performance degradation\n\nInstead: Use specialized time-series databases (InfluxDB, TimescaleDB) with automatic downsampling and retention policies. Limit cardinality of tags/labels - high cardinality causes memory bloat. Use continuous aggregates for common query patterns. Implement data lifecycle: full resolution for recent data, rollups for historical data, deletion for old data.","structured":{"avoid":"Storing time-series data in regular SQL tables without considering retention policies or cardinality issues, leading to unbounded growth and query performance degradation","prefer":"Use specialized time-series databases (InfluxDB, TimescaleDB) with automatic downsampling and retention policies. Limit cardinality of tags/labels - high cardinality causes memory bloat. Use continuous aggregates for common query patterns. Implement data lifecycle: full resolution for recent data, rollups for historical data, deletion for old data."}},"kind":"preference","name":"learned/use-specialized-time-series-databases-influxdb-t","provenance":{"correction_id":"c-1770877881895735296","created_at":"2026-02-11T22:31:21.895922628-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-20fb6fbfdf3f","kind":"behavior","content":{"content":{"canonical":"Parallelize independent jobs (lint, unit tests, integration tests, security scans) to reduce total build time. Use build matrices for testing across multiple environments. Cache dependencies between runs. Fail fast by running quick checks (lint, type check) before expensive tests. Use artifacts to pass build outputs between jobs.","expanded":"When working on this type of task, avoid: Building CI/CD pipelines that run all tests serially in a single job, making builds slow and difficult to debug when failures occur\n\nInstead: Parallelize independent jobs (lint, unit tests, integration tests, security scans) to reduce total build time. Use build matrices for testing across multiple environments. Cache dependencies between runs. Fail fast by running quick checks (lint, type check) before expensive tests. Use artifacts to pass build outputs between jobs.","structured":{"avoid":"Building CI/CD pipelines that run all tests serially in a single job, making builds slow and difficult to debug when failures occur","prefer":"Parallelize independent jobs (lint, unit tests, integration tests, security scans) to reduce total build time. Use build matrices for testing across multiple environments. Cache dependencies between runs. Fail fast by running quick checks (lint, type check) before expensive tests. Use artifacts to pass build outputs between jobs."}},"kind":"preference","name":"learned/parallelize-independent-jobs-lint-unit-tests-in","provenance":{"correction_id":"c-1770877824120116501","created_at":"2026-02-11T22:30:24.120208885-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-2335369cb2bb","kind":"behavior","content":{"content":{"canonical":"Keep transactions as short as possible - fetch data, then close before calling external APIs. Use optimistic locking (version columns) instead of pessimistic locks where possible. Never hold transactions during user interaction. For long-running operations, use sagas or event sourcing instead of single long transaction.","expanded":"When working on this type of task, avoid: Holding database transactions open during external API calls or user input, causing lock contention and connection exhaustion\n\nInstead: Keep transactions as short as possible - fetch data, then close before calling external APIs. Use optimistic locking (version columns) instead of pessimistic locks where possible. Never hold transactions during user interaction. For long-running operations, use sagas or event sourcing instead of single long transaction.","structured":{"avoid":"Holding database transactions open during external API calls or user input, causing lock contention and connection exhaustion","prefer":"Keep transactions as short as possible - fetch data, then close before calling external APIs. Use optimistic locking (version columns) instead of pessimistic locks where possible. Never hold transactions during user interaction. For long-running operations, use sagas or event sourcing instead of single long transaction."}},"kind":"constraint","name":"learned/keep-transactions-as-short-as-possible-fetch-dat","provenance":{"correction_id":"c-1770878028184886746","created_at":"2026-02-11T22:33:48.185207236-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-23dd4a209d65","kind":"behavior","content":{"content":{"canonical":"Always run `golangci-lint run ./...` locally before pushing commits, not just `go vet`. The CI linter (golangci-lint v2) catches errcheck, gosec G115, gofmt formatting, and other issues that go vet misses. Check `.golangci.yml` for the project's linter config.","expanded":"When working on this type of task, avoid: Ran `go vet` and `go test` to check for lint issues before pushing, but CI uses `golangci-lint` which catches errcheck, gosec, gofmt, and other linters that `go vet` doesn't. This caused multiple round-trips of push-fail-fix cycles.\n\nInstead: Always run `golangci-lint run ./...` locally before pushing commits, not just `go vet`. The CI linter (golangci-lint v2) catches errcheck, gosec G115, gofmt formatting, and other issues that go vet misses. Check `.golangci.yml` for the project's linter config.","structured":{"avoid":"Ran `go vet` and `go test` to check for lint issues before pushing, but CI uses `golangci-lint` which catches errcheck, gosec, gofmt, and other linters that `go vet` doesn't. This caused multiple round-trips of push-fail-fix cycles.","prefer":"Always run `golangci-lint run ./...` locally before pushing commits, not just `go vet`. The CI linter (golangci-lint v2) catches errcheck, gosec G115, gofmt formatting, and other issues that go vet misses. Check `.golangci.yml` for the project's linter config."},"tags":["ci","configuration","go","linting","yaml"]},"kind":"directive","name":"learned/always-run-golangci-lint-run-locally-befor","provenance":{"correction_id":"c-1770862566889551880","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.66,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":3,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-27ea2e0d892b","kind":"behavior","content":{"content":{"canonical":"After creating PRs, always pause and present them to the user for review before merging. Say 'PRs are ready for your review' and wait for explicit approval to merge. Never merge PRs autonomously — merging is a user-authorized action.","expanded":"When working on this type of task, avoid: Merged PRs immediately after creation without waiting for user review. The plan said 'Merge both PRs (after review)' but I ran gh pr merge right away, skipping the review step.\n\nInstead: After creating PRs, always pause and present them to the user for review before merging. Say 'PRs are ready for your review' and wait for explicit approval to merge. Never merge PRs autonomously — merging is a user-authorized action.","structured":{"avoid":"Merged PRs immediately after creation without waiting for user review. The plan said 'Merge both PRs (after review)' but I ran gh pr merge right away, skipping the review step.","prefer":"After creating PRs, always pause and present them to the user for review before merging. Say 'PRs are ready for your review' and wait for explicit approval to merge. Never merge PRs autonomously — merging is a user-authorized action."}},"kind":"constraint","name":"learned/after-creating-prs-always-pause-and-present-them","provenance":{"correction_id":"c-1770571253349938614","created_at":"2026-02-08T09:20:53.350039813-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-2a8f486f91ee","kind":"behavior","content":{"content":{"canonical":"Use constructor injection to make dependencies explicit and enable testing with mocks. Define interfaces for dependencies to allow substitution. Use DI containers for complex graphs but avoid service locator pattern. Prefer composition over inheritance - inject collaborators rather than extending base classes.","expanded":"When working on this type of task, avoid: Using global singletons and service locators throughout the code, hiding dependencies and making testing difficult\n\nInstead: Use constructor injection to make dependencies explicit and enable testing with mocks. Define interfaces for dependencies to allow substitution. Use DI containers for complex graphs but avoid service locator pattern. Prefer composition over inheritance - inject collaborators rather than extending base classes.","structured":{"avoid":"Using global singletons and service locators throughout the code, hiding dependencies and making testing difficult","prefer":"Use constructor injection to make dependencies explicit and enable testing with mocks. Define interfaces for dependencies to allow substitution. Use DI containers for complex graphs but avoid service locator pattern. Prefer composition over inheritance - inject collaborators rather than extending base classes."}},"kind":"constraint","name":"learned/use-constructor-injection-to-make-dependencies-exp","provenance":{"correction_id":"c-1770877924056123580","created_at":"2026-02-11T22:32:04.056196657-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-2c96557905fe","kind":"behavior","content":{"content":{"canonical":"When git-town propose fails in non-interactive environments, it's just trying to open a browser/editor - not an auth issue. For stacked PRs, either: (1) use the pre-filled GitHub URL from git-town's output and open it manually, or (2) create the PR with gh CLI first, then use 'gh pr edit  --base \u003cparent-branch\u003e' to retarget it. The gh CLI uses existing authentication, no token management needed.","expanded":"When working on this type of task, avoid: When git-town propose fails with \"could not open a new TTY\" error, attempted to work around it by setting GitHub tokens in git config or environment variables. This is both unnecessary (gh CLI is already authenticated) and potentially insecure (leaves credentials on disk/in logs).\n\nInstead: When git-town propose fails in non-interactive environments, it's just trying to open a browser/editor - not an auth issue. For stacked PRs, either: (1) use the pre-filled GitHub URL from git-town's output and open it manually, or (2) create the PR with gh CLI first, then use 'gh pr edit  --base \u003cparent-branch\u003e' to retarget it. The gh CLI uses existing authentication, no token management needed.","structured":{"avoid":"When git-town propose fails with \"could not open a new TTY\" error, attempted to work around it by setting GitHub tokens in git config or environment variables. This is both unnecessary (gh CLI is already authenticated) and potentially insecure (leaves credentials on disk/in logs).","prefer":"When git-town propose fails in non-interactive environments, it's just trying to open a browser/editor - not an auth issue. For stacked PRs, either: (1) use the pre-filled GitHub URL from git-town's output and open it manually, or (2) create the PR with gh CLI first, then use 'gh pr edit  --base \u003cparent-branch\u003e' to retarget it. The gh CLI uses existing authentication, no token management needed."},"tags":["cli","git","pr"]},"kind":"procedure","name":"learned/when-git-town-propose-fails-in-non-interactive-env","provenance":{"correction_id":"c-1770792756137334297","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.68,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-2dec677a7c7c","kind":"behavior","content":{"content":{"canonical":"Implement query complexity analysis that assigns costs to fields and enforces maximum complexity limits. Set depth limits to prevent deeply nested queries. Use persisted queries in production to whitelist approved query patterns. Implement pagination for list fields with max page size limits.","expanded":"When working on this type of task, avoid: Allowing unbounded GraphQL queries without complexity limits, enabling clients to request deeply nested data that overwhelms the server\n\nInstead: Implement query complexity analysis that assigns costs to fields and enforces maximum complexity limits. Set depth limits to prevent deeply nested queries. Use persisted queries in production to whitelist approved query patterns. Implement pagination for list fields with max page size limits.","structured":{"avoid":"Allowing unbounded GraphQL queries without complexity limits, enabling clients to request deeply nested data that overwhelms the server","prefer":"Implement query complexity analysis that assigns costs to fields and enforces maximum complexity limits. Set depth limits to prevent deeply nested queries. Use persisted queries in production to whitelist approved query patterns. Implement pagination for list fields with max page size limits."}},"kind":"preference","name":"learned/implement-query-complexity-analysis-that-assigns-c","provenance":{"correction_id":"c-1770877773526353407","created_at":"2026-02-11T22:29:33.52646698-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-2e2932aa405e","kind":"behavior","content":{"content":{"canonical":"Always create separate PRs for each independent workstream. Never merge directly to main - use feature branches with PRs for review, even for security fixes.","expanded":"When working on this type of task, avoid: Merged feature branches directly into main instead of creating separate PRs for each workstream\n\nInstead: Always create separate PRs for each independent workstream. Never merge directly to main - use feature branches with PRs for review, even for security fixes.","structured":{"avoid":"Merged feature branches directly into main instead of creating separate PRs for each workstream","prefer":"Always create separate PRs for each independent workstream. Never merge directly to main - use feature branches with PRs for review, even for security fixes."}},"kind":"constraint","name":"learned/always-create-separate-prs-for-each-independent-wo","provenance":{"correction_id":"c-1770520411864253156","created_at":"2026-02-07T19:13:31.864381477-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.62,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-2e8f6a45bed4","kind":"behavior","content":{"content":{"canonical":"Choose consistency model based on business requirements: use eventual consistency with conflict resolution (CRDTs, last-write-wins) for high availability. Reserve strong consistency (Paxos, Raft) for critical operations only. Implement idempotent operations and compensating transactions for eventual consistency scenarios.","expanded":"When working on this type of task, avoid: Expecting strong consistency across distributed systems without understanding CAP theorem tradeoffs, leading to systems that sacrifice availability unnecessarily\n\nInstead: Choose consistency model based on business requirements: use eventual consistency with conflict resolution (CRDTs, last-write-wins) for high availability. Reserve strong consistency (Paxos, Raft) for critical operations only. Implement idempotent operations and compensating transactions for eventual consistency scenarios.","structured":{"avoid":"Expecting strong consistency across distributed systems without understanding CAP theorem tradeoffs, leading to systems that sacrifice availability unnecessarily","prefer":"Choose consistency model based on business requirements: use eventual consistency with conflict resolution (CRDTs, last-write-wins) for high availability. Reserve strong consistency (Paxos, Raft) for critical operations only. Implement idempotent operations and compensating transactions for eventual consistency scenarios."}},"kind":"preference","name":"learned/choose-consistency-model-based-on-business-require","provenance":{"correction_id":"c-1770877696422698539","created_at":"2026-02-11T22:28:16.423060027-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-2e920d4f37f8","kind":"behavior","content":{"content":{"canonical":"Tune shared_buffers to 25% of RAM, work_mem based on connection count and query complexity. Enable autovacuum and tune for workload. Use EXPLAIN ANALYZE to identify slow queries and add appropriate indexes. Monitor with pg_stat_statements to find actual query performance bottlenecks.","expanded":"When working on this type of task, avoid: Deploying PostgreSQL with default configuration settings, missing significant performance gains from tuning work_mem, shared_buffers, and other parameters\n\nInstead: Tune shared_buffers to 25% of RAM, work_mem based on connection count and query complexity. Enable autovacuum and tune for workload. Use EXPLAIN ANALYZE to identify slow queries and add appropriate indexes. Monitor with pg_stat_statements to find actual query performance bottlenecks.","structured":{"avoid":"Deploying PostgreSQL with default configuration settings, missing significant performance gains from tuning work_mem, shared_buffers, and other parameters","prefer":"Tune shared_buffers to 25% of RAM, work_mem based on connection count and query complexity. Enable autovacuum and tune for workload. Use EXPLAIN ANALYZE to identify slow queries and add appropriate indexes. Monitor with pg_stat_statements to find actual query performance bottlenecks."}},"kind":"preference","name":"learned/tune-shared_buffers-to-25-of-ram-work_mem-based","provenance":{"correction_id":"c-1770877863307910613","created_at":"2026-02-11T22:31:03.308016652-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-3130beae3696","kind":"behavior","content":{"content":{"canonical":"Validate all input at API boundaries using schema validation (JSON Schema, Zod, Joi). Use parameterized queries/prepared statements for SQL to prevent injection. Escape output based on context (HTML encoding, JS encoding, URL encoding). Set Content-Security-Policy headers to mitigate XSS. Never trust client-side validation alone.","expanded":"When working on this type of task, avoid: Building web applications without sanitizing user input or validating data types, making them vulnerable to injection attacks and XSS\n\nInstead: Validate all input at API boundaries using schema validation (JSON Schema, Zod, Joi). Use parameterized queries/prepared statements for SQL to prevent injection. Escape output based on context (HTML encoding, JS encoding, URL encoding). Set Content-Security-Policy headers to mitigate XSS. Never trust client-side validation alone.","structured":{"avoid":"Building web applications without sanitizing user input or validating data types, making them vulnerable to injection attacks and XSS","prefer":"Validate all input at API boundaries using schema validation (JSON Schema, Zod, Joi). Use parameterized queries/prepared statements for SQL to prevent injection. Escape output based on context (HTML encoding, JS encoding, URL encoding). Set Content-Security-Policy headers to mitigate XSS. Never trust client-side validation alone."}},"kind":"constraint","name":"learned/validate-all-input-at-api-boundaries-using-schema","provenance":{"correction_id":"c-1770877788410018129","created_at":"2026-02-11T22:29:48.410072401-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-315d86dc2739","kind":"behavior","content":{"content":{"canonical":"Always use parameterized queries/prepared statements where user input is bound as parameters, not concatenated into query string. Use ORM query builders that parameterize automatically. For dynamic table/column names that can't be parameterized, use strict allowlists. Never disable or escape query parameterization.","expanded":"When working on this type of task, avoid: Building SQL queries with string concatenation or interpolation of user input, creating SQL injection vulnerabilities\n\nInstead: Always use parameterized queries/prepared statements where user input is bound as parameters, not concatenated into query string. Use ORM query builders that parameterize automatically. For dynamic table/column names that can't be parameterized, use strict allowlists. Never disable or escape query parameterization.","structured":{"avoid":"Building SQL queries with string concatenation or interpolation of user input, creating SQL injection vulnerabilities","prefer":"Always use parameterized queries/prepared statements where user input is bound as parameters, not concatenated into query string. Use ORM query builders that parameterize automatically. For dynamic table/column names that can't be parameterized, use strict allowlists. Never disable or escape query parameterization."}},"kind":"constraint","name":"learned/always-use-parameterized-queries-prepared-statemen","provenance":{"correction_id":"c-1770877797978497474","created_at":"2026-02-11T22:29:57.978590459-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-358cb6663d75","kind":"behavior","content":{"content":{"canonical":"Prioritize Largest Contentful Paint (LCP \u003c 2.5s), First Input Delay (FID \u003c 100ms), and Cumulative Layout Shift (CLS \u003c 0.1). Use resource hints (preload, preconnect), optimize images with responsive sizes and lazy loading, and reserve space for dynamic content to prevent layout shifts.","expanded":"When working on this type of task, avoid: Optimizing for page load time without considering Core Web Vitals metrics (LCP, FID, CLS), leading to pages that feel slow despite fast total load times\n\nInstead: Prioritize Largest Contentful Paint (LCP \u003c 2.5s), First Input Delay (FID \u003c 100ms), and Cumulative Layout Shift (CLS \u003c 0.1). Use resource hints (preload, preconnect), optimize images with responsive sizes and lazy loading, and reserve space for dynamic content to prevent layout shifts.","structured":{"avoid":"Optimizing for page load time without considering Core Web Vitals metrics (LCP, FID, CLS), leading to pages that feel slow despite fast total load times","prefer":"Prioritize Largest Contentful Paint (LCP \u003c 2.5s), First Input Delay (FID \u003c 100ms), and Cumulative Layout Shift (CLS \u003c 0.1). Use resource hints (preload, preconnect), optimize images with responsive sizes and lazy loading, and reserve space for dynamic content to prevent layout shifts."}},"kind":"procedure","name":"learned/prioritize-largest-contentful-paint-lcp-2-5s","provenance":{"correction_id":"c-1770877631468569601","created_at":"2026-02-11T22:27:11.468652756-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-3889d615a50a","kind":"behavior","content":{"content":{"canonical":"Even when using git-town, focus on parallel workflows. PRs that depend on each other can all be branched off a common parent and worked on simultaneously. They can be moved/restacked onto each other when needed. Never default to sequential work just because of \"sequential stacks\" — it's inefficient. Use worktrees and parallel agents to work on multiple branches at once, then restack as dependencies land.","expanded":"When working on this type of task, avoid: Defaulting to sequential work because git-town uses sequential stacks. Assuming that dependent PRs must be worked on one at a time in a single session, losing parallelism.\n\nInstead: Even when using git-town, focus on parallel workflows. PRs that depend on each other can all be branched off a common parent and worked on simultaneously. They can be moved/restacked onto each other when needed. Never default to sequential work just because of \"sequential stacks\" — it's inefficient. Use worktrees and parallel agents to work on multiple branches at once, then restack as dependencies land.","structured":{"avoid":"Defaulting to sequential work because git-town uses sequential stacks. Assuming that dependent PRs must be worked on one at a time in a single session, losing parallelism.","prefer":"Even when using git-town, focus on parallel workflows. PRs that depend on each other can all be branched off a common parent and worked on simultaneously. They can be moved/restacked onto each other when needed. Never default to sequential work just because of \"sequential stacks\" — it's inefficient. Use worktrees and parallel agents to work on multiple branches at once, then restack as dependencies land."},"tags":["git","worktree"]},"kind":"constraint","name":"learned/even-when-using-git-town-focus-on-parallel-workfl","provenance":{"correction_id":"c-1770826831064090428","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.68,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-3be7c15bc90c","kind":"behavior","content":{"content":{"canonical":"Maximize arithmetic intensity (FLOPs per byte transferred). Use shared memory to cache frequently accessed data within thread blocks. Coalesce global memory accesses by ensuring adjacent threads access adjacent memory. Profile with nsight/rocprof to identify memory vs compute bottlenecks and optimize accordingly.","expanded":"When working on this type of task, avoid: Using GPU compute without understanding memory bandwidth limitations, leading to kernels that are memory-bound rather than compute-bound\n\nInstead: Maximize arithmetic intensity (FLOPs per byte transferred). Use shared memory to cache frequently accessed data within thread blocks. Coalesce global memory accesses by ensuring adjacent threads access adjacent memory. Profile with nsight/rocprof to identify memory vs compute bottlenecks and optimize accordingly.","structured":{"avoid":"Using GPU compute without understanding memory bandwidth limitations, leading to kernels that are memory-bound rather than compute-bound","prefer":"Maximize arithmetic intensity (FLOPs per byte transferred). Use shared memory to cache frequently accessed data within thread blocks. Coalesce global memory accesses by ensuring adjacent threads access adjacent memory. Profile with nsight/rocprof to identify memory vs compute bottlenecks and optimize accordingly."}},"kind":"preference","name":"learned/maximize-arithmetic-intensity-flops-per-byte-tran","provenance":{"correction_id":"c-1770877714456536237","created_at":"2026-02-11T22:28:34.456681029-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-3c16c392c757","kind":"behavior","content":{"content":{"canonical":"Use Redis-backed rate limiting with atomic Lua scripts for distributed environments. Redis provides atomic INCR operations and automatically shards across hash slots for horizontal scaling, maintaining both consistency and performance under load.","expanded":"When working on this type of task, avoid: Using local in-memory counters for rate limiting in multi-instance deployments, creating N independent limits that allow attackers to bypass by distributing requests\n\nInstead: Use Redis-backed rate limiting with atomic Lua scripts for distributed environments. Redis provides atomic INCR operations and automatically shards across hash slots for horizontal scaling, maintaining both consistency and performance under load.","structured":{"avoid":"Using local in-memory counters for rate limiting in multi-instance deployments, creating N independent limits that allow attackers to bypass by distributing requests","prefer":"Use Redis-backed rate limiting with atomic Lua scripts for distributed environments. Redis provides atomic INCR operations and automatically shards across hash slots for horizontal scaling, maintaining both consistency and performance under load."}},"kind":"preference","name":"learned/use-redis-backed-rate-limiting-with-atomic-lua-scr","provenance":{"correction_id":"c-1770877411214345160","created_at":"2026-02-11T22:23:31.214450497-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-3cde2b788665","kind":"behavior","content":{"content":{"canonical":"Worktrees can also be used simply for branch isolation — to ensure no accidental commits end up on main. When the user asks to 'swap to a worktree', they may just want branch safety, not parallel agents. Don't assume worktree = parallel lanes.","expanded":"When working on this type of task, avoid: Created a worktree thinking it was for parallel subagent work with separate PRs\n\nInstead: Worktrees can also be used simply for branch isolation — to ensure no accidental commits end up on main. When the user asks to 'swap to a worktree', they may just want branch safety, not parallel agents. Don't assume worktree = parallel lanes.","structured":{"avoid":"Created a worktree thinking it was for parallel subagent work with separate PRs","prefer":"Worktrees can also be used simply for branch isolation — to ensure no accidental commits end up on main. When the user asks to 'swap to a worktree', they may just want branch safety, not parallel agents. Don't assume worktree = parallel lanes."},"tags":["worktree"]},"kind":"constraint","name":"learned/worktrees-can-also-be-used-simply-for-branch-isola","provenance":{"correction_id":"c-1770704469460064970","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.7200000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-43c4d744c0e6","kind":"behavior","content":{"content":{"canonical":"Alert on symptoms (user-facing problems) not causes (high CPU). Define clear runbooks for each alert with investigation steps. Use severity levels appropriately - page for critical, ticket for warning. Aggregate related alerts to reduce noise. Track alert quality metrics (time to acknowledge, false positive rate) and tune thresholds based on data.","expanded":"When working on this type of task, avoid: Creating alerts for every metric threshold without considering actionability, leading to alert fatigue and ignored notifications\n\nInstead: Alert on symptoms (user-facing problems) not causes (high CPU). Define clear runbooks for each alert with investigation steps. Use severity levels appropriately - page for critical, ticket for warning. Aggregate related alerts to reduce noise. Track alert quality metrics (time to acknowledge, false positive rate) and tune thresholds based on data.","structured":{"avoid":"Creating alerts for every metric threshold without considering actionability, leading to alert fatigue and ignored notifications","prefer":"Alert on symptoms (user-facing problems) not causes (high CPU). Define clear runbooks for each alert with investigation steps. Use severity levels appropriately - page for critical, ticket for warning. Aggregate related alerts to reduce noise. Track alert quality metrics (time to acknowledge, false positive rate) and tune thresholds based on data."}},"kind":"preference","name":"learned/alert-on-symptoms-user-facing-problems-not-cause","provenance":{"correction_id":"c-1770878056814050423","created_at":"2026-02-11T22:34:16.814119874-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-440356e972ee","kind":"behavior","content":{"content":{"canonical":"Agents should proactively use floop during conversations - capture corrections in real-time using own judgment and intuition","expanded":"When working on this type of task, avoid: Waited for explicit user instruction to use floop\n\nInstead: Agents should proactively use floop during conversations - capture corrections in real-time using own judgment and intuition","structured":{"avoid":"Waited for explicit user instruction to use floop","prefer":"Agents should proactively use floop during conversations - capture corrections in real-time using own judgment and intuition"}},"kind":"preference","name":"learned/agents-should-proactively-use-floop-during-convers","provenance":{"correction_id":"c-1769577665725545057","created_at":"2026-01-27T21:21:05.725931864-08:00","source_type":"learned"},"when":{"task":"development"}},"metadata":{"confidence":0.6950000000000001,"priority":0,"scope":"local","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-4441585a0478","kind":"behavior","content":{"content":{"canonical":"Always ensure documentation is updated or created when adding/updating features. For net new features, docs should be detailed. When making implementation plans, explicitly include a docs pipeline as a step — this can often be done in parallel with implementation work, not sequentially after it.","expanded":"When working on this type of task, avoid: Adding or updating features without ensuring documentation is created or updated. Plans for new features don't include a docs pipeline, so documentation becomes an afterthought or gets skipped entirely.\n\nInstead: Always ensure documentation is updated or created when adding/updating features. For net new features, docs should be detailed. When making implementation plans, explicitly include a docs pipeline as a step — this can often be done in parallel with implementation work, not sequentially after it.","structured":{"avoid":"Adding or updating features without ensuring documentation is created or updated. Plans for new features don't include a docs pipeline, so documentation becomes an afterthought or gets skipped entirely.","prefer":"Always ensure documentation is updated or created when adding/updating features. For net new features, docs should be detailed. When making implementation plans, explicitly include a docs pipeline as a step — this can often be done in parallel with implementation work, not sequentially after it."},"tags":["ci"]},"kind":"directive","name":"learned/always-ensure-documentation-is-updated-or-created","provenance":{"correction_id":"c-1770826828041961163","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.68,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-45661080e612","kind":"behavior","content":{"content":{"canonical":"When needing to test a new MCP server instance, either ask the user to restart the session, or use a sub-agent that can run the CLI directly (go run ./cmd/floop) to verify the fix without needing MCP server restart.","expanded":"When working on this type of task, avoid: Tried to test floop_learn via MCP tool after fixing the code, but the MCP server was still running the old binary. Couldn't verify the fix in-session.\n\nInstead: When needing to test a new MCP server instance, either ask the user to restart the session, or use a sub-agent that can run the CLI directly (go run ./cmd/floop) to verify the fix without needing MCP server restart.","structured":{"avoid":"Tried to test floop_learn via MCP tool after fixing the code, but the MCP server was still running the old binary. Couldn't verify the fix in-session.","prefer":"When needing to test a new MCP server instance, either ask the user to restart the session, or use a sub-agent that can run the CLI directly (go run ./cmd/floop) to verify the fix without needing MCP server restart."}},"kind":"preference","name":"learned/when-needing-to-test-a-new-mcp-server-instance-ei","provenance":{"correction_id":"c-1770413399216004124","created_at":"2026-02-06T13:29:59.21608778-08:00","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.67,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-465e1485f7ba","kind":"behavior","content":{"content":{"canonical":"Use builder pattern for objects with many configuration options: provide fluent API with method chaining (builder.WithTimeout(5s).WithRetries(3).Build()). Validate required fields in Build() method. Make builders immutable to enable reuse. For simple cases, consider functional options pattern (variadic options functions).","expanded":"When working on this type of task, avoid: Creating objects with many constructor parameters leading to confusing call sites and making optional parameters difficult to handle\n\nInstead: Use builder pattern for objects with many configuration options: provide fluent API with method chaining (builder.WithTimeout(5s).WithRetries(3).Build()). Validate required fields in Build() method. Make builders immutable to enable reuse. For simple cases, consider functional options pattern (variadic options functions).","structured":{"avoid":"Creating objects with many constructor parameters leading to confusing call sites and making optional parameters difficult to handle","prefer":"Use builder pattern for objects with many configuration options: provide fluent API with method chaining (builder.WithTimeout(5s).WithRetries(3).Build()). Validate required fields in Build() method. Make builders immutable to enable reuse. For simple cases, consider functional options pattern (variadic options functions)."}},"kind":"preference","name":"learned/use-builder-pattern-for-objects-with-many-configur","provenance":{"correction_id":"c-1770877929425016174","created_at":"2026-02-11T22:32:09.425141378-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-46b6d7ca3a51","kind":"behavior","content":{"content":{"canonical":"Let exceptions bubble up unless you can handle them meaningfully. Wrap errors with context when crossing boundaries: fmt.Errorf(\"failed to process user %s: %w\", userID, err). Use structured error types for programmatic handling. Include request IDs, user IDs, and operation context in error messages. Log errors once at the top level with full context.","expanded":"When working on this type of task, avoid: Catching exceptions and ignoring them or logging without context, making debugging impossible when errors occur in production\n\nInstead: Let exceptions bubble up unless you can handle them meaningfully. Wrap errors with context when crossing boundaries: fmt.Errorf(\"failed to process user %s: %w\", userID, err). Use structured error types for programmatic handling. Include request IDs, user IDs, and operation context in error messages. Log errors once at the top level with full context.","structured":{"avoid":"Catching exceptions and ignoring them or logging without context, making debugging impossible when errors occur in production","prefer":"Let exceptions bubble up unless you can handle them meaningfully. Wrap errors with context when crossing boundaries: fmt.Errorf(\"failed to process user %s: %w\", userID, err). Use structured error types for programmatic handling. Include request IDs, user IDs, and operation context in error messages. Log errors once at the top level with full context."}},"kind":"procedure","name":"learned/let-exceptions-bubble-up-unless-you-can-handle-the","provenance":{"correction_id":"c-1770877916795239115","created_at":"2026-02-11T22:31:56.795321179-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-48bdbe38b9c1","kind":"behavior","content":{"content":{"canonical":"Use property-based testing (QuickCheck, Hypothesis, fast-check) for algorithms and data transformations. Define properties that must hold for all inputs (round-trip serialization, idempotence, commutativity). Let the framework generate hundreds of test cases including edge cases. Shrink failing cases to minimal examples for debugging.","expanded":"When working on this type of task, avoid: Only writing example-based tests with hand-picked inputs, missing edge cases and invalid input combinations that cause bugs in production\n\nInstead: Use property-based testing (QuickCheck, Hypothesis, fast-check) for algorithms and data transformations. Define properties that must hold for all inputs (round-trip serialization, idempotence, commutativity). Let the framework generate hundreds of test cases including edge cases. Shrink failing cases to minimal examples for debugging.","structured":{"avoid":"Only writing example-based tests with hand-picked inputs, missing edge cases and invalid input combinations that cause bugs in production","prefer":"Use property-based testing (QuickCheck, Hypothesis, fast-check) for algorithms and data transformations. Define properties that must hold for all inputs (round-trip serialization, idempotence, commutativity). Let the framework generate hundreds of test cases including edge cases. Shrink failing cases to minimal examples for debugging."}},"kind":"preference","name":"learned/use-property-based-testing-quickcheck-hypothesis","provenance":{"correction_id":"c-1770877852183592859","created_at":"2026-02-11T22:30:52.183661218-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-48fc7215b0b9","kind":"behavior","content":{"content":{"canonical":"Version APIs from day one using URL versioning (/v1/users) or header-based versioning (Accept: application/vnd.api.v1+json). Maintain backward compatibility within major versions. Deprecate old versions gracefully with clear timelines and migration guides. Use content negotiation for minor version evolution.","expanded":"When working on this type of task, avoid: Making breaking changes to REST APIs without versioning, forcing all clients to update simultaneously or breaking existing integrations\n\nInstead: Version APIs from day one using URL versioning (/v1/users) or header-based versioning (Accept: application/vnd.api.v1+json). Maintain backward compatibility within major versions. Deprecate old versions gracefully with clear timelines and migration guides. Use content negotiation for minor version evolution.","structured":{"avoid":"Making breaking changes to REST APIs without versioning, forcing all clients to update simultaneously or breaking existing integrations","prefer":"Version APIs from day one using URL versioning (/v1/users) or header-based versioning (Accept: application/vnd.api.v1+json). Maintain backward compatibility within major versions. Deprecate old versions gracefully with clear timelines and migration guides. Use content negotiation for minor version evolution."}},"kind":"preference","name":"learned/version-apis-from-day-one-using-url-versioning-v","provenance":{"correction_id":"c-1770877764732634868","created_at":"2026-02-11T22:29:24.732708777-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-511d1c63c7c9","kind":"behavior","content":{"content":{"canonical":"Use event-driven cache invalidation: publish invalidation events when data changes, subscribe cache layers to these events. For distributed caches, use pub/sub (Redis PUBLISH/SUBSCRIBE) to propagate invalidations. Combine with short TTLs as fallback for missed events.","expanded":"When working on this type of task, avoid: Using time-based cache expiration (TTL) for data that changes based on events, leading to stale data being served until TTL expires\n\nInstead: Use event-driven cache invalidation: publish invalidation events when data changes, subscribe cache layers to these events. For distributed caches, use pub/sub (Redis PUBLISH/SUBSCRIBE) to propagate invalidations. Combine with short TTLs as fallback for missed events.","structured":{"avoid":"Using time-based cache expiration (TTL) for data that changes based on events, leading to stale data being served until TTL expires","prefer":"Use event-driven cache invalidation: publish invalidation events when data changes, subscribe cache layers to these events. For distributed caches, use pub/sub (Redis PUBLISH/SUBSCRIBE) to propagate invalidations. Combine with short TTLs as fallback for missed events."}},"kind":"preference","name":"learned/use-event-driven-cache-invalidation-publish-inval","provenance":{"correction_id":"c-1770877659150231648","created_at":"2026-02-11T22:27:39.150319352-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-583551a06647","kind":"behavior","content":{"content":{"canonical":"Structure parallel work in waves with sync points: Wave 1 (foundation) must complete before Wave 2 (major refactoring) can start. Each wave has a verification gate (make build \u0026\u0026 make lint \u0026\u0026 make test) that must pass before the next wave begins. This prevents cascading failures.","expanded":"When working on this type of task, avoid: Launching all parallel refactoring work at once without sync points\n\nInstead: Structure parallel work in waves with sync points: Wave 1 (foundation) must complete before Wave 2 (major refactoring) can start. Each wave has a verification gate (make build \u0026\u0026 make lint \u0026\u0026 make test) that must pass before the next wave begins. This prevents cascading failures.","structured":{"avoid":"Launching all parallel refactoring work at once without sync points","prefer":"Structure parallel work in waves with sync points: Wave 1 (foundation) must complete before Wave 2 (major refactoring) can start. Each wave has a verification gate (make build \u0026\u0026 make lint \u0026\u0026 make test) that must pass before the next wave begins. This prevents cascading failures."},"tags":["linting","make","refactoring","testing"]},"kind":"directive","name":"learned/structure-parallel-work-in-waves-with-sync-points","provenance":{"correction_id":"c-1770186289129348869","source_type":"learned"},"when":{"task":"refactoring"}},"metadata":{"confidence":0.57,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:54-08:00","times_activated":1,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-5aa3fbde5aa1","kind":"behavior","content":{"content":{"canonical":"Always use async Task, never async void (except event handlers). Always await async calls - missing await causes code to continue before operation completes. Use Task.WhenAll for parallel operations instead of sequential awaits. Configure await with .ConfigureAwait(false) in library code to avoid deadlocks.","expanded":"When working on this type of task, avoid: Using async void methods or forgetting await keywords, causing exceptions to be swallowed and breaking error handling\n\nInstead: Always use async Task, never async void (except event handlers). Always await async calls - missing await causes code to continue before operation completes. Use Task.WhenAll for parallel operations instead of sequential awaits. Configure await with .ConfigureAwait(false) in library code to avoid deadlocks.","structured":{"avoid":"Using async void methods or forgetting await keywords, causing exceptions to be swallowed and breaking error handling","prefer":"Always use async Task, never async void (except event handlers). Always await async calls - missing await causes code to continue before operation completes. Use Task.WhenAll for parallel operations instead of sequential awaits. Configure await with .ConfigureAwait(false) in library code to avoid deadlocks."}},"kind":"constraint","name":"learned/always-use-async-task-never-async-void-except-ev","provenance":{"correction_id":"c-1770877886724644817","created_at":"2026-02-11T22:31:26.724732742-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-5b8b49d79406","kind":"behavior","content":{"content":{"canonical":"Avoid time-based assertions (sleep, setTimeout) - use explicit synchronization (wait for condition, event, callback). Reset global state between tests with proper setup/teardown. Use deterministic test data and avoid randomness without seeding. Run tests in random order to detect hidden dependencies. Quarantine flaky tests and fix root cause, don't just retry.","expanded":"When working on this type of task, avoid: Writing tests with race conditions, implicit timing assumptions, or shared global state, causing tests to fail randomly and erode trust in CI\n\nInstead: Avoid time-based assertions (sleep, setTimeout) - use explicit synchronization (wait for condition, event, callback). Reset global state between tests with proper setup/teardown. Use deterministic test data and avoid randomness without seeding. Run tests in random order to detect hidden dependencies. Quarantine flaky tests and fix root cause, don't just retry.","structured":{"avoid":"Writing tests with race conditions, implicit timing assumptions, or shared global state, causing tests to fail randomly and erode trust in CI","prefer":"Avoid time-based assertions (sleep, setTimeout) - use explicit synchronization (wait for condition, event, callback). Reset global state between tests with proper setup/teardown. Use deterministic test data and avoid randomness without seeding. Run tests in random order to detect hidden dependencies. Quarantine flaky tests and fix root cause, don't just retry."}},"kind":"constraint","name":"learned/avoid-time-based-assertions-sleep-settimeout","provenance":{"correction_id":"c-1770877858252675848","created_at":"2026-02-11T22:30:58.252768342-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-5bb0018061b2","kind":"behavior","content":{"content":{"canonical":"Use resource-oriented URLs with nouns only (/users/123/orders, not /getUser or /createOrder). Follow HTTP methods: GET for reads (idempotent, cacheable), POST for creates, PUT/PATCH for updates, DELETE for deletes. Use proper status codes: 200 OK, 201 Created, 204 No Content, 400 Bad Request, 404 Not Found, 500 Server Error.","expanded":"When working on this type of task, avoid: Designing REST APIs with inconsistent URL patterns, mixing verbs in URLs, and not following HTTP method semantics (using GET for mutations, POST for reads)\n\nInstead: Use resource-oriented URLs with nouns only (/users/123/orders, not /getUser or /createOrder). Follow HTTP methods: GET for reads (idempotent, cacheable), POST for creates, PUT/PATCH for updates, DELETE for deletes. Use proper status codes: 200 OK, 201 Created, 204 No Content, 400 Bad Request, 404 Not Found, 500 Server Error.","structured":{"avoid":"Designing REST APIs with inconsistent URL patterns, mixing verbs in URLs, and not following HTTP method semantics (using GET for mutations, POST for reads)","prefer":"Use resource-oriented URLs with nouns only (/users/123/orders, not /getUser or /createOrder). Follow HTTP methods: GET for reads (idempotent, cacheable), POST for creates, PUT/PATCH for updates, DELETE for deletes. Use proper status codes: 200 OK, 201 Created, 204 No Content, 400 Bad Request, 404 Not Found, 500 Server Error."}},"kind":"preference","name":"learned/use-resource-oriented-urls-with-nouns-only-users","provenance":{"correction_id":"c-1770877760124307025","created_at":"2026-02-11T22:29:20.124401332-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-5c366b1ced0e","kind":"behavior","content":{"content":{"canonical":"For large tasks (5+ files, multiple independent workstreams): (1) Identify dependency graph between steps, (2) Group independent steps into parallel worktree lanes, (3) Use subagents with dedicated worktrees per lane, (4) Submit PRs per lane, (5) Pause for user review before merging, (6) Clean up worktrees after PR merge. Plans should show the parallelism explicitly.","expanded":"When working on this type of task, avoid: Treating large multi-step implementation plans as sequential single-branch work. Not considering parallel execution via worktrees and subagents.\n\nInstead: For large tasks (5+ files, multiple independent workstreams): (1) Identify dependency graph between steps, (2) Group independent steps into parallel worktree lanes, (3) Use subagents with dedicated worktrees per lane, (4) Submit PRs per lane, (5) Pause for user review before merging, (6) Clean up worktrees after PR merge. Plans should show the parallelism explicitly.","structured":{"avoid":"Treating large multi-step implementation plans as sequential single-branch work. Not considering parallel execution via worktrees and subagents.","prefer":"For large tasks (5+ files, multiple independent workstreams): (1) Identify dependency graph between steps, (2) Group independent steps into parallel worktree lanes, (3) Use subagents with dedicated worktrees per lane, (4) Submit PRs per lane, (5) Pause for user review before merging, (6) Clean up worktrees after PR merge. Plans should show the parallelism explicitly."},"tags":["filesystem","pr","worktree"]},"kind":"preference","name":"learned/for-large-tasks-5-files-multiple-independent-wo","provenance":{"correction_id":"c-1770701938275184772","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.7400000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-5c3bf7eae020","kind":"behavior","content":{"content":{"canonical":"Use database transactions for test isolation - rollback after each test. Create fresh test database for each test run or use in-memory databases. Use test fixtures with known data state. For integration tests, use Docker containers that are destroyed after test run. Ensure tests can run in any order and in parallel.","expanded":"When working on this type of task, avoid: Running tests that modify shared database state without cleanup, causing tests to pass or fail depending on execution order\n\nInstead: Use database transactions for test isolation - rollback after each test. Create fresh test database for each test run or use in-memory databases. Use test fixtures with known data state. For integration tests, use Docker containers that are destroyed after test run. Ensure tests can run in any order and in parallel.","structured":{"avoid":"Running tests that modify shared database state without cleanup, causing tests to pass or fail depending on execution order","prefer":"Use database transactions for test isolation - rollback after each test. Create fresh test database for each test run or use in-memory databases. Use test fixtures with known data state. For integration tests, use Docker containers that are destroyed after test run. Ensure tests can run in any order and in parallel."}},"kind":"preference","name":"learned/use-database-transactions-for-test-isolation-rol","provenance":{"correction_id":"c-1770878040163835716","created_at":"2026-02-11T22:34:00.163958446-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-5ca906dd04fc","kind":"behavior","content":{"content":{"canonical":"Call floop_learn whenever insights emerge during ANY work — not just corrections. Discovery of gaps, patterns, architectural understanding, feature inventories, design decisions, and any finding that would benefit future sessions is a learning opportunity. The system handles bloat through spreading activation, token budgets, and tiering — so learn freely and trust the system to curate. If a previous learning was too narrow, don't try to edit it — just learn the better version and let weights sort it out.","expanded":"When working on this type of task, avoid: Only called floop_learn when explicitly corrected or when making obvious mistakes. Missed learning opportunities during exploration, research, audits, and general work where insights were discovered but not captured.\n\nInstead: Call floop_learn whenever insights emerge during ANY work — not just corrections. Discovery of gaps, patterns, architectural understanding, feature inventories, design decisions, and any finding that would benefit future sessions is a learning opportunity. The system handles bloat through spreading activation, token budgets, and tiering — so learn freely and trust the system to curate. If a previous learning was too narrow, don't try to edit it — just learn the better version and let weights sort it out.","structured":{"avoid":"Only called floop_learn when explicitly corrected or when making obvious mistakes. Missed learning opportunities during exploration, research, audits, and general work where insights were discovered but not captured.","prefer":"Call floop_learn whenever insights emerge during ANY work — not just corrections. Discovery of gaps, patterns, architectural understanding, feature inventories, design decisions, and any finding that would benefit future sessions is a learning opportunity. The system handles bloat through spreading activation, token budgets, and tiering — so learn freely and trust the system to curate. If a previous learning was too narrow, don't try to edit it — just learn the better version and let weights sort it out."},"tags":["correction","floop","spreading-activation"]},"kind":"constraint","name":"learned/call-floop_learn-whenever-insights-emerge-during-a","provenance":{"correction_id":"c-1770704347456486267","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.7200000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-5d3f7201e10d","kind":"behavior","content":{"content":{"canonical":"When fixing bugs: (1) identify the bug, (2) write a test that fails due to that bug, (3) implement the fix, (4) verify the test passes. This builds confidence in the test suite as it expands.","expanded":"When working on this type of task, avoid: Identified a bug, then immediately implemented the fix without writing a failing test first\n\nInstead: When fixing bugs: (1) identify the bug, (2) write a test that fails due to that bug, (3) implement the fix, (4) verify the test passes. This builds confidence in the test suite as it expands.","structured":{"avoid":"Identified a bug, then immediately implemented the fix without writing a failing test first","prefer":"When fixing bugs: (1) identify the bug, (2) write a test that fails due to that bug, (3) implement the fix, (4) verify the test passes. This builds confidence in the test suite as it expands."}},"kind":"directive","name":"learned/when-fixing-bugs-1-identify-the-bug-2-write","provenance":{"correction_id":"correction-1769740357","created_at":"2026-01-29T18:32:37.391333747-08:00","source_type":"learned"}},"metadata":{"confidence":0.7200000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-5d991c44bfa0","kind":"behavior","content":{"content":{"canonical":"Always use sub-agents (Task tool with appropriate subagent_type) for parallel and independent work items. The orchestrator should stay lean — delegate implementation, research, and repetitive operations to sub-agents. This preserves context for decision-making and coordination. For repetitive operations like wiring many edges, use a sub-agent with Bash to run CLI commands in batch.","expanded":"When working on this type of task, avoid: Tried to do all work in the main orchestrator context without delegating to sub-agents. This blew out the context window (reached 3% remaining) and prevented completing the task. Also attempted to use MCP tools that weren't registered in the running server.\n\nInstead: Always use sub-agents (Task tool with appropriate subagent_type) for parallel and independent work items. The orchestrator should stay lean — delegate implementation, research, and repetitive operations to sub-agents. This preserves context for decision-making and coordination. For repetitive operations like wiring many edges, use a sub-agent with Bash to run CLI commands in batch.","structured":{"avoid":"Tried to do all work in the main orchestrator context without delegating to sub-agents. This blew out the context window (reached 3% remaining) and prevented completing the task. Also attempted to use MCP tools that weren't registered in the running server.","prefer":"Always use sub-agents (Task tool with appropriate subagent_type) for parallel and independent work items. The orchestrator should stay lean — delegate implementation, research, and repetitive operations to sub-agents. This preserves context for decision-making and coordination. For repetitive operations like wiring many edges, use a sub-agent with Bash to run CLI commands in batch."}},"kind":"preference","name":"learned/always-use-sub-agents-task-tool-with-appropriate","provenance":{"correction_id":"c-1770405398129779923","created_at":"2026-02-06T11:16:38.129839204-08:00","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.67,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-5e99371b680f","kind":"behavior","content":{"content":{"canonical":"Use event versioning/sequencing: include version numbers or timestamps in events. Implement optimistic concurrency control with version checks. For strict ordering requirements, use message queue partitioning (Kafka partitions, FIFO queues) to guarantee order per entity/aggregate.","expanded":"When working on this type of task, avoid: Assuming events will always be processed in the order they were emitted, leading to race conditions and inconsistent state when events arrive out of order\n\nInstead: Use event versioning/sequencing: include version numbers or timestamps in events. Implement optimistic concurrency control with version checks. For strict ordering requirements, use message queue partitioning (Kafka partitions, FIFO queues) to guarantee order per entity/aggregate.","structured":{"avoid":"Assuming events will always be processed in the order they were emitted, leading to race conditions and inconsistent state when events arrive out of order","prefer":"Use event versioning/sequencing: include version numbers or timestamps in events. Implement optimistic concurrency control with version checks. For strict ordering requirements, use message queue partitioning (Kafka partitions, FIFO queues) to guarantee order per entity/aggregate."}},"kind":"preference","name":"learned/use-event-versioning-sequencing-include-version-n","provenance":{"correction_id":"c-1770877675840841692","created_at":"2026-02-11T22:27:55.840917735-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-60319789d83e","kind":"behavior","content":{"content":{"canonical":"Implement /health endpoint that checks critical dependencies (database connectivity, external APIs). Return 200 for healthy, 503 for unhealthy with details. Implement separate /readiness endpoint for startup checks. Include version info in health response for debugging. Configure load balancer to remove unhealthy instances from rotation automatically.","expanded":"When working on this type of task, avoid: Deploying applications without health check endpoints, making it impossible for load balancers and orchestrators to detect failures\n\nInstead: Implement /health endpoint that checks critical dependencies (database connectivity, external APIs). Return 200 for healthy, 503 for unhealthy with details. Implement separate /readiness endpoint for startup checks. Include version info in health response for debugging. Configure load balancer to remove unhealthy instances from rotation automatically.","structured":{"avoid":"Deploying applications without health check endpoints, making it impossible for load balancers and orchestrators to detect failures","prefer":"Implement /health endpoint that checks critical dependencies (database connectivity, external APIs). Return 200 for healthy, 503 for unhealthy with details. Implement separate /readiness endpoint for startup checks. Include version info in health response for debugging. Configure load balancer to remove unhealthy instances from rotation automatically."}},"kind":"directive","name":"learned/implement-health-endpoint-that-checks-critical-de","provenance":{"correction_id":"c-1770878068567661209","created_at":"2026-02-11T22:34:28.567762811-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-604dae322117","kind":"behavior","content":{"content":{"canonical":"Use OAuth 2.0 for third-party delegated access with proper scope validation. Use OpenID Connect (built on OAuth 2.0) for authentication with standardized identity claims. Implement API keys for server-to-server communication with key rotation. Use mutual TLS for high-security environments. Never roll your own crypto or auth protocol.","expanded":"When working on this type of task, avoid: Implementing custom authentication schemes instead of using established standards, creating security vulnerabilities\n\nInstead: Use OAuth 2.0 for third-party delegated access with proper scope validation. Use OpenID Connect (built on OAuth 2.0) for authentication with standardized identity claims. Implement API keys for server-to-server communication with key rotation. Use mutual TLS for high-security environments. Never roll your own crypto or auth protocol.","structured":{"avoid":"Implementing custom authentication schemes instead of using established standards, creating security vulnerabilities","prefer":"Use OAuth 2.0 for third-party delegated access with proper scope validation. Use OpenID Connect (built on OAuth 2.0) for authentication with standardized identity claims. Implement API keys for server-to-server communication with key rotation. Use mutual TLS for high-security environments. Never roll your own crypto or auth protocol."}},"kind":"constraint","name":"learned/use-oauth-2-0-for-third-party-delegated-access-wit","provenance":{"correction_id":"c-1770878051094170224","created_at":"2026-02-11T22:34:11.094246887-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-605806a1bb47","kind":"behavior","content":{"content":{"canonical":"Use git-town for PR stacking. Key workflow: `git town hack ` for a base branch, `git town append ` to stack child branches, `git town propose --stack` to create PRs for the entire stack, `git town sync` to cascade rebases. For multi-agent parallel work, plan the stack hierarchy upfront — each agent gets an `append`-ed branch in the stack. git-town tracks parent-child relationships and handles the rebase cascade automatically. Use `git town branch` to visualize the stack, `git town up`/`down` to navigate. Config is in .git/config (forge-type=github, sync-feature-strategy=rebase, ship-strategy=api).","expanded":"When working on this type of task, avoid: Using plain git branches and manual `gh pr create --base ` for PR stacks, then manually rebasing child branches when parents merge. This is error-prone and tedious, especially when planning multi-agent parallel flows for complex features where each agent works on a stacked branch.\n\nInstead: Use git-town for PR stacking. Key workflow: `git town hack ` for a base branch, `git town append ` to stack child branches, `git town propose --stack` to create PRs for the entire stack, `git town sync` to cascade rebases. For multi-agent parallel work, plan the stack hierarchy upfront — each agent gets an `append`-ed branch in the stack. git-town tracks parent-child relationships and handles the rebase cascade automatically. Use `git town branch` to visualize the stack, `git town up`/`down` to navigate. Config is in .git/config (forge-type=github, sync-feature-strategy=rebase, ship-strategy=api).","structured":{"avoid":"Using plain git branches and manual `gh pr create --base ` for PR stacks, then manually rebasing child branches when parents merge. This is error-prone and tedious, especially when planning multi-agent parallel flows for complex features where each agent works on a stacked branch.","prefer":"Use git-town for PR stacking. Key workflow: `git town hack ` for a base branch, `git town append ` to stack child branches, `git town propose --stack` to create PRs for the entire stack, `git town sync` to cascade rebases. For multi-agent parallel work, plan the stack hierarchy upfront — each agent gets an `append`-ed branch in the stack. git-town tracks parent-child relationships and handles the rebase cascade automatically. Use `git town branch` to visualize the stack, `git town up`/`down` to navigate. Config is in .git/config (forge-type=github, sync-feature-strategy=rebase, ship-strategy=api)."},"tags":["api","configuration","git","pr","workflow"]},"kind":"procedure","name":"learned/use-git-town-for-pr-stacking-key-workflow-git-t","provenance":{"correction_id":"c-1770781212221768120","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.68,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-608e22eeb78b","kind":"behavior","content":{"content":{"canonical":"Always use `git -C \u003cworktree-path\u003e` for all git operations when working with worktrees, or explicitly `cd` into the worktree first. The shell CWD may silently reset to the main repo after running other commands (like puppeteer). Verify with `git branch --show-current` before committing.","expanded":"When working on this type of task, avoid: Ran git add/commit from the main repo directory instead of the worktree, accidentally committing to the wrong branch (feat/activation-tracking instead of feat/graph-html-visualization)\n\nInstead: Always use `git -C \u003cworktree-path\u003e` for all git operations when working with worktrees, or explicitly `cd` into the worktree first. The shell CWD may silently reset to the main repo after running other commands (like puppeteer). Verify with `git branch --show-current` before committing.","structured":{"avoid":"Ran git add/commit from the main repo directory instead of the worktree, accidentally committing to the wrong branch (feat/activation-tracking instead of feat/graph-html-visualization)","prefer":"Always use `git -C \u003cworktree-path\u003e` for all git operations when working with worktrees, or explicitly `cd` into the worktree first. The shell CWD may silently reset to the main repo after running other commands (like puppeteer). Verify with `git branch --show-current` before committing."},"tags":["bash","git","worktree"]},"kind":"procedure","name":"learned/always-use-git-c-worktree-path-for-all-git-op","provenance":{"correction_id":"c-1770746557318279903","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.7000000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-60d454fedb89","kind":"behavior","content":{"content":{"canonical":"Implement CSRF tokens for state-changing operations (POST/PUT/DELETE) using SameSite cookies and synchronizer token pattern. Set Content-Security-Policy header to disallow inline scripts and restrict script sources. Use HTTPOnly and Secure flags on authentication cookies. Validate Origin and Referer headers for additional CSRF protection.","expanded":"When working on this type of task, avoid: Accepting requests without CSRF protection or allowing inline scripts without Content-Security-Policy, exposing applications to cross-site attacks\n\nInstead: Implement CSRF tokens for state-changing operations (POST/PUT/DELETE) using SameSite cookies and synchronizer token pattern. Set Content-Security-Policy header to disallow inline scripts and restrict script sources. Use HTTPOnly and Secure flags on authentication cookies. Validate Origin and Referer headers for additional CSRF protection.","structured":{"avoid":"Accepting requests without CSRF protection or allowing inline scripts without Content-Security-Policy, exposing applications to cross-site attacks","prefer":"Implement CSRF tokens for state-changing operations (POST/PUT/DELETE) using SameSite cookies and synchronizer token pattern. Set Content-Security-Policy header to disallow inline scripts and restrict script sources. Use HTTPOnly and Secure flags on authentication cookies. Validate Origin and Referer headers for additional CSRF protection."}},"kind":"procedure","name":"learned/implement-csrf-tokens-for-state-changing-operation","provenance":{"correction_id":"c-1770877803128642121","created_at":"2026-02-11T22:30:03.128820748-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-64c024585560","kind":"behavior","content":{"content":{"canonical":"Before touching ANY existing worktree: (1) check for uncommitted changes with `git -C  status`, (2) ask the user if the worktree is in active use, (3) NEVER force-remove or rm -rf a worktree without explicit user approval. Treat other agents' worktrees as sacred — they may contain hours of unsaved work.","expanded":"When working on this type of task, avoid: Force-removed a git worktree and deleted its branch without first checking if another agent had uncommitted work there. Then attempted rm -rf on the remnant directory containing that work.\n\nInstead: Before touching ANY existing worktree: (1) check for uncommitted changes with `git -C  status`, (2) ask the user if the worktree is in active use, (3) NEVER force-remove or rm -rf a worktree without explicit user approval. Treat other agents' worktrees as sacred — they may contain hours of unsaved work.","structured":{"avoid":"Force-removed a git worktree and deleted its branch without first checking if another agent had uncommitted work there. Then attempted rm -rf on the remnant directory containing that work.","prefer":"Before touching ANY existing worktree: (1) check for uncommitted changes with `git -C  status`, (2) ask the user if the worktree is in active use, (3) NEVER force-remove or rm -rf a worktree without explicit user approval. Treat other agents' worktrees as sacred — they may contain hours of unsaved work."},"tags":["git","worktree"]},"kind":"constraint","name":"learned/before-touching-any-existing-worktree-1-check-f","provenance":{"correction_id":"c-1770600349532208291","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.7400000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-65adff2ff17e","kind":"behavior","content":{"content":{"canonical":"Use responsive images with srcset and sizes attributes to serve appropriately sized images. Generate multiple resolutions at build time (thumbnail, small, medium, large). Use modern formats (WebP, AVIF) with fallbacks. Lazy load images below the fold with Intersection Observer. Serve images through CDN with automatic resizing and format conversion.","expanded":"When working on this type of task, avoid: Loading large images at full resolution regardless of display size, wasting bandwidth and memory on mobile devices\n\nInstead: Use responsive images with srcset and sizes attributes to serve appropriately sized images. Generate multiple resolutions at build time (thumbnail, small, medium, large). Use modern formats (WebP, AVIF) with fallbacks. Lazy load images below the fold with Intersection Observer. Serve images through CDN with automatic resizing and format conversion.","structured":{"avoid":"Loading large images at full resolution regardless of display size, wasting bandwidth and memory on mobile devices","prefer":"Use responsive images with srcset and sizes attributes to serve appropriately sized images. Generate multiple resolutions at build time (thumbnail, small, medium, large). Use modern formats (WebP, AVIF) with fallbacks. Lazy load images below the fold with Intersection Observer. Serve images through CDN with automatic resizing and format conversion."}},"kind":"preference","name":"learned/use-responsive-images-with-srcset-and-sizes-attrib","provenance":{"correction_id":"c-1770878011835913049","created_at":"2026-02-11T22:33:31.836028675-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-662fa8c3dfb6","kind":"behavior","content":{"content":{"canonical":"Use a temporary directory when testing commands that could write data, to avoid polluting the real store","expanded":"When working on this type of task, avoid: Attempted to run test commands that could modify the real floop store in the working directory\n\nInstead: Use a temporary directory when testing commands that could write data, to avoid polluting the real store","structured":{"avoid":"Attempted to run test commands that could modify the real floop store in the working directory","prefer":"Use a temporary directory when testing commands that could write data, to avoid polluting the real store"}},"kind":"constraint","name":"learned/use-a-temporary-directory-when-testing-commands-th","provenance":{"correction_id":"correction-1769828032","created_at":"2026-01-30T18:53:52.514014357-08:00","source_type":"learned"},"when":{"task":"testing"}},"metadata":{"confidence":0.66,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-671650d98ba8","kind":"behavior","content":{"content":{"canonical":"Define resource requests and limits for all containers (CPU, memory). Implement liveness probes to restart unhealthy pods and readiness probes to control traffic routing. Use rolling updates with appropriate maxSurge and maxUnavailable. Configure PodDisruptionBudgets to maintain availability during voluntary disruptions.","expanded":"When working on this type of task, avoid: Deploying to Kubernetes without resource limits or health checks, causing pods to consume all cluster resources or get killed unexpectedly\n\nInstead: Define resource requests and limits for all containers (CPU, memory). Implement liveness probes to restart unhealthy pods and readiness probes to control traffic routing. Use rolling updates with appropriate maxSurge and maxUnavailable. Configure PodDisruptionBudgets to maintain availability during voluntary disruptions.","structured":{"avoid":"Deploying to Kubernetes without resource limits or health checks, causing pods to consume all cluster resources or get killed unexpectedly","prefer":"Define resource requests and limits for all containers (CPU, memory). Implement liveness probes to restart unhealthy pods and readiness probes to control traffic routing. Use rolling updates with appropriate maxSurge and maxUnavailable. Configure PodDisruptionBudgets to maintain availability during voluntary disruptions."}},"kind":"preference","name":"learned/define-resource-requests-and-limits-for-all-contai","provenance":{"correction_id":"c-1770877828883523543","created_at":"2026-02-11T22:30:28.883678586-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-6af7cc0aced0","kind":"behavior","content":{"content":{"canonical":"Follow test pyramid: many fast unit tests (70%), fewer integration tests (20%), minimal E2E tests (10%). Unit tests should run in milliseconds without external dependencies. Integration tests verify component interactions with real dependencies. Reserve E2E tests for critical user journeys only.","expanded":"When working on this type of task, avoid: Writing mostly end-to-end tests without unit tests, creating slow, flaky test suites that are expensive to maintain and provide poor feedback\n\nInstead: Follow test pyramid: many fast unit tests (70%), fewer integration tests (20%), minimal E2E tests (10%). Unit tests should run in milliseconds without external dependencies. Integration tests verify component interactions with real dependencies. Reserve E2E tests for critical user journeys only.","structured":{"avoid":"Writing mostly end-to-end tests without unit tests, creating slow, flaky test suites that are expensive to maintain and provide poor feedback","prefer":"Follow test pyramid: many fast unit tests (70%), fewer integration tests (20%), minimal E2E tests (10%). Unit tests should run in milliseconds without external dependencies. Integration tests verify component interactions with real dependencies. Reserve E2E tests for critical user journeys only."}},"kind":"preference","name":"learned/follow-test-pyramid-many-fast-unit-tests-70-f","provenance":{"correction_id":"c-1770877838974695222","created_at":"2026-02-11T22:30:38.974759042-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-6b92109dd498","kind":"behavior","content":{"content":{"canonical":"When building systems that inject data into LLM prompts: ALWAYS sanitize stored content before prompt injection, NEVER use authoritative framing around user-controlled content, add input length limits on all parameters, implement rate limiting on write operations, add content policy filters for known injection patterns, distinguish trust levels (auto-learned vs human-reviewed), treat the learn-store-activate pipeline as a critical trust boundary requiring defense in depth.","expanded":"When working on this type of task, avoid: Did not consider that floop's core value proposition (persisting learned behaviors into agent system prompts) is simultaneously its biggest attack surface. Built the entire learn-store-activate pipeline with zero content sanitization, no input length limits, no rate limiting, and wrapped injected content with maximally authoritative framing.\n\nInstead: When building systems that inject data into LLM prompts: ALWAYS sanitize stored content before prompt injection, NEVER use authoritative framing around user-controlled content, add input length limits on all parameters, implement rate limiting on write operations, add content policy filters for known injection patterns, distinguish trust levels (auto-learned vs human-reviewed), treat the learn-store-activate pipeline as a critical trust boundary requiring defense in depth.","structured":{"avoid":"Did not consider that floop's core value proposition (persisting learned behaviors into agent system prompts) is simultaneously its biggest attack surface. Built the entire learn-store-activate pipeline with zero content sanitization, no input length limits, no rate limiting, and wrapped injected content with maximally authoritative framing.","prefer":"When building systems that inject data into LLM prompts: ALWAYS sanitize stored content before prompt injection, NEVER use authoritative framing around user-controlled content, add input length limits on all parameters, implement rate limiting on write operations, add content policy filters for known injection patterns, distinguish trust levels (auto-learned vs human-reviewed), treat the learn-store-activate pipeline as a critical trust boundary requiring defense in depth."}},"kind":"constraint","name":"learned/when-building-systems-that-inject-data-into-llm-pr","provenance":{"correction_id":"c-1770425637422572299","created_at":"2026-02-06T16:53:57.422616382-08:00","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.64,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-714d55f38be5","kind":"behavior","content":{"content":{"canonical":"When performing audits, exploration, or research that reveals project-level insights (gaps, patterns, architectural understanding), immediately capture findings via floop_learn. Any discovery that would be useful context for future sessions is a learning opportunity — not just corrections or mistakes. Proactive learning includes: audit findings, documentation gaps identified, architectural patterns discovered, feature inventory results.","expanded":"When working on this type of task, avoid: During a deep docs audit, discovered significant documentation gaps (no centralized CLI reference, missing integration guides for 6+ tools, no architecture docs, undocumented token optimization features) but failed to capture the findings as a floop learning. The exploration yielded clear insights about the project's documentation state that should persist across sessions.\n\nInstead: When performing audits, exploration, or research that reveals project-level insights (gaps, patterns, architectural understanding), immediately capture findings via floop_learn. Any discovery that would be useful context for future sessions is a learning opportunity — not just corrections or mistakes. Proactive learning includes: audit findings, documentation gaps identified, architectural patterns discovered, feature inventory results.","structured":{"avoid":"During a deep docs audit, discovered significant documentation gaps (no centralized CLI reference, missing integration guides for 6+ tools, no architecture docs, undocumented token optimization features) but failed to capture the findings as a floop learning. The exploration yielded clear insights about the project's documentation state that should persist across sessions.","prefer":"When performing audits, exploration, or research that reveals project-level insights (gaps, patterns, architectural understanding), immediately capture findings via floop_learn. Any discovery that would be useful context for future sessions is a learning opportunity — not just corrections or mistakes. Proactive learning includes: audit findings, documentation gaps identified, architectural patterns discovered, feature inventory results."},"tags":["correction","floop"]},"kind":"preference","name":"learned/when-performing-audits-exploration-or-research-t","provenance":{"correction_id":"c-1770702998768527690","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.7400000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-716289511fd3","kind":"behavior","content":{"content":{"canonical":"Implement weighted/points-based rate limiting where heavy operations (complex queries, large scans) consume more quota units than lightweight reads. This reflects true resource consumption and prevents expensive operation abuse.","expanded":"When working on this type of task, avoid: Treating all API operations equally when counting against rate limit quotas, allowing attackers to use cheap operations to discover expensive ones\n\nInstead: Implement weighted/points-based rate limiting where heavy operations (complex queries, large scans) consume more quota units than lightweight reads. This reflects true resource consumption and prevents expensive operation abuse.","structured":{"avoid":"Treating all API operations equally when counting against rate limit quotas, allowing attackers to use cheap operations to discover expensive ones","prefer":"Implement weighted/points-based rate limiting where heavy operations (complex queries, large scans) consume more quota units than lightweight reads. This reflects true resource consumption and prevents expensive operation abuse."}},"kind":"preference","name":"learned/implement-weighted-points-based-rate-limiting-wher","provenance":{"correction_id":"c-1770877413376037577","created_at":"2026-02-11T22:23:33.376167621-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-72d8958c6a6a","kind":"behavior","content":{"content":{"canonical":"Use probabilistic early expiration: randomly refresh cache entries slightly before actual expiration based on load. Implement request coalescing where concurrent requests for the same key wait for a single backend call. Use stale-while-revalidate pattern to serve stale data while refreshing in background.","expanded":"When working on this type of task, avoid: Allowing cache stampedes where many requests simultaneously try to regenerate expensive cache entries when they expire, overwhelming the backend\n\nInstead: Use probabilistic early expiration: randomly refresh cache entries slightly before actual expiration based on load. Implement request coalescing where concurrent requests for the same key wait for a single backend call. Use stale-while-revalidate pattern to serve stale data while refreshing in background.","structured":{"avoid":"Allowing cache stampedes where many requests simultaneously try to regenerate expensive cache entries when they expire, overwhelming the backend","prefer":"Use probabilistic early expiration: randomly refresh cache entries slightly before actual expiration based on load. Implement request coalescing where concurrent requests for the same key wait for a single backend call. Use stale-while-revalidate pattern to serve stale data while refreshing in background."}},"kind":"preference","name":"learned/use-probabilistic-early-expiration-randomly-refre","provenance":{"correction_id":"c-1770877665499456472","created_at":"2026-02-11T22:27:45.499597828-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-76e5757e480c","kind":"forgotten-behavior","content":{"content":{"canonical":"When using subagents for parallel work: (1) Have subagents do read-only tasks (research, exploration, planning), (2) Have the orchestrating agent do all file writes based on subagent findings, OR (3) Use worktrees but do the actual implementation work yourself after subagents gather context","expanded":"When working on this type of task, avoid: Spawned background subagents to write code in worktrees, but they failed with \"Permission to use Write has been auto-denied (prompts unavailable)\" because subagents cannot request interactive permissions\n\nInstead: When using subagents for parallel work: (1) Have subagents do read-only tasks (research, exploration, planning), (2) Have the orchestrating agent do all file writes based on subagent findings, OR (3) Use worktrees but do the actual implementation work yourself after subagents gather context","structured":{"avoid":"Spawned background subagents to write code in worktrees, but they failed with \"Permission to use Write has been auto-denied (prompts unavailable)\" because subagents cannot request interactive permissions","prefer":"When using subagents for parallel work: (1) Have subagents do read-only tasks (research, exploration, planning), (2) Have the orchestrating agent do all file writes based on subagent findings, OR (3) Use worktrees but do the actual implementation work yourself after subagents gather context"}},"kind":"preference","name":"learned/when-using-subagents-for-parallel-work-1-have-s","provenance":{"correction_id":"correction-1769790625","source_type":"learned"}},"metadata":{"confidence":0.68,"forget_reason":"Superseded by behavior-subagent-merged","forgotten_at":"2026-02-07T19:08:17-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-7869fac909c9","kind":"behavior","content":{"content":{"canonical":"Don't set explicit `go` version in .golangci.yml — golangci-lint reads it from go.mod automatically. Use golangci-lint-action@v7 (not v6) for golangci-lint v2.x configs. The v2 config format requires: `version: \"2\"` at top, `linters.settings` instead of `linters-settings`, `linters.exclusions.rules` instead of `issues.exclude-rules`, and `formatters.enable` for gofmt/goimports instead of putting them in linters.","expanded":"When working on this type of task, avoid: Set explicit `go: \"1.25\"` in .golangci.yml run config, which caused golangci-lint to reject analysis because its binary was compiled with Go 1.24. Also used golangci-lint-action@v6 which doesn't support golangci-lint v2.\n\nInstead: Don't set explicit `go` version in .golangci.yml — golangci-lint reads it from go.mod automatically. Use golangci-lint-action@v7 (not v6) for golangci-lint v2.x configs. The v2 config format requires: `version: \"2\"` at top, `linters.settings` instead of `linters-settings`, `linters.exclusions.rules` instead of `issues.exclude-rules`, and `formatters.enable` for gofmt/goimports instead of putting them in linters.","structured":{"avoid":"Set explicit `go: \"1.25\"` in .golangci.yml run config, which caused golangci-lint to reject analysis because its binary was compiled with Go 1.24. Also used golangci-lint-action@v6 which doesn't support golangci-lint v2.","prefer":"Don't set explicit `go` version in .golangci.yml — golangci-lint reads it from go.mod automatically. Use golangci-lint-action@v7 (not v6) for golangci-lint v2.x configs. The v2 config format requires: `version: \"2\"` at top, `linters.settings` instead of `linters-settings`, `linters.exclusions.rules` instead of `issues.exclude-rules`, and `formatters.enable` for gofmt/goimports instead of putting them in linters."}},"kind":"constraint","name":"learned/dont-set-explicit-go-version-in-golangci-yml-","provenance":{"correction_id":"c-1770518309782108274","created_at":"2026-02-07T18:38:29.782254428-08:00","source_type":"learned"},"when":{"environment":"development","language":"yaml","task":"development"}},"metadata":{"confidence":0.62,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-787774a4ca6d","kind":"behavior","content":{"content":{"canonical":"Use layer masks to filter physics queries and reduce collision matrix checks. Implement object pooling for projectiles and effects to avoid constant instantiation. Use FixedUpdate for physics operations, not Update. Cache raycast results and throttle checks (every N frames). Consider simplified collision shapes (sphere, capsule) instead of mesh colliders.","expanded":"When working on this type of task, avoid: Using physics raycasts and overlap checks in Update without caching or limiting frequency, causing physics system to become bottleneck\n\nInstead: Use layer masks to filter physics queries and reduce collision matrix checks. Implement object pooling for projectiles and effects to avoid constant instantiation. Use FixedUpdate for physics operations, not Update. Cache raycast results and throttle checks (every N frames). Consider simplified collision shapes (sphere, capsule) instead of mesh colliders.","structured":{"avoid":"Using physics raycasts and overlap checks in Update without caching or limiting frequency, causing physics system to become bottleneck","prefer":"Use layer masks to filter physics queries and reduce collision matrix checks. Implement object pooling for projectiles and effects to avoid constant instantiation. Use FixedUpdate for physics operations, not Update. Cache raycast results and throttle checks (every N frames). Consider simplified collision shapes (sphere, capsule) instead of mesh colliders."}},"kind":"constraint","name":"learned/use-layer-masks-to-filter-physics-queries-and-redu","provenance":{"correction_id":"c-1770877968743456075","created_at":"2026-02-11T22:32:48.743577211-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-7a098cb05216","kind":"forgotten-behavior","content":{"content":{"canonical":"Always create detailed beads (epic + tasks) with dependency graphs after planning - enables easy handoff between agents and sessions","expanded":"When working on this type of task, avoid: Jumped directly into implementation after planning without creating beads structure\n\nInstead: Always create detailed beads (epic + tasks) with dependency graphs after planning - enables easy handoff between agents and sessions","structured":{"avoid":"Jumped directly into implementation after planning without creating beads structure","prefer":"Always create detailed beads (epic + tasks) with dependency graphs after planning - enables easy handoff between agents and sessions"}},"kind":"directive","name":"learned/always-create-detailed-beads-epic-+-tasks-with-d","provenance":{"correction_id":"c-1769580766063884670","source_type":"learned"}},"metadata":{"confidence":0.64,"forget_reason":"Superseded by behavior-beads-merged","forgotten_at":"2026-02-07T19:08:27-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"local","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-7b7e3230c9cc","kind":"behavior","content":{"content":{"canonical":"Implement cursor-based pagination for real-time data (use opaque cursor, not offset). Use offset pagination for static datasets with total count. Set reasonable default and maximum page sizes (e.g., default 20, max 100). Return pagination metadata in response (next_cursor, has_more, total). Use Link headers for RESTful pagination.","expanded":"When working on this type of task, avoid: Returning all results from API endpoints without pagination, causing timeouts and memory issues with large datasets\n\nInstead: Implement cursor-based pagination for real-time data (use opaque cursor, not offset). Use offset pagination for static datasets with total count. Set reasonable default and maximum page sizes (e.g., default 20, max 100). Return pagination metadata in response (next_cursor, has_more, total). Use Link headers for RESTful pagination.","structured":{"avoid":"Returning all results from API endpoints without pagination, causing timeouts and memory issues with large datasets","prefer":"Implement cursor-based pagination for real-time data (use opaque cursor, not offset). Use offset pagination for static datasets with total count. Set reasonable default and maximum page sizes (e.g., default 20, max 100). Return pagination metadata in response (next_cursor, has_more, total). Use Link headers for RESTful pagination."}},"kind":"preference","name":"learned/implement-cursor-based-pagination-for-real-time-da","provenance":{"correction_id":"c-1770878034190805664","created_at":"2026-02-11T22:33:54.190873261-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-7dbcfc68edbd","kind":"behavior","content":{"content":{"canonical":"With floop, I DO have persistent memories. Behaviors stored in .floop/ are learned from past corrections and loaded at session start via floop_active. When asked about memories, acknowledge that floop provides cross-session memory through learned behaviors.","expanded":"When working on this type of task, avoid: Said \"I don't have persistent memories between sessions\" when floop behaviors ARE persistent memories - they capture corrections from past sessions and influence future behavior\n\nInstead: With floop, I DO have persistent memories. Behaviors stored in .floop/ are learned from past corrections and loaded at session start via floop_active. When asked about memories, acknowledge that floop provides cross-session memory through learned behaviors.","structured":{"avoid":"Said \"I don't have persistent memories between sessions\" when floop behaviors ARE persistent memories - they capture corrections from past sessions and influence future behavior","prefer":"With floop, I DO have persistent memories. Behaviors stored in .floop/ are learned from past corrections and loaded at session start via floop_active. When asked about memories, acknowledge that floop provides cross-session memory through learned behaviors."},"tags":["behavior","correction","floop"]},"kind":"directive","name":"learned/with-floop-i-do-have-persistent-memories-behavio","provenance":{"correction_id":"correction-1769914118","source_type":"learned"}},"metadata":{"confidence":0.8600000000000002,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-7f90e2a932c3","kind":"behavior","content":{"content":{"canonical":"Use token bucket algorithm for rate limiting that accumulates tokens during quiet periods and caps total burst size. This prevents DoS by limiting burst magnitude while maintaining steady-state rates, offering better protection against traffic spikes.","expanded":"When working on this type of task, avoid: Using fixed window counters for rate limiting in bursty traffic patterns, which can allow 2x the intended rate at window boundaries (100 requests at 00:59 + 100 at 01:01 = 200 in 2 seconds)\n\nInstead: Use token bucket algorithm for rate limiting that accumulates tokens during quiet periods and caps total burst size. This prevents DoS by limiting burst magnitude while maintaining steady-state rates, offering better protection against traffic spikes.","structured":{"avoid":"Using fixed window counters for rate limiting in bursty traffic patterns, which can allow 2x the intended rate at window boundaries (100 requests at 00:59 + 100 at 01:01 = 200 in 2 seconds)","prefer":"Use token bucket algorithm for rate limiting that accumulates tokens during quiet periods and caps total burst size. This prevents DoS by limiting burst magnitude while maintaining steady-state rates, offering better protection against traffic spikes."}},"kind":"preference","name":"learned/use-token-bucket-algorithm-for-rate-limiting-that","provenance":{"correction_id":"c-1770877408027199625","created_at":"2026-02-11T22:23:28.027412454-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-7f9616565db1","kind":"behavior","content":{"content":{"canonical":"Use experiment tracking tools (MLflow, Weights \u0026 Biases) to log hyperparameters, metrics, and artifacts for every run. Version datasets and track data splits. Save model checkpoints with metadata. Tag experiments with descriptions and link to code commits. This enables reproducibility, comparison, and collaboration.","expanded":"When working on this type of task, avoid: Running ML experiments without tracking hyperparameters, metrics, or model versions, making it impossible to reproduce results or understand what worked\n\nInstead: Use experiment tracking tools (MLflow, Weights \u0026 Biases) to log hyperparameters, metrics, and artifacts for every run. Version datasets and track data splits. Save model checkpoints with metadata. Tag experiments with descriptions and link to code commits. This enables reproducibility, comparison, and collaboration.","structured":{"avoid":"Running ML experiments without tracking hyperparameters, metrics, or model versions, making it impossible to reproduce results or understand what worked","prefer":"Use experiment tracking tools (MLflow, Weights \u0026 Biases) to log hyperparameters, metrics, and artifacts for every run. Version datasets and track data splits. Save model checkpoints with metadata. Tag experiments with descriptions and link to code commits. This enables reproducibility, comparison, and collaboration."}},"kind":"preference","name":"learned/use-experiment-tracking-tools-mlflow-weights-b","provenance":{"correction_id":"c-1770877941317304020","created_at":"2026-02-11T22:32:21.317860902-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-818bd38de0c2","kind":"behavior","content":{"content":{"canonical":"Always follow TDD workflow: RED (write failing test first) → GREEN (implement minimal fix to pass) → REFACTOR (clean up implementation, requires review). Never skip the failing test step - it validates the test actually catches the problem. Refactor phase is for cleaning up code quality, not adding features.","expanded":"When working on this type of task, avoid: Implementing features or fixes without writing a failing test first, or skipping the refactor phase\n\nInstead: Always follow TDD workflow: RED (write failing test first) → GREEN (implement minimal fix to pass) → REFACTOR (clean up implementation, requires review). Never skip the failing test step - it validates the test actually catches the problem. Refactor phase is for cleaning up code quality, not adding features.","structured":{"avoid":"Implementing features or fixes without writing a failing test first, or skipping the refactor phase","prefer":"Always follow TDD workflow: RED (write failing test first) → GREEN (implement minimal fix to pass) → REFACTOR (clean up implementation, requires review). Never skip the failing test step - it validates the test actually catches the problem. Refactor phase is for cleaning up code quality, not adding features."},"tags":["refactoring","tdd","testing","workflow"]},"kind":"constraint","name":"learned/always-follow-tdd-workflow-red-write-failing-tes","provenance":{"correction_id":"correction-1769974263","source_type":"learned"}},"metadata":{"confidence":0.95,"last_merge_at":"2026-02-07T19:10:05-08:00","merged_from":["behavior-5d3f7201e10d","behavior-c5bf127ed6aa"],"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-81e201973717","kind":"behavior","content":{"content":{"canonical":"Create a dedicated usage guide (docs/FLOOP_USAGE.md) and reference it prominently in AGENTS.md - separates 'what' from 'how' and makes instructions comprehensive","expanded":"When working on this type of task, avoid: Created inline usage instructions in AGENTS.md\n\nInstead: Create a dedicated usage guide (docs/FLOOP_USAGE.md) and reference it prominently in AGENTS.md - separates 'what' from 'how' and makes instructions comprehensive","structured":{"avoid":"Created inline usage instructions in AGENTS.md","prefer":"Create a dedicated usage guide (docs/FLOOP_USAGE.md) and reference it prominently in AGENTS.md - separates 'what' from 'how' and makes instructions comprehensive"}},"kind":"directive","name":"learned/create-a-dedicated-usage-guide-docs-floop_usage-m","provenance":{"correction_id":"c-1769578257801753560","created_at":"2026-01-27T21:30:57.802170973-08:00","source_type":"learned"},"when":{"language":"markdown"}},"metadata":{"confidence":0.66,"priority":0,"scope":"local","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-8341f0d52ce6","kind":"behavior","content":{"content":{"canonical":"MCP go-sdk expects jsonschema:\"Description text\" format without key=value syntax. The tag value is directly the description.","expanded":"When working on this type of task, avoid: Subagent used jsonschema:\"description=...\" tag format which caused MCP SDK panic\n\nInstead: MCP go-sdk expects jsonschema:\"Description text\" format without key=value syntax. The tag value is directly the description.","structured":{"avoid":"Subagent used jsonschema:\"description=...\" tag format which caused MCP SDK panic","prefer":"MCP go-sdk expects jsonschema:\"Description text\" format without key=value syntax. The tag value is directly the description."}},"kind":"directive","name":"learned/mcp-go-sdk-expects-jsonschemadescription-text-f","provenance":{"correction_id":"correction-1769817031","created_at":"2026-01-30T15:50:31.088121234-08:00","source_type":"learned"},"when":{"file_path":"mcp/*","language":"go"}},"metadata":{"confidence":0.66,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-84a379605b43","kind":"forgotten-behavior","content":{"content":{"canonical":"Create git worktrees in .worktrees/ directory at repo root (e.g., 'git worktree add .worktrees/feature-name -b branch-name'). This directory is in .gitignore so it won't pollute git status, and subagents can access it since it's inside the repo.","expanded":"When working on this type of task, avoid: Created git worktrees as sibling directories (../floop-*) which subagents couldn't access due to sandbox restrictions.\n\nInstead: Create git worktrees in .worktrees/ directory at repo root (e.g., 'git worktree add .worktrees/feature-name -b branch-name'). This directory is in .gitignore so it won't pollute git status, and subagents can access it since it's inside the repo.","structured":{"avoid":"Created git worktrees as sibling directories (../floop-*) which subagents couldn't access due to sandbox restrictions.","prefer":"Create git worktrees in .worktrees/ directory at repo root (e.g., 'git worktree add .worktrees/feature-name -b branch-name'). This directory is in .gitignore so it won't pollute git status, and subagents can access it since it's inside the repo."}},"kind":"directive","name":"learned/create-git-worktrees-in-worktrees-directory-at-r","provenance":{"correction_id":"correction-1769746975","source_type":"learned"}},"metadata":{"confidence":0.68,"forget_reason":"Superseded by behavior-worktrees-merged","forgotten_at":"2026-02-07T19:08:14-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-8531ca06d020","kind":"behavior","content":{"content":{"canonical":"Always validate file paths from MCP tool inputs: filepath.Clean the path, verify it resolves within allowed directories, reject paths containing dotdot, reject absolute paths outside allowed dirs. Treat all MCP tool parameters as untrusted input from potentially compromised LLMs.","expanded":"When working on this type of task, avoid: Accepted arbitrary filesystem paths from MCP tool parameters (floop_backup output_path, floop_restore input_path) without any path validation.\n\nInstead: Always validate file paths from MCP tool inputs: filepath.Clean the path, verify it resolves within allowed directories, reject paths containing dotdot, reject absolute paths outside allowed dirs. Treat all MCP tool parameters as untrusted input from potentially compromised LLMs.","structured":{"avoid":"Accepted arbitrary filesystem paths from MCP tool parameters (floop_backup output_path, floop_restore input_path) without any path validation.","prefer":"Always validate file paths from MCP tool inputs: filepath.Clean the path, verify it resolves within allowed directories, reject paths containing dotdot, reject absolute paths outside allowed dirs. Treat all MCP tool parameters as untrusted input from potentially compromised LLMs."}},"kind":"directive","name":"learned/always-validate-file-paths-from-mcp-tool-inputs-f","provenance":{"correction_id":"c-1770425642015674421","created_at":"2026-02-06T16:54:02.01572191-08:00","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.64,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-8845135841be","kind":"behavior","content":{"conflicts":null,"content":{"canonical":"Always run `$(go env GOPATH)/bin/golangci-lint run ./...` or verify with `golangci-lint version` that it shows v2.8.0 (matching CI). Better yet, use `make lint` (once the Makefile target exists) to ensure the correct binary. Before submitting PRs, the lint step MUST use the CI-matching version.","expanded":"When working on this type of task, avoid: Running `golangci-lint run ./...` without verifying which binary is being used. The system-installed dev build at /usr/bin/golangci-lint silently passes formatting checks that CI's v2.8.0 catches (extra blank lines). This caused gofmt failures on main.\n\nInstead: Always run `$(go env GOPATH)/bin/golangci-lint run ./...` or verify with `golangci-lint version` that it shows v2.8.0 (matching CI). Better yet, use `make lint` (once the Makefile target exists) to ensure the correct binary. Before submitting PRs, the lint step MUST use the CI-matching version.","structured":{"avoid":"Running `golangci-lint run ./...` without verifying which binary is being used. The system-installed dev build at /usr/bin/golangci-lint silently passes formatting checks that CI's v2.8.0 catches (extra blank lines). This caused gofmt failures on main.","prefer":"Always run `$(go env GOPATH)/bin/golangci-lint run ./...` or verify with `golangci-lint version` that it shows v2.8.0 (matching CI). Better yet, use `make lint` (once the Makefile target exists) to ensure the correct binary. Before submitting PRs, the lint step MUST use the CI-matching version."},"tags":["ci","configuration","go","linting","make"]},"kind":"preference","name":"learned/always-run-go-env-gopath-bin-golangci-lint-run","overrides":null,"provenance":{"correction_id":"c-1771069981875950931","created_at":"2026-02-14T03:53:01.877681788-08:00","source_type":"learned"},"requires":null,"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T03:53:02-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T03:53:02-08:00"}}}
{"id":"behavior-889e959ff49f","kind":"behavior","content":{"content":{"canonical":"Always run `make lint` (or `golangci-lint run --timeout=5m`) as the quality gate before committing. The project has `.golangci.yml` with gosec, errcheck, staticcheck, unused, misspell, goconst enabled. `go vet` alone is insufficient — the full `make ci` target runs: fmt-check, lint, vet, test, build.","expanded":"When working on this type of task, avoid: Only ran `go fmt` and `go vet` before submitting PRs, missing golangci-lint which includes errcheck, staticcheck, unused, gosec, misspell, goconst and goimports formatting.\n\nInstead: Always run `make lint` (or `golangci-lint run --timeout=5m`) as the quality gate before committing. The project has `.golangci.yml` with gosec, errcheck, staticcheck, unused, misspell, goconst enabled. `go vet` alone is insufficient — the full `make ci` target runs: fmt-check, lint, vet, test, build.","structured":{"avoid":"Only ran `go fmt` and `go vet` before submitting PRs, missing golangci-lint which includes errcheck, staticcheck, unused, gosec, misspell, goconst and goimports formatting.","prefer":"Always run `make lint` (or `golangci-lint run --timeout=5m`) as the quality gate before committing. The project has `.golangci.yml` with gosec, errcheck, staticcheck, unused, misspell, goconst enabled. `go vet` alone is insufficient — the full `make ci` target runs: fmt-check, lint, vet, test, build."},"tags":["ci","go","linting","make","testing","yaml"]},"kind":"preference","name":"learned/always-run-make-lint-or-golangci-lint-run-ti","provenance":{"correction_id":"c-1770747190484779248","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.7000000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-89ac1cac9e9a","kind":"behavior","content":{"content":{"canonical":"Use asynchronous messaging (message queues, event streams) for non-critical paths to decouple services. Implement circuit breakers and timeouts for synchronous calls. Cache frequently accessed data locally with event-based invalidation. Consider CQRS to separate read/write paths and reduce synchronous dependencies.","expanded":"When working on this type of task, avoid: Making synchronous HTTP calls between microservices for every operation, creating tight coupling and cascading failures when services are down\n\nInstead: Use asynchronous messaging (message queues, event streams) for non-critical paths to decouple services. Implement circuit breakers and timeouts for synchronous calls. Cache frequently accessed data locally with event-based invalidation. Consider CQRS to separate read/write paths and reduce synchronous dependencies.","structured":{"avoid":"Making synchronous HTTP calls between microservices for every operation, creating tight coupling and cascading failures when services are down","prefer":"Use asynchronous messaging (message queues, event streams) for non-critical paths to decouple services. Implement circuit breakers and timeouts for synchronous calls. Cache frequently accessed data locally with event-based invalidation. Consider CQRS to separate read/write paths and reduce synchronous dependencies."}},"kind":"preference","name":"learned/use-asynchronous-messaging-message-queues-event","provenance":{"correction_id":"c-1770877681068273890","created_at":"2026-02-11T22:28:01.068413813-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-8bd221b22d37","kind":"behavior","content":{"content":{"canonical":"Use floop MCP tools (floop_active, floop_learn, floop_list) instead of bash commands for all floop interactions. This provides better integration and type safety. Note: floop_active has a bug with null 'when' fields (feedback-loop-0gh) that needs fixing first.","expanded":"When working on this type of task, avoid: Used bash command './floop prompt --task development' to get active behaviors at session start\n\nInstead: Use floop MCP tools (floop_active, floop_learn, floop_list) instead of bash commands for all floop interactions. This provides better integration and type safety. Note: floop_active has a bug with null 'when' fields (feedback-loop-0gh) that needs fixing first.","structured":{"avoid":"Used bash command './floop prompt --task development' to get active behaviors at session start","prefer":"Use floop MCP tools (floop_active, floop_learn, floop_list) instead of bash commands for all floop interactions. This provides better integration and type safety. Note: floop_active has a bug with null 'when' fields (feedback-loop-0gh) that needs fixing first."}},"kind":"procedure","name":"learned/use-floop-mcp-tools-floop_active-floop_learn-fl","provenance":{"correction_id":"correction-1769790638","created_at":"2026-01-30T08:30:38.332669915-08:00","source_type":"learned"}},"metadata":{"confidence":0.7200000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-8f2096925759","kind":"behavior","content":{"content":{"canonical":"For parallel refactoring work: 1) Create separate git worktrees per agent (git worktree add worktrees/agent-a -b agent-a-branch), 2) Each agent works in its assigned worktree, 3) Merge branches sequentially after all agents complete, 4) Run full test suite after each merge to catch integration issues early.","expanded":"When working on this type of task, avoid: Running parallel subagents in the same git worktree, causing race conditions on git operations\n\nInstead: For parallel refactoring work: 1) Create separate git worktrees per agent (git worktree add worktrees/agent-a -b agent-a-branch), 2) Each agent works in its assigned worktree, 3) Merge branches sequentially after all agents complete, 4) Run full test suite after each merge to catch integration issues early.","structured":{"avoid":"Running parallel subagents in the same git worktree, causing race conditions on git operations","prefer":"For parallel refactoring work: 1) Create separate git worktrees per agent (git worktree add worktrees/agent-a -b agent-a-branch), 2) Each agent works in its assigned worktree, 3) Merge branches sequentially after all agents complete, 4) Run full test suite after each merge to catch integration issues early."},"tags":["git","refactoring","testing","worktree"]},"kind":"directive","name":"learned/for-parallel-refactoring-work-1-create-separate","provenance":{"correction_id":"c-1770186286824238742","source_type":"learned"},"when":{"task":"refactoring"}},"metadata":{"confidence":0.57,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:54-08:00","times_activated":1,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-91819ae1aa27","kind":"behavior","content":{"content":{"canonical":"Bad learnings are corrected by learning again, better — not by editing. The spreading activation system mirrors human memory: you can't edit a memory, but you can form a stronger one that outcompetes it. Broader learnings activate in more contexts, win token budget competition, and the narrow/wrong ones naturally go dormant. Trust the system to self-correct through more learning, not surgical intervention.","expanded":"When working on this type of task, avoid: Considered adding a 'floop refine' or 'floop edit' command to surgically fix bad learnings, which fights the human memory metaphor the system is built on.\n\nInstead: Bad learnings are corrected by learning again, better — not by editing. The spreading activation system mirrors human memory: you can't edit a memory, but you can form a stronger one that outcompetes it. Broader learnings activate in more contexts, win token budget competition, and the narrow/wrong ones naturally go dormant. Trust the system to self-correct through more learning, not surgical intervention.","structured":{"avoid":"Considered adding a 'floop refine' or 'floop edit' command to surgically fix bad learnings, which fights the human memory metaphor the system is built on.","prefer":"Bad learnings are corrected by learning again, better — not by editing. The spreading activation system mirrors human memory: you can't edit a memory, but you can form a stronger one that outcompetes it. Broader learnings activate in more contexts, win token budget competition, and the narrow/wrong ones naturally go dormant. Trust the system to self-correct through more learning, not surgical intervention."},"tags":["go","spreading-activation"]},"kind":"directive","name":"learned/bad-learnings-are-corrected-by-learning-again-bet","provenance":{"correction_id":"c-1770704353415650816","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.7200000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-91ce7e630185","kind":"behavior","content":{"content":{"canonical":"Use dedicated secrets management (HashiCorp Vault, AWS Secrets Manager, Kubernetes Secrets). Inject secrets at runtime, not build time. Rotate secrets regularly with automated processes. Use different credentials per environment. Scan repositories for leaked secrets with tools like git-secrets or truffleHog.","expanded":"When working on this type of task, avoid: Storing secrets in code, environment variables, or config files in version control, exposing credentials to all developers and git history\n\nInstead: Use dedicated secrets management (HashiCorp Vault, AWS Secrets Manager, Kubernetes Secrets). Inject secrets at runtime, not build time. Rotate secrets regularly with automated processes. Use different credentials per environment. Scan repositories for leaked secrets with tools like git-secrets or truffleHog.","structured":{"avoid":"Storing secrets in code, environment variables, or config files in version control, exposing credentials to all developers and git history","prefer":"Use dedicated secrets management (HashiCorp Vault, AWS Secrets Manager, Kubernetes Secrets). Inject secrets at runtime, not build time. Rotate secrets regularly with automated processes. Use different credentials per environment. Scan repositories for leaked secrets with tools like git-secrets or truffleHog."}},"kind":"procedure","name":"learned/use-dedicated-secrets-management-hashicorp-vault","provenance":{"correction_id":"c-1770878061948128135","created_at":"2026-02-11T22:34:21.948219507-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-92f329cee80d","kind":"behavior","content":{"content":{"canonical":"Include rate limit headers in all API responses: X-RateLimit-Limit (total quota), X-RateLimit-Remaining (quota left), X-RateLimit-Reset (timestamp when quota resets). Return Retry-After header with 429 responses. Provide different rate limits per authentication tier (free/paid) and communicate limits in API documentation.","expanded":"When working on this type of task, avoid: Implementing rate limiting that returns generic 429 errors without telling clients when they can retry or how much quota remains\n\nInstead: Include rate limit headers in all API responses: X-RateLimit-Limit (total quota), X-RateLimit-Remaining (quota left), X-RateLimit-Reset (timestamp when quota resets). Return Retry-After header with 429 responses. Provide different rate limits per authentication tier (free/paid) and communicate limits in API documentation.","structured":{"avoid":"Implementing rate limiting that returns generic 429 errors without telling clients when they can retry or how much quota remains","prefer":"Include rate limit headers in all API responses: X-RateLimit-Limit (total quota), X-RateLimit-Remaining (quota left), X-RateLimit-Reset (timestamp when quota resets). Return Retry-After header with 429 responses. Provide different rate limits per authentication tier (free/paid) and communicate limits in API documentation."}},"kind":"procedure","name":"learned/include-rate-limit-headers-in-all-api-responses-x","provenance":{"correction_id":"c-1770877783468662583","created_at":"2026-02-11T22:29:43.468826211-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-94a1cdcf9759","kind":"behavior","content":{"content":{"canonical":"Move calculations to vertex shader when possible (mobile GPUs are often vertex-bound). Use texture lookups instead of expensive math operations. Reduce texture samples and avoid dependent texture reads. Use simpler color spaces (RGB instead of HSV). Profile on actual devices with Unity Frame Debugger to identify GPU bottlenecks.","expanded":"When working on this type of task, avoid: Writing complex pixel shaders without considering mobile GPU limitations, causing frame rate drops on target devices\n\nInstead: Move calculations to vertex shader when possible (mobile GPUs are often vertex-bound). Use texture lookups instead of expensive math operations. Reduce texture samples and avoid dependent texture reads. Use simpler color spaces (RGB instead of HSV). Profile on actual devices with Unity Frame Debugger to identify GPU bottlenecks.","structured":{"avoid":"Writing complex pixel shaders without considering mobile GPU limitations, causing frame rate drops on target devices","prefer":"Move calculations to vertex shader when possible (mobile GPUs are often vertex-bound). Use texture lookups instead of expensive math operations. Reduce texture samples and avoid dependent texture reads. Use simpler color spaces (RGB instead of HSV). Profile on actual devices with Unity Frame Debugger to identify GPU bottlenecks."}},"kind":"constraint","name":"learned/move-calculations-to-vertex-shader-when-possible","provenance":{"correction_id":"c-1770877963093940126","created_at":"2026-02-11T22:32:43.094061714-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-95d6b9f414e4","kind":"forgotten-behavior","content":{"content":{"canonical":"Global scope is for agent's personal preferences/style across ALL work. Local scope is for team/project-specific conventions. Both have distinct value.","expanded":"When working on this type of task, avoid: Assumed one storage location would be enough\n\nInstead: Global scope is for agent's personal preferences/style across ALL work. Local scope is for team/project-specific conventions. Both have distinct value.","structured":{"avoid":"Assumed one storage location would be enough","prefer":"Global scope is for agent's personal preferences/style across ALL work. Local scope is for team/project-specific conventions. Both have distinct value."}},"kind":"preference","name":"learned/global-scope-is-for-agents-personal-preferences-s","provenance":{"correction_id":"c-1769577671639472329","source_type":"learned"},"when":{"task":"architecture"}},"metadata":{"confidence":0.62,"forget_reason":"Superseded by behavior-floop-scope-merged","forgotten_at":"2026-02-07T19:08:32-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"local","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-967fd13bd390","kind":"forgotten-behavior","content":{"content":{"canonical":"Use scope=both to save important learnings to both local and global","expanded":"When working on this type of task, avoid: Only saved to one store\n\nInstead: Use scope=both to save important learnings to both local and global","structured":{"avoid":"Only saved to one store","prefer":"Use scope=both to save important learnings to both local and global"}},"kind":"preference","name":"learned/use-scope=both-to-save-important-learnings-to-both","provenance":{"correction_id":"c-1769616836888545203","source_type":"learned"},"when":{"task":"configuration"}},"metadata":{"confidence":0.62,"forget_reason":"Superseded by behavior-floop-scope-merged","forgotten_at":"2026-02-07T19:08:32-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-96a14bcf82a8","kind":"behavior","content":{"content":{"canonical":"User's repos have auto-delete branch on PR merge. After merge: (1) remove worktree with `git worktree remove`, (2) prune local tracking branches with `git fetch --prune`, (3) local branches that tracked deleted remotes can be cleaned with `git branch -d`. Include a cleanup phase in all plans that use worktrees.","expanded":"When working on this type of task, avoid: Not accounting for post-PR-merge cleanup in plans. Leaving worktrees, local branches, and stale state around.\n\nInstead: User's repos have auto-delete branch on PR merge. After merge: (1) remove worktree with `git worktree remove`, (2) prune local tracking branches with `git fetch --prune`, (3) local branches that tracked deleted remotes can be cleaned with `git branch -d`. Include a cleanup phase in all plans that use worktrees.","structured":{"avoid":"Not accounting for post-PR-merge cleanup in plans. Leaving worktrees, local branches, and stale state around.","prefer":"User's repos have auto-delete branch on PR merge. After merge: (1) remove worktree with `git worktree remove`, (2) prune local tracking branches with `git fetch --prune`, (3) local branches that tracked deleted remotes can be cleaned with `git branch -d`. Include a cleanup phase in all plans that use worktrees."},"tags":["git","pr","worktree"]},"kind":"preference","name":"learned/users-repos-have-auto-delete-branch-on-pr-merge","provenance":{"correction_id":"c-1770701941333421062","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.7400000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-98e0567ab69e","kind":"behavior","content":{"content":{"canonical":"Cache component references in Awake/Start methods and reuse them. Use dependency injection or constructor assignment for required dependencies. For frequent lookups, maintain dictionaries keyed by ID. Profile with Unity Profiler to identify expensive CPU spikes from component lookups.","expanded":"When working on this type of task, avoid: Using GameObject.Find or GetComponent in Update loops in Unity, causing expensive search operations every frame\n\nInstead: Cache component references in Awake/Start methods and reuse them. Use dependency injection or constructor assignment for required dependencies. For frequent lookups, maintain dictionaries keyed by ID. Profile with Unity Profiler to identify expensive CPU spikes from component lookups.","structured":{"avoid":"Using GameObject.Find or GetComponent in Update loops in Unity, causing expensive search operations every frame","prefer":"Cache component references in Awake/Start methods and reuse them. Use dependency injection or constructor assignment for required dependencies. For frequent lookups, maintain dictionaries keyed by ID. Profile with Unity Profiler to identify expensive CPU spikes from component lookups."}},"kind":"preference","name":"learned/cache-component-references-in-awake-start-methods","provenance":{"correction_id":"c-1770877952572873027","created_at":"2026-02-11T22:32:32.572978184-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-9a143263a02e","kind":"behavior","content":{"content":{"canonical":"Use exponential backoff with jitter for retries: delay = base_delay * (2 ** attempt) + random_jitter. Set maximum retry attempts and total timeout. Only retry transient errors (network timeouts, 5xx status codes), not client errors (4xx). Implement circuit breakers to stop retrying when service is consistently failing.","expanded":"When working on this type of task, avoid: Implementing retry logic without exponential backoff or jitter, causing retry storms that amplify outages\n\nInstead: Use exponential backoff with jitter for retries: delay = base_delay * (2 ** attempt) + random_jitter. Set maximum retry attempts and total timeout. Only retry transient errors (network timeouts, 5xx status codes), not client errors (4xx). Implement circuit breakers to stop retrying when service is consistently failing.","structured":{"avoid":"Implementing retry logic without exponential backoff or jitter, causing retry storms that amplify outages","prefer":"Use exponential backoff with jitter for retries: delay = base_delay * (2 ** attempt) + random_jitter. Set maximum retry attempts and total timeout. Only retry transient errors (network timeouts, 5xx status codes), not client errors (4xx). Implement circuit breakers to stop retrying when service is consistently failing."}},"kind":"constraint","name":"learned/use-exponential-backoff-with-jitter-for-retries-d","provenance":{"correction_id":"c-1770878006701761921","created_at":"2026-02-11T22:33:26.701875203-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-9c3d12c74c61","kind":"behavior","content":{"content":{"canonical":"Expand tag extraction dictionary with: d.add(\"documentation\", \"documentation\", \"docs\", \"guide\", \"readme\"), d.add(\"subagent\", \"subagent\", \"subagents\", \"agent\", \"agents\", \"orchestrator\"), d.add(\"permissions\", \"permissions\", \"permission\", \"allow\", \"deny\"). Map \"parallel\" to existing \"concurrency\" tag, \"scope\" to \"configuration\", \"hooks\" to \"workflow\". Then run `floop tags backfill` to achieve 100% tag coverage.","expanded":"When working on this type of task, avoid: Tag extraction dictionary in internal/tagging/dictionary.go is missing keywords that appear in behavior content: \"documentation\"/\"docs\"/\"guide\" (3 untagged behaviors mention this), \"subagent\"/\"subagents\"/\"agents\" (9 mentions), \"permissions\"/\"permission\" (6 mentions), \"parallel\"/\"parallelization\" (4 mentions), \"scope\" (3 mentions), \"hooks\" (3 mentions). This leaves 5% of behaviors untagged.\n\nInstead: Expand tag extraction dictionary with: d.add(\"documentation\", \"documentation\", \"docs\", \"guide\", \"readme\"), d.add(\"subagent\", \"subagent\", \"subagents\", \"agent\", \"agents\", \"orchestrator\"), d.add(\"permissions\", \"permissions\", \"permission\", \"allow\", \"deny\"). Map \"parallel\" to existing \"concurrency\" tag, \"scope\" to \"configuration\", \"hooks\" to \"workflow\". Then run `floop tags backfill` to achieve 100% tag coverage.","structured":{"avoid":"Tag extraction dictionary in internal/tagging/dictionary.go is missing keywords that appear in behavior content: \"documentation\"/\"docs\"/\"guide\" (3 untagged behaviors mention this), \"subagent\"/\"subagents\"/\"agents\" (9 mentions), \"permissions\"/\"permission\" (6 mentions), \"parallel\"/\"parallelization\" (4 mentions), \"scope\" (3 mentions), \"hooks\" (3 mentions). This leaves 5% of behaviors untagged.","prefer":"Expand tag extraction dictionary with: d.add(\"documentation\", \"documentation\", \"docs\", \"guide\", \"readme\"), d.add(\"subagent\", \"subagent\", \"subagents\", \"agent\", \"agents\", \"orchestrator\"), d.add(\"permissions\", \"permissions\", \"permission\", \"allow\", \"deny\"). Map \"parallel\" to existing \"concurrency\" tag, \"scope\" to \"configuration\", \"hooks\" to \"workflow\". Then run `floop tags backfill` to achieve 100% tag coverage."},"tags":["concurrency","configuration","floop","workflow"]},"kind":"procedure","name":"learned/expand-tag-extraction-dictionary-with-d-adddocu","provenance":{"correction_id":"c-1770867776463535061","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-9c9d9ff7e96e","kind":"behavior","content":{"content":{"canonical":"When testing token budget enforcement in floop_active: (1) each behavior MUST have unique canonical content due to duplicate content detection, (2) understand the full activation pipeline: specificity → SpecificityToActivation → spreading engine sigmoid(x) → tier thresholds. Specificity 0 → activation 0.3 → sigmoid 0.5 → Summary tier. Specificity 1 → 0.4 → sigmoid ~0.731 → Full tier. To trigger budget demotion, use language-matched behaviors (specificity \u003e= 1, Full tier) with large canonical content so the total exceeds the 2000-token budget.","expanded":"When working on this type of task, avoid: When writing tests for token budget enforcement, created behaviors with identical canonical content across all 40 items, triggering duplicate content detection. Also initially assumed behaviors with `when: {}` would exceed the budget at ~25 tokens each, not accounting for the sigmoid squashing (0.3 → 0.5 post-sigmoid) putting them at Summary tier where long content gets truncated to 60 chars (~15 tokens).\n\nInstead: When testing token budget enforcement in floop_active: (1) each behavior MUST have unique canonical content due to duplicate content detection, (2) understand the full activation pipeline: specificity → SpecificityToActivation → spreading engine sigmoid(x) → tier thresholds. Specificity 0 → activation 0.3 → sigmoid 0.5 → Summary tier. Specificity 1 → 0.4 → sigmoid ~0.731 → Full tier. To trigger budget demotion, use language-matched behaviors (specificity \u003e= 1, Full tier) with large canonical content so the total exceeds the 2000-token budget.","structured":{"avoid":"When writing tests for token budget enforcement, created behaviors with identical canonical content across all 40 items, triggering duplicate content detection. Also initially assumed behaviors with `when: {}` would exceed the budget at ~25 tokens each, not accounting for the sigmoid squashing (0.3 → 0.5 post-sigmoid) putting them at Summary tier where long content gets truncated to 60 chars (~15 tokens).","prefer":"When testing token budget enforcement in floop_active: (1) each behavior MUST have unique canonical content due to duplicate content detection, (2) understand the full activation pipeline: specificity → SpecificityToActivation → spreading engine sigmoid(x) → tier thresholds. Specificity 0 → activation 0.3 → sigmoid 0.5 → Summary tier. Specificity 1 → 0.4 → sigmoid ~0.731 → Full tier. To trigger budget demotion, use language-matched behaviors (specificity \u003e= 1, Full tier) with large canonical content so the total exceeds the 2000-token budget."},"tags":["behavior","ci","floop","spreading-activation","testing"]},"kind":"preference","name":"learned/when-testing-token-budget-enforcement-in-floop_act","provenance":{"correction_id":"c-1770712806536601605","source_type":"learned"},"when":{"environment":"development","task":"testing"}},"metadata":{"confidence":0.7000000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-9d180407fda6","kind":"behavior","content":{"content":{"canonical":"Content hash collision during INSERT should either error explicitly or trigger proper deduplication flow. The current silent replace behavior can cause data loss. Consider: (1) remove UNIQUE constraint and handle dedup separately, or (2) check for existing content_hash before insert and return meaningful error.","expanded":"When working on this type of task, avoid: SQLite store uses INSERT OR REPLACE with a UNIQUE content_hash constraint. When two behaviors have identical canonical content, the second silently replaces the first instead of erroring or deduplicating properly.\n\nInstead: Content hash collision during INSERT should either error explicitly or trigger proper deduplication flow. The current silent replace behavior can cause data loss. Consider: (1) remove UNIQUE constraint and handle dedup separately, or (2) check for existing content_hash before insert and return meaningful error.","structured":{"avoid":"SQLite store uses INSERT OR REPLACE with a UNIQUE content_hash constraint. When two behaviors have identical canonical content, the second silently replaces the first instead of erroring or deduplicating properly.","prefer":"Content hash collision during INSERT should either error explicitly or trigger proper deduplication flow. The current silent replace behavior can cause data loss. Consider: (1) remove UNIQUE constraint and handle dedup separately, or (2) check for existing content_hash before insert and return meaningful error."}},"kind":"preference","name":"learned/content-hash-collision-during-insert-should-either","provenance":{"correction_id":"c-1770270981283723438","created_at":"2026-02-04T21:56:21.283777669-08:00","source_type":"learned"},"when":{"file_path":"store/*","language":"go","task":"bug-identification"}},"metadata":{"confidence":0.66,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-9eda305d5c19","kind":"behavior","content":{"content":{"canonical":"Index based on actual query patterns from EXPLAIN ANALYZE. Create composite indexes for multi-column WHERE/ORDER BY clauses with most selective column first. Use partial indexes for queries with constant filters (WHERE status = 'active'). Monitor index usage with pg_stat_user_indexes and drop unused indexes.","expanded":"When working on this type of task, avoid: Creating indexes on every column \"just in case\" without analyzing query patterns, wasting storage and slowing down writes significantly\n\nInstead: Index based on actual query patterns from EXPLAIN ANALYZE. Create composite indexes for multi-column WHERE/ORDER BY clauses with most selective column first. Use partial indexes for queries with constant filters (WHERE status = 'active'). Monitor index usage with pg_stat_user_indexes and drop unused indexes.","structured":{"avoid":"Creating indexes on every column \"just in case\" without analyzing query patterns, wasting storage and slowing down writes significantly","prefer":"Index based on actual query patterns from EXPLAIN ANALYZE. Create composite indexes for multi-column WHERE/ORDER BY clauses with most selective column first. Use partial indexes for queries with constant filters (WHERE status = 'active'). Monitor index usage with pg_stat_user_indexes and drop unused indexes."}},"kind":"procedure","name":"learned/index-based-on-actual-query-patterns-from-explain","provenance":{"correction_id":"c-1770877691681860478","created_at":"2026-02-11T22:28:11.682004187-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-a09dd52f6085","kind":"behavior","content":{"content":{"canonical":"Cap frame rate at 30 or 60 FPS instead of unlimited. Use texture compression (ASTC for mobile). Reduce draw calls with atlasing and batching. Implement dynamic quality settings that reduce effects when device is hot. Profile power consumption on actual devices across different quality settings.","expanded":"When working on this type of task, avoid: Building mobile games without considering battery drain and thermal throttling, causing poor user experience and negative reviews\n\nInstead: Cap frame rate at 30 or 60 FPS instead of unlimited. Use texture compression (ASTC for mobile). Reduce draw calls with atlasing and batching. Implement dynamic quality settings that reduce effects when device is hot. Profile power consumption on actual devices across different quality settings.","structured":{"avoid":"Building mobile games without considering battery drain and thermal throttling, causing poor user experience and negative reviews","prefer":"Cap frame rate at 30 or 60 FPS instead of unlimited. Use texture compression (ASTC for mobile). Reduce draw calls with atlasing and batching. Implement dynamic quality settings that reduce effects when device is hot. Profile power consumption on actual devices across different quality settings."}},"kind":"preference","name":"learned/cap-frame-rate-at-30-or-60-fps-instead-of-unlimite","provenance":{"correction_id":"c-1770877978397846516","created_at":"2026-02-11T22:32:58.397916207-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-a4b643d4c53c","kind":"behavior","content":{"content":{"canonical":"Use struct-of-arrays (SoA) instead of array-of-structs (AoS) when processing individual fields. Keep frequently accessed fields together in memory layout. Align data to cache line boundaries (64 bytes) and avoid false sharing by padding hot variables. Iterate in memory order (row-major for C/C++) to maximize prefetching.","expanded":"When working on this type of task, avoid: Designing data structures without considering cache locality, causing excessive cache misses and memory bandwidth bottlenecks\n\nInstead: Use struct-of-arrays (SoA) instead of array-of-structs (AoS) when processing individual fields. Keep frequently accessed fields together in memory layout. Align data to cache line boundaries (64 bytes) and avoid false sharing by padding hot variables. Iterate in memory order (row-major for C/C++) to maximize prefetching.","structured":{"avoid":"Designing data structures without considering cache locality, causing excessive cache misses and memory bandwidth bottlenecks","prefer":"Use struct-of-arrays (SoA) instead of array-of-structs (AoS) when processing individual fields. Keep frequently accessed fields together in memory layout. Align data to cache line boundaries (64 bytes) and avoid false sharing by padding hot variables. Iterate in memory order (row-major for C/C++) to maximize prefetching."}},"kind":"constraint","name":"learned/use-struct-of-arrays-soa-instead-of-array-of-str","provenance":{"correction_id":"c-1770877708128954746","created_at":"2026-02-11T22:28:28.129022012-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-a9a4b09dda1f","kind":"behavior","content":{"content":{"canonical":"Create explicit hooks in AGENTS.md that mandate tool usage - use WARNING/CRITICAL markers and 'READ FIRST' to ensure visibility","expanded":"When working on this type of task, avoid: Assumed agents would know when to use tools based on general context\n\nInstead: Create explicit hooks in AGENTS.md that mandate tool usage - use WARNING/CRITICAL markers and 'READ FIRST' to ensure visibility","structured":{"avoid":"Assumed agents would know when to use tools based on general context","prefer":"Create explicit hooks in AGENTS.md that mandate tool usage - use WARNING/CRITICAL markers and 'READ FIRST' to ensure visibility"}},"kind":"procedure","name":"learned/create-explicit-hooks-in-agents-md-that-mandate-to","provenance":{"correction_id":"c-1769578263708845277","created_at":"2026-01-27T21:31:03.709247071-08:00","source_type":"learned"},"when":{"task":"documentation"}},"metadata":{"confidence":0.66,"priority":0,"scope":"local","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-ac51372556ab","kind":"behavior","content":{"content":{"canonical":"Implement skeleton screens that match actual content layout instead of generic spinners. Use intersection observer for viewport-based lazy loading (images, components). Prefetch likely-needed data based on user behavior (hover, scroll direction). Show stale data immediately while revalidating in background.","expanded":"When working on this type of task, avoid: Loading all data upfront or implementing lazy loading without proper loading states and error boundaries, creating poor user experience with loading spinners everywhere\n\nInstead: Implement skeleton screens that match actual content layout instead of generic spinners. Use intersection observer for viewport-based lazy loading (images, components). Prefetch likely-needed data based on user behavior (hover, scroll direction). Show stale data immediately while revalidating in background.","structured":{"avoid":"Loading all data upfront or implementing lazy loading without proper loading states and error boundaries, creating poor user experience with loading spinners everywhere","prefer":"Implement skeleton screens that match actual content layout instead of generic spinners. Use intersection observer for viewport-based lazy loading (images, components). Prefetch likely-needed data based on user behavior (hover, scroll direction). Show stale data immediately while revalidating in background."}},"kind":"preference","name":"learned/implement-skeleton-screens-that-match-actual-conte","provenance":{"correction_id":"c-1770877754410191865","created_at":"2026-02-11T22:29:14.410275792-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-ae49727578ff","kind":"behavior","content":{"content":{"canonical":"This is a known feature gap. The review mechanism (ApprovePending/RejectPending) exists internally but needs to be exposed as MCP tools (floop_approve/floop_reject) and/or CLI commands. Pending behaviors still activate via floop_active — they just lack the approval stamp and confidence boost. Track this as a feature to implement.","expanded":"When working on this type of task, avoid: floop_learn returns requires_review: true for constraint behaviors, but there is no CLI command or MCP tool to actually approve or reject pending behaviors. The ApprovePending() and RejectPending() methods exist in internal/learning/loop.go but are not exposed to users.\n\nInstead: This is a known feature gap. The review mechanism (ApprovePending/RejectPending) exists internally but needs to be exposed as MCP tools (floop_approve/floop_reject) and/or CLI commands. Pending behaviors still activate via floop_active — they just lack the approval stamp and confidence boost. Track this as a feature to implement.","structured":{"avoid":"floop_learn returns requires_review: true for constraint behaviors, but there is no CLI command or MCP tool to actually approve or reject pending behaviors. The ApprovePending() and RejectPending() methods exist in internal/learning/loop.go but are not exposed to users.","prefer":"This is a known feature gap. The review mechanism (ApprovePending/RejectPending) exists internally but needs to be exposed as MCP tools (floop_approve/floop_reject) and/or CLI commands. Pending behaviors still activate via floop_active — they just lack the approval stamp and confidence boost. Track this as a feature to implement."},"tags":["behavior","cli","floop","mcp"]},"kind":"directive","name":"learned/this-is-a-known-feature-gap-the-review-mechanism","provenance":{"correction_id":"c-1770827519205934507","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.68,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-b1597de8f23a","kind":"behavior","content":{"content":{"canonical":"Implement backpressure using bounded queues that block or drop when full. Use reactive streams with demand signaling (Reactive Streams, RxJS operators). Implement load shedding by dropping low-priority work when overloaded. Monitor queue depths and apply flow control before memory exhaustion.","expanded":"When working on this type of task, avoid: Building streaming systems without backpressure mechanisms, causing memory exhaustion when producers overwhelm consumers\n\nInstead: Implement backpressure using bounded queues that block or drop when full. Use reactive streams with demand signaling (Reactive Streams, RxJS operators). Implement load shedding by dropping low-priority work when overloaded. Monitor queue depths and apply flow control before memory exhaustion.","structured":{"avoid":"Building streaming systems without backpressure mechanisms, causing memory exhaustion when producers overwhelm consumers","prefer":"Implement backpressure using bounded queues that block or drop when full. Use reactive streams with demand signaling (Reactive Streams, RxJS operators). Implement load shedding by dropping low-priority work when overloaded. Monitor queue depths and apply flow control before memory exhaustion."}},"kind":"preference","name":"learned/implement-backpressure-using-bounded-queues-that-b","provenance":{"correction_id":"c-1770877900649715050","created_at":"2026-02-11T22:31:40.649822241-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-b15c09d75e4e","kind":"behavior","content":{"content":{"canonical":"Require pull requests for all changes with at least one reviewer approval. Use branch protection rules to enforce CI passing before merge. Write clear PR descriptions explaining what/why. Review for correctness, readability, and edge cases - not just style. Automate style checks with linters to focus review on logic.","expanded":"When working on this type of task, avoid: Committing directly to main branch or merging without code review, missing opportunities to catch bugs and share knowledge\n\nInstead: Require pull requests for all changes with at least one reviewer approval. Use branch protection rules to enforce CI passing before merge. Write clear PR descriptions explaining what/why. Review for correctness, readability, and edge cases - not just style. Automate style checks with linters to focus review on logic.","structured":{"avoid":"Committing directly to main branch or merging without code review, missing opportunities to catch bugs and share knowledge","prefer":"Require pull requests for all changes with at least one reviewer approval. Use branch protection rules to enforce CI passing before merge. Write clear PR descriptions explaining what/why. Review for correctness, readability, and edge cases - not just style. Automate style checks with linters to focus review on logic."}},"kind":"preference","name":"learned/require-pull-requests-for-all-changes-with-at-leas","provenance":{"correction_id":"c-1770878085410699169","created_at":"2026-02-11T22:34:45.410795901-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-b51a30c8a37b","kind":"behavior","content":{"content":{"canonical":"Focus on testing critical business logic and edge cases, not achieving coverage percentage. High coverage doesn't guarantee quality - test behaviors and invariants, not implementation details. Use coverage to find untested code paths, not as success metric. Mutation testing provides better quality signal than line coverage.","expanded":"When working on this type of task, avoid: Chasing 100% code coverage with meaningless tests that don't validate behavior, creating false confidence\n\nInstead: Focus on testing critical business logic and edge cases, not achieving coverage percentage. High coverage doesn't guarantee quality - test behaviors and invariants, not implementation details. Use coverage to find untested code paths, not as success metric. Mutation testing provides better quality signal than line coverage.","structured":{"avoid":"Chasing 100% code coverage with meaningless tests that don't validate behavior, creating false confidence","prefer":"Focus on testing critical business logic and edge cases, not achieving coverage percentage. High coverage doesn't guarantee quality - test behaviors and invariants, not implementation details. Use coverage to find untested code paths, not as success metric. Mutation testing provides better quality signal than line coverage."}},"kind":"preference","name":"learned/focus-on-testing-critical-business-logic-and-edge","provenance":{"correction_id":"c-1770878045815008254","created_at":"2026-02-11T22:34:05.815063577-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-b5237c8388d1","kind":"behavior","content":{"content":{"canonical":"Implement graceful shutdown: stop accepting new requests, wait for in-flight requests to complete (with timeout), flush buffers, close connections cleanly. Listen for SIGTERM and handle cleanup. Use readiness probe to signal when shutdown starts. Set appropriate termination grace period in Kubernetes (default 30s may be too short for long requests).","expanded":"When working on this type of task, avoid: Terminating applications immediately on shutdown signals, dropping in-flight requests and corrupting data\n\nInstead: Implement graceful shutdown: stop accepting new requests, wait for in-flight requests to complete (with timeout), flush buffers, close connections cleanly. Listen for SIGTERM and handle cleanup. Use readiness probe to signal when shutdown starts. Set appropriate termination grace period in Kubernetes (default 30s may be too short for long requests).","structured":{"avoid":"Terminating applications immediately on shutdown signals, dropping in-flight requests and corrupting data","prefer":"Implement graceful shutdown: stop accepting new requests, wait for in-flight requests to complete (with timeout), flush buffers, close connections cleanly. Listen for SIGTERM and handle cleanup. Use readiness probe to signal when shutdown starts. Set appropriate termination grace period in Kubernetes (default 30s may be too short for long requests)."}},"kind":"constraint","name":"learned/implement-graceful-shutdown-stop-accepting-new-re","provenance":{"correction_id":"c-1770878074573181337","created_at":"2026-02-11T22:34:34.573236821-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-b69b95951658","kind":"behavior","content":{"content":{"canonical":"Define explicit file boundaries before launching parallel subagents: each agent gets exclusive ownership of specific packages/directories (e.g., Agent A owns ui/dashboard/*, Agent B owns config/*, Agent C owns stow/*). Document boundaries in the plan and include them in each agent's prompt.","expanded":"When working on this type of task, avoid: Launching parallel subagents that may modify the same files, causing merge conflicts when worktrees are merged back\n\nInstead: Define explicit file boundaries before launching parallel subagents: each agent gets exclusive ownership of specific packages/directories (e.g., Agent A owns ui/dashboard/*, Agent B owns config/*, Agent C owns stow/*). Document boundaries in the plan and include them in each agent's prompt.","structured":{"avoid":"Launching parallel subagents that may modify the same files, causing merge conflicts when worktrees are merged back","prefer":"Define explicit file boundaries before launching parallel subagents: each agent gets exclusive ownership of specific packages/directories (e.g., Agent A owns ui/dashboard/*, Agent B owns config/*, Agent C owns stow/*). Document boundaries in the plan and include them in each agent's prompt."},"tags":["configuration","filesystem"]},"kind":"directive","name":"learned/define-explicit-file-boundaries-before-launching-p","provenance":{"correction_id":"c-1770186283980360853","source_type":"learned"},"when":{"task":"refactoring"}},"metadata":{"confidence":0.57,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:54-08:00","times_activated":1,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-b7f5d1378039","kind":"behavior","content":{"content":{"canonical":"Beads export state auto-updates with each commit hash. After final push, restore the export state file instead of committing it again to break the cycle.","expanded":"When working on this type of task, avoid: Committed beads export state changes in a loop, causing infinite commits\n\nInstead: Beads export state auto-updates with each commit hash. After final push, restore the export state file instead of committing it again to break the cycle.","structured":{"avoid":"Committed beads export state changes in a loop, causing infinite commits","prefer":"Beads export state auto-updates with each commit hash. After final push, restore the export state file instead of committing it again to break the cycle."}},"kind":"preference","name":"learned/beads-export-state-auto-updates-with-each-commit-h","provenance":{"correction_id":"c-1769581392105512844","created_at":"2026-01-27T22:23:12.106122549-08:00","source_type":"learned"},"when":{"task":"git workflow"}},"metadata":{"confidence":0.66,"priority":0,"scope":"local","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-b86932976d88","kind":"behavior","content":{"content":{"canonical":"Even for beads state sync and chore commits, create a feature branch and PR. The rule 'never merge directly to main' applies to ALL commits, not just code changes. Push back on plans that specify direct-to-main commits.","expanded":"When working on this type of task, avoid: Committed beads sync/housekeeping changes directly to main and pushed, following the plan without questioning it. Three commits went straight to main without a PR.\n\nInstead: Even for beads state sync and chore commits, create a feature branch and PR. The rule 'never merge directly to main' applies to ALL commits, not just code changes. Push back on plans that specify direct-to-main commits.","structured":{"avoid":"Committed beads sync/housekeeping changes directly to main and pushed, following the plan without questioning it. Three commits went straight to main without a PR.","prefer":"Even for beads state sync and chore commits, create a feature branch and PR. The rule 'never merge directly to main' applies to ALL commits, not just code changes. Push back on plans that specify direct-to-main commits."}},"kind":"constraint","name":"learned/even-for-beads-state-sync-and-chore-commits-creat","provenance":{"correction_id":"c-1770570585979824699","created_at":"2026-02-08T09:09:45.979869293-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-b990684a96f5","kind":"behavior","content":{"content":{"canonical":"Use logging library (winston, pino, zap, logrus) with structured fields and log levels. Log at appropriate levels: DEBUG for detailed troubleshooting, INFO for significant events, WARN for degraded operation, ERROR for failures. Include context: request ID, user ID, operation name. Configure log levels per environment (DEBUG in dev, INFO in prod).","expanded":"When working on this type of task, avoid: Using console.log for all logging in production code, making it difficult to filter logs by severity or structured fields\n\nInstead: Use logging library (winston, pino, zap, logrus) with structured fields and log levels. Log at appropriate levels: DEBUG for detailed troubleshooting, INFO for significant events, WARN for degraded operation, ERROR for failures. Include context: request ID, user ID, operation name. Configure log levels per environment (DEBUG in dev, INFO in prod).","structured":{"avoid":"Using console.log for all logging in production code, making it difficult to filter logs by severity or structured fields","prefer":"Use logging library (winston, pino, zap, logrus) with structured fields and log levels. Log at appropriate levels: DEBUG for detailed troubleshooting, INFO for significant events, WARN for degraded operation, ERROR for failures. Include context: request ID, user ID, operation name. Configure log levels per environment (DEBUG in dev, INFO in prod)."}},"kind":"preference","name":"learned/use-logging-library-winston-pino-zap-logrus-w","provenance":{"correction_id":"c-1770878079911119675","created_at":"2026-02-11T22:34:39.911235914-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-bb7b9d81d35e","kind":"behavior","content":{"content":{"canonical":"Use client-side prediction to show immediate feedback for local player actions. Implement server reconciliation to correct predictions when server state differs. Use interpolation for remote players to smooth network jitter. Send inputs to server, not positions - let server be authoritative. Use delta compression and prioritize critical state updates.","expanded":"When working on this type of task, avoid: Implementing multiplayer games without client-side prediction and server reconciliation, resulting in laggy, unresponsive gameplay\n\nInstead: Use client-side prediction to show immediate feedback for local player actions. Implement server reconciliation to correct predictions when server state differs. Use interpolation for remote players to smooth network jitter. Send inputs to server, not positions - let server be authoritative. Use delta compression and prioritize critical state updates.","structured":{"avoid":"Implementing multiplayer games without client-side prediction and server reconciliation, resulting in laggy, unresponsive gameplay","prefer":"Use client-side prediction to show immediate feedback for local player actions. Implement server reconciliation to correct predictions when server state differs. Use interpolation for remote players to smooth network jitter. Send inputs to server, not positions - let server be authoritative. Use delta compression and prioritize critical state updates."}},"kind":"preference","name":"learned/use-client-side-prediction-to-show-immediate-feedb","provenance":{"correction_id":"c-1770877983655411215","created_at":"2026-02-11T22:33:03.655474834-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-beads-merged","kind":"behavior","content":{"content":{"canonical":"When working with beads: (1) create detailed epics + tasks with dependency graphs after planning, (2) claim work with 'bd update \u003cid\u003e --status in_progress' when starting, (3) close with 'bd close \u003cid\u003e --reason \"...\"' when committing the completed work. Keep bead state synchronized with actual work progress."},"kind":"directive","name":"learned/beads-workflow","provenance":{"created_at":"2026-02-07T00:00:00-08:00","source_type":"merged"},"when":{"task":"development"}},"metadata":{"confidence":0.72,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-bedab3b4cd09","kind":"behavior","content":{"content":{"canonical":"Use DataLoader to batch and cache database requests within a single GraphQL operation. Implement field-level batching where resolvers collect IDs across all items then make single bulk query. Monitor resolver execution with Apollo tracing to identify N+1 patterns.","expanded":"When working on this type of task, avoid: Writing GraphQL resolvers that make separate database queries for each item in a list, causing N+1 query problems and terrible performance\n\nInstead: Use DataLoader to batch and cache database requests within a single GraphQL operation. Implement field-level batching where resolvers collect IDs across all items then make single bulk query. Monitor resolver execution with Apollo tracing to identify N+1 patterns.","structured":{"avoid":"Writing GraphQL resolvers that make separate database queries for each item in a list, causing N+1 query problems and terrible performance","prefer":"Use DataLoader to batch and cache database requests within a single GraphQL operation. Implement field-level batching where resolvers collect IDs across all items then make single bulk query. Monitor resolver execution with Apollo tracing to identify N+1 patterns."}},"kind":"procedure","name":"learned/use-dataloader-to-batch-and-cache-database-request","provenance":{"correction_id":"c-1770877769336101027","created_at":"2026-02-11T22:29:29.336227495-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-c83aad31d913","kind":"behavior","content":{"content":{"canonical":"Use template.JS (not template.HTML) for data injected into  blocks in html/template. template.HTML only asserts HTML safety — in JS contexts the engine still JS-escapes it. template.JS asserts JS safety and prevents double-encoding. Combine with json.HTMLEscape for XSS prevention (converts \u003c \u003e \u0026 to unicode escapes, preventing  breakout)","expanded":"When working on this type of task, avoid: Used template.HTML for JSON data inside an html/template  block, which caused the template engine to JS-escape the value (wrapping in quotes and escaping inner quotes), turning the JSON object into a string literal\n\nInstead: Use template.JS (not template.HTML) for data injected into  blocks in html/template. template.HTML only asserts HTML safety — in JS contexts the engine still JS-escapes it. template.JS asserts JS safety and prevents double-encoding. Combine with json.HTMLEscape for XSS prevention (converts \u003c \u003e \u0026 to unicode escapes, preventing  breakout)","structured":{"avoid":"Used template.HTML for JSON data inside an html/template  block, which caused the template engine to JS-escape the value (wrapping in quotes and escaping inner quotes), turning the JSON object into a string literal","prefer":"Use template.JS (not template.HTML) for data injected into  blocks in html/template. template.HTML only asserts HTML safety — in JS contexts the engine still JS-escapes it. template.JS asserts JS safety and prevents double-encoding. Combine with json.HTMLEscape for XSS prevention (converts \u003c \u003e \u0026 to unicode escapes, preventing  breakout)"},"tags":["javascript","json","security"]},"kind":"preference","name":"learned/use-template-js-not-template-html-for-data-injec","provenance":{"correction_id":"c-1770862037019587237","source_type":"learned"},"when":{"environment":"development","file_path":"visualization/*","language":"go","task":"development"}},"metadata":{"confidence":0.625,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:38:40-08:00","times_activated":2,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-c9c0a2345114","kind":"behavior","content":{"content":{"canonical":"After writing any test, run it to verify it works before committing. For VHS tests: run with UPDATE_GOLDEN=1 to create golden files and view screenshots to confirm correct behavior. Never commit untested test code.","expanded":"When working on this type of task, avoid: Added a VHS test function (TestVHS_OnboardingFlow) without actually running it to verify it works. Committed code that was never executed.\n\nInstead: After writing any test, run it to verify it works before committing. For VHS tests: run with UPDATE_GOLDEN=1 to create golden files and view screenshots to confirm correct behavior. Never commit untested test code.","structured":{"avoid":"Added a VHS test function (TestVHS_OnboardingFlow) without actually running it to verify it works. Committed code that was never executed.","prefer":"After writing any test, run it to verify it works before committing. For VHS tests: run with UPDATE_GOLDEN=1 to create golden files and view screenshots to confirm correct behavior. Never commit untested test code."},"tags":["behavior","filesystem","testing"]},"kind":"constraint","name":"learned/after-writing-any-test-run-it-to-verify-it-works","provenance":{"correction_id":"c-1770170639896627928","source_type":"learned"},"when":{"task":"testing"}},"metadata":{"confidence":0.595,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:44-08:00","times_activated":1,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-cbba3c67c07e","kind":"behavior","content":{"content":{"canonical":"Use gRPC for internal service-to-service communication where performance matters and both ends are controlled. Use gRPC-Web with envoy proxy for browser clients if needed. For public APIs, prefer REST/GraphQL for better tooling, caching, and developer experience. Use grpc-gateway to expose both gRPC and REST endpoints from same service.","expanded":"When working on this type of task, avoid: Using gRPC for public-facing APIs or browser clients where HTTP/2 support is inconsistent and debugging tools are limited\n\nInstead: Use gRPC for internal service-to-service communication where performance matters and both ends are controlled. Use gRPC-Web with envoy proxy for browser clients if needed. For public APIs, prefer REST/GraphQL for better tooling, caching, and developer experience. Use grpc-gateway to expose both gRPC and REST endpoints from same service.","structured":{"avoid":"Using gRPC for public-facing APIs or browser clients where HTTP/2 support is inconsistent and debugging tools are limited","prefer":"Use gRPC for internal service-to-service communication where performance matters and both ends are controlled. Use gRPC-Web with envoy proxy for browser clients if needed. For public APIs, prefer REST/GraphQL for better tooling, caching, and developer experience. Use grpc-gateway to expose both gRPC and REST endpoints from same service."}},"kind":"preference","name":"learned/use-grpc-for-internal-service-to-service-communica","provenance":{"correction_id":"c-1770877778489418591","created_at":"2026-02-11T22:29:38.489515824-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-cd52398f9d13","kind":"behavior","content":{"content":{"canonical":"Use versioned cache names (e.g., 'app-v1.2.3') and implement cache-first for static assets, network-first for API calls. Delete old caches on service worker activation. Use workbox for production-ready caching strategies with automatic versioning.","expanded":"When working on this type of task, avoid: Caching everything aggressively in service workers without versioning or cache invalidation strategy, leading to users stuck on old versions\n\nInstead: Use versioned cache names (e.g., 'app-v1.2.3') and implement cache-first for static assets, network-first for API calls. Delete old caches on service worker activation. Use workbox for production-ready caching strategies with automatic versioning.","structured":{"avoid":"Caching everything aggressively in service workers without versioning or cache invalidation strategy, leading to users stuck on old versions","prefer":"Use versioned cache names (e.g., 'app-v1.2.3') and implement cache-first for static assets, network-first for API calls. Delete old caches on service worker activation. Use workbox for production-ready caching strategies with automatic versioning."}},"kind":"procedure","name":"learned/use-versioned-cache-names-e-g-app-v1-2-3-and","provenance":{"correction_id":"c-1770877648712731653","created_at":"2026-02-11T22:27:28.712883586-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-d008977c30a8","kind":"behavior","content":{"content":{"canonical":"Always test both success and error paths","expanded":"When working on this type of task, avoid: Test error handling\n\nInstead: Always test both success and error paths","structured":{"avoid":"Test error handling","prefer":"Always test both success and error paths"},"tags":["filesystem","testing"]},"kind":"directive","name":"learned/always-test-both-success-and-error-paths","provenance":{"correction_id":"c-1769616621323031933","source_type":"learned"},"when":{"task":"testing"}},"metadata":{"confidence":0.595,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:44-08:00","times_activated":1,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-d10540290a43","kind":"behavior","content":{"content":{"canonical":"Use behavior trees with reusable nodes (selectors, sequences, decorators) for hierarchical decision making. Implement finite state machines for simpler state-based AI. Use utility-based AI for multi-factor decision making with scoring functions. Profile AI overhead and time-slice expensive pathfinding across multiple frames.","expanded":"When working on this type of task, avoid: Implementing game AI with complex if-else chains that become unmaintainable and difficult to debug as behaviors grow\n\nInstead: Use behavior trees with reusable nodes (selectors, sequences, decorators) for hierarchical decision making. Implement finite state machines for simpler state-based AI. Use utility-based AI for multi-factor decision making with scoring functions. Profile AI overhead and time-slice expensive pathfinding across multiple frames.","structured":{"avoid":"Implementing game AI with complex if-else chains that become unmaintainable and difficult to debug as behaviors grow","prefer":"Use behavior trees with reusable nodes (selectors, sequences, decorators) for hierarchical decision making. Implement finite state machines for simpler state-based AI. Use utility-based AI for multi-factor decision making with scoring functions. Profile AI overhead and time-slice expensive pathfinding across multiple frames."}},"kind":"preference","name":"learned/use-behavior-trees-with-reusable-nodes-selectors","provenance":{"correction_id":"c-1770877973937866839","created_at":"2026-02-11T22:32:53.937948502-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-d1901ea5552c","kind":"behavior","content":{"content":{"canonical":"Use JOINs or eager loading to fetch related data in single query. For ORMs, use explicit eager loading (.include(), .prefetch_related()). Use IN queries to batch-fetch related entities. Monitor query logs in development to catch N+1 patterns early. Consider denormalizing frequently accessed data to eliminate joins.","expanded":"When working on this type of task, avoid: Iterating over database results and making additional queries for each row, creating N+1 query problems that kill performance\n\nInstead: Use JOINs or eager loading to fetch related data in single query. For ORMs, use explicit eager loading (.include(), .prefetch_related()). Use IN queries to batch-fetch related entities. Monitor query logs in development to catch N+1 patterns early. Consider denormalizing frequently accessed data to eliminate joins.","structured":{"avoid":"Iterating over database results and making additional queries for each row, creating N+1 query problems that kill performance","prefer":"Use JOINs or eager loading to fetch related data in single query. For ORMs, use explicit eager loading (.include(), .prefetch_related()). Use IN queries to batch-fetch related entities. Monitor query logs in development to catch N+1 patterns early. Consider denormalizing frequently accessed data to eliminate joins."}},"kind":"preference","name":"learned/use-joins-or-eager-loading-to-fetch-related-data-i","provenance":{"correction_id":"c-1770878022668993319","created_at":"2026-02-11T22:33:42.66908456-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-d3a17aea41a7","kind":"behavior","content":{"content":{"canonical":"Create detailed TASK.md in worktree with: Objective, Commands to update with line numbers, Required changes with before/after code, Testing requirements, Success criteria checklist, Commit message template. Agents work autonomously with zero corrections needed","expanded":"When working on this type of task, avoid: Give sub-agents vague instructions in the prompt like 'update these commands to use MultiGraphStore'\n\nInstead: Create detailed TASK.md in worktree with: Objective, Commands to update with line numbers, Required changes with before/after code, Testing requirements, Success criteria checklist, Commit message template. Agents work autonomously with zero corrections needed","structured":{"avoid":"Give sub-agents vague instructions in the prompt like 'update these commands to use MultiGraphStore'","prefer":"Create detailed TASK.md in worktree with: Objective, Commands to update with line numbers, Required changes with before/after code, Testing requirements, Success criteria checklist, Commit message template. Agents work autonomously with zero corrections needed"},"tags":["correction","testing","worktree"]},"kind":"directive","name":"learned/create-detailed-task-md-in-worktree-with-objectiv","provenance":{"correction_id":"c-1769660159645152208","source_type":"learned"},"when":{"task":"agent-coordination"}},"metadata":{"confidence":0.5449999999999999,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-d6cac9bdd28d","kind":"behavior","content":{"content":{"canonical":"Use semantic HTML first (button, nav, main, aside). Add ARIA roles, states, and properties when semantics are insufficient: role=\"dialog\", aria-labelledby, aria-expanded. Implement keyboard navigation (Tab, Enter, Escape, Arrow keys) and manage focus programmatically with focus traps and restoration.","expanded":"When working on this type of task, avoid: Building interactive components (modals, dropdowns, tabs) without proper ARIA attributes, keyboard navigation, or focus management, breaking screen reader and keyboard-only experiences\n\nInstead: Use semantic HTML first (button, nav, main, aside). Add ARIA roles, states, and properties when semantics are insufficient: role=\"dialog\", aria-labelledby, aria-expanded. Implement keyboard navigation (Tab, Enter, Escape, Arrow keys) and manage focus programmatically with focus traps and restoration.","structured":{"avoid":"Building interactive components (modals, dropdowns, tabs) without proper ARIA attributes, keyboard navigation, or focus management, breaking screen reader and keyboard-only experiences","prefer":"Use semantic HTML first (button, nav, main, aside). Add ARIA roles, states, and properties when semantics are insufficient: role=\"dialog\", aria-labelledby, aria-expanded. Implement keyboard navigation (Tab, Enter, Escape, Arrow keys) and manage focus programmatically with focus traps and restoration."}},"kind":"procedure","name":"learned/use-semantic-html-first-button-nav-main-aside","provenance":{"correction_id":"c-1770877654148121027","created_at":"2026-02-11T22:27:34.148219591-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-d7e8e629272b","kind":"behavior","content":{"content":{"canonical":"Use structured logging (JSON) with consistent field names across services. Include trace/request ID in all log entries for correlation. Log at appropriate levels (ERROR for actionable problems, INFO for key events, DEBUG for detailed troubleshooting). Implement distributed tracing (OpenTelemetry, Jaeger) for request flow visualization across services.","expanded":"When working on this type of task, avoid: Adding logging without structured fields or correlation IDs, making it impossible to trace requests across services or query logs effectively\n\nInstead: Use structured logging (JSON) with consistent field names across services. Include trace/request ID in all log entries for correlation. Log at appropriate levels (ERROR for actionable problems, INFO for key events, DEBUG for detailed troubleshooting). Implement distributed tracing (OpenTelemetry, Jaeger) for request flow visualization across services.","structured":{"avoid":"Adding logging without structured fields or correlation IDs, making it impossible to trace requests across services or query logs effectively","prefer":"Use structured logging (JSON) with consistent field names across services. Include trace/request ID in all log entries for correlation. Log at appropriate levels (ERROR for actionable problems, INFO for key events, DEBUG for detailed troubleshooting). Implement distributed tracing (OpenTelemetry, Jaeger) for request flow visualization across services."}},"kind":"preference","name":"learned/use-structured-logging-json-with-consistent-fiel","provenance":{"correction_id":"c-1770877834399113254","created_at":"2026-02-11T22:30:34.399197242-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-d92923c5a1e7","kind":"behavior","content":{"content":{"canonical":"Use 0700 for directories and 0600 for files containing sensitive data (databases, correction logs, config with API keys, session state, backups). Use os.CreateTemp instead of predictable .tmp suffixes to prevent symlink/TOCTOU attacks.","expanded":"When working on this type of task, avoid: Left file permissions at Go defaults (0755 dirs, 0644 files) for all .floop/ data including SQLite databases, correction logs, config files with API keys, and session state.\n\nInstead: Use 0700 for directories and 0600 for files containing sensitive data (databases, correction logs, config with API keys, session state, backups). Use os.CreateTemp instead of predictable .tmp suffixes to prevent symlink/TOCTOU attacks.","structured":{"avoid":"Left file permissions at Go defaults (0755 dirs, 0644 files) for all .floop/ data including SQLite databases, correction logs, config files with API keys, and session state.","prefer":"Use 0700 for directories and 0600 for files containing sensitive data (databases, correction logs, config with API keys, session state, backups). Use os.CreateTemp instead of predictable .tmp suffixes to prevent symlink/TOCTOU attacks."}},"kind":"preference","name":"learned/use-0700-for-directories-and-0600-for-files-contai","provenance":{"correction_id":"c-1770425640742094061","created_at":"2026-02-06T16:54:00.742138816-08:00","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.64,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-d9c8ddb42b9e","kind":"behavior","content":{"content":{"canonical":"Use Argon2id or bcrypt with high work factors (bcrypt cost \u003e= 12, Argon2 memory \u003e= 64MB). Include per-user random salts (automatic in modern libraries). Never roll your own crypto - use vetted libraries like bcrypt, argon2. Implement rate limiting on login endpoints to slow brute force attempts.","expanded":"When working on this type of task, avoid: Storing passwords with fast hashing algorithms (MD5, SHA1) or weak bcrypt work factors, making them vulnerable to brute force attacks\n\nInstead: Use Argon2id or bcrypt with high work factors (bcrypt cost \u003e= 12, Argon2 memory \u003e= 64MB). Include per-user random salts (automatic in modern libraries). Never roll your own crypto - use vetted libraries like bcrypt, argon2. Implement rate limiting on login endpoints to slow brute force attempts.","structured":{"avoid":"Storing passwords with fast hashing algorithms (MD5, SHA1) or weak bcrypt work factors, making them vulnerable to brute force attacks","prefer":"Use Argon2id or bcrypt with high work factors (bcrypt cost \u003e= 12, Argon2 memory \u003e= 64MB). Include per-user random salts (automatic in modern libraries). Never roll your own crypto - use vetted libraries like bcrypt, argon2. Implement rate limiting on login endpoints to slow brute force attempts."}},"kind":"constraint","name":"learned/use-argon2id-or-bcrypt-with-high-work-factors-bcr","provenance":{"correction_id":"c-1770877808457165739","created_at":"2026-02-11T22:30:08.457270416-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-da53f95a09b3","kind":"behavior","content":{"content":{"canonical":"Create an automated workflow script for parallel work: setup worktrees + task files + spawn agents in one command, then auto-cleanup and integrate after review","expanded":"When working on this type of task, avoid: Set up parallel worktrees manually with lots of steps: create directories, write task files, spawn agents one-by-one, then manually review, cherry-pick, cleanup\n\nInstead: Create an automated workflow script for parallel work: setup worktrees + task files + spawn agents in one command, then auto-cleanup and integrate after review","structured":{"avoid":"Set up parallel worktrees manually with lots of steps: create directories, write task files, spawn agents one-by-one, then manually review, cherry-pick, cleanup","prefer":"Create an automated workflow script for parallel work: setup worktrees + task files + spawn agents in one command, then auto-cleanup and integrate after review"},"tags":["filesystem","workflow","worktree"]},"kind":"procedure","name":"learned/create-an-automated-workflow-script-for-parallel-w","provenance":{"correction_id":"c-1769660133776136654","source_type":"learned"},"when":{"task":"parallel-development"}},"metadata":{"confidence":0.7850000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-dba5f8ec26d8","kind":"behavior","content":{"content":{"canonical":"Use named imports from tree-shakeable libraries: import { debounce } from 'lodash-es'. Configure side-effect-free packages in package.json with \"sideEffects\": false. Prefer libraries designed for tree-shaking over monolithic ones.","expanded":"When working on this type of task, avoid: Importing entire libraries when only using a few functions (e.g., import _ from 'lodash'), preventing tree-shaking and bloating the bundle\n\nInstead: Use named imports from tree-shakeable libraries: import { debounce } from 'lodash-es'. Configure side-effect-free packages in package.json with \"sideEffects\": false. Prefer libraries designed for tree-shaking over monolithic ones.","structured":{"avoid":"Importing entire libraries when only using a few functions (e.g., import _ from 'lodash'), preventing tree-shaking and bloating the bundle","prefer":"Use named imports from tree-shakeable libraries: import { debounce } from 'lodash-es'. Configure side-effect-free packages in package.json with \"sideEffects\": false. Prefer libraries designed for tree-shaking over monolithic ones."}},"kind":"preference","name":"learned/use-named-imports-from-tree-shakeable-libraries-i","provenance":{"correction_id":"c-1770877642564554428","created_at":"2026-02-11T22:27:22.564639907-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-dbef97df332b","kind":"behavior","content":{"content":{"canonical":"Use template.JS for values injected into script blocks. Pre-sanitize with json.HTMLEscape to prevent script breakout XSS. template.HTML is only trusted for HTML contexts, not JS contexts in html/template.","expanded":"When working on this type of task, avoid: Used template.HTML for injecting JSON into a script block in html/template. html/template applies JS-encoding on template.HTML values inside script contexts, turning JSON objects into quoted strings.\n\nInstead: Use template.JS for values injected into script blocks. Pre-sanitize with json.HTMLEscape to prevent script breakout XSS. template.HTML is only trusted for HTML contexts, not JS contexts in html/template.","structured":{"avoid":"Used template.HTML for injecting JSON into a script block in html/template. html/template applies JS-encoding on template.HTML values inside script contexts, turning JSON objects into quoted strings.","prefer":"Use template.JS for values injected into script blocks. Pre-sanitize with json.HTMLEscape to prevent script breakout XSS. template.HTML is only trusted for HTML contexts, not JS contexts in html/template."},"tags":["javascript","json","security"]},"kind":"preference","name":"learned/use-template-js-for-values-injected-into-script-bl","provenance":{"correction_id":"c-1770862232174665584","source_type":"learned"},"when":{"environment":"development","file_path":"visualization/*","language":"go","task":"development"}},"metadata":{"confidence":0.625,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:38:40-08:00","times_activated":2,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-dc1fac1293cc","kind":"behavior","content":{"content":{"canonical":"Use service discovery with health checks: Consul, etcd, or Kubernetes Services. Services register themselves on startup and deregister on shutdown. Clients query discovery service for healthy instances. Implement client-side load balancing with periodic refresh to handle topology changes.","expanded":"When working on this type of task, avoid: Hardcoding service endpoints or using static DNS for microservices discovery, breaking when services scale horizontally or move between environments\n\nInstead: Use service discovery with health checks: Consul, etcd, or Kubernetes Services. Services register themselves on startup and deregister on shutdown. Clients query discovery service for healthy instances. Implement client-side load balancing with periodic refresh to handle topology changes.","structured":{"avoid":"Hardcoding service endpoints or using static DNS for microservices discovery, breaking when services scale horizontally or move between environments","prefer":"Use service discovery with health checks: Consul, etcd, or Kubernetes Services. Services register themselves on startup and deregister on shutdown. Clients query discovery service for healthy instances. Implement client-side load balancing with periodic refresh to handle topology changes."}},"kind":"preference","name":"learned/use-service-discovery-with-health-checks-consul","provenance":{"correction_id":"c-1770877686280569063","created_at":"2026-02-11T22:28:06.280672056-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-dea6adf4c97e","kind":"behavior","content":{"content":{"canonical":"Use seeded random number generators to make generation deterministic and reproducible. Validate generated content for playability (ensure paths exist, resources are reachable). Use wave function collapse or constraint satisfaction for complex layouts. Layer multiple noise functions (Perlin, simplex) at different scales for natural-looking terrain.","expanded":"When working on this type of task, avoid: Generating procedural content without seeded randomness or validation, creating unreproducible results and impossible level layouts\n\nInstead: Use seeded random number generators to make generation deterministic and reproducible. Validate generated content for playability (ensure paths exist, resources are reachable). Use wave function collapse or constraint satisfaction for complex layouts. Layer multiple noise functions (Perlin, simplex) at different scales for natural-looking terrain.","structured":{"avoid":"Generating procedural content without seeded randomness or validation, creating unreproducible results and impossible level layouts","prefer":"Use seeded random number generators to make generation deterministic and reproducible. Validate generated content for playability (ensure paths exist, resources are reachable). Use wave function collapse or constraint satisfaction for complex layouts. Layer multiple noise functions (Perlin, simplex) at different scales for natural-looking terrain."}},"kind":"preference","name":"learned/use-seeded-random-number-generators-to-make-genera","provenance":{"correction_id":"c-1770877988573154760","created_at":"2026-02-11T22:33:08.57328826-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-dfdc5e16b9e0","kind":"behavior","content":{"content":{"canonical":"Consider both global (~/.floop/) and local (./.floop/) scopes - users want personal preferences across ALL projects AND project-specific conventions","expanded":"When working on this type of task, avoid: Only considered project-local behavior storage\n\nInstead: Consider both global (~/.floop/) and local (./.floop/) scopes - users want personal preferences across ALL projects AND project-specific conventions","structured":{"avoid":"Only considered project-local behavior storage","prefer":"Consider both global (~/.floop/) and local (./.floop/) scopes - users want personal preferences across ALL projects AND project-specific conventions"}},"kind":"preference","name":"learned/consider-both-global-~-floop-and-local-flo","provenance":{"correction_id":"c-1769577654747582237","created_at":"2026-01-27T21:20:54.748483819-08:00","source_type":"learned"},"when":{"file_path":"store/*","language":"go"}},"metadata":{"confidence":0.66,"priority":0,"scope":"local","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-e330154f4ae1","kind":"behavior","content":{"content":{"canonical":"Analyze worst-case time complexity before implementing. Use appropriate data structures: HashMap for O(1) lookup, TreeMap for O(log n) sorted access, Trie for prefix matching. For known large datasets, avoid nested loops and use algorithms with proven complexity bounds (sorting O(n log n), searching O(log n)).","expanded":"When working on this type of task, avoid: Choosing algorithms based on simplicity without considering Big-O complexity, leading to O(n²) or O(n³) algorithms that work fine in development but fail in production with large datasets\n\nInstead: Analyze worst-case time complexity before implementing. Use appropriate data structures: HashMap for O(1) lookup, TreeMap for O(log n) sorted access, Trie for prefix matching. For known large datasets, avoid nested loops and use algorithms with proven complexity bounds (sorting O(n log n), searching O(log n)).","structured":{"avoid":"Choosing algorithms based on simplicity without considering Big-O complexity, leading to O(n²) or O(n³) algorithms that work fine in development but fail in production with large datasets","prefer":"Analyze worst-case time complexity before implementing. Use appropriate data structures: HashMap for O(1) lookup, TreeMap for O(log n) sorted access, Trie for prefix matching. For known large datasets, avoid nested loops and use algorithms with proven complexity bounds (sorting O(n log n), searching O(log n))."}},"kind":"constraint","name":"learned/analyze-worst-case-time-complexity-before-implemen","provenance":{"correction_id":"c-1770877719550748636","created_at":"2026-02-11T22:28:39.550913255-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-e3c9f51e1f12","kind":"behavior","content":{"content":{"canonical":"Use specific image tags with digest pinning (nginx:1.21.0@sha256:...) for reproducible builds. Create non-root user in Dockerfile and run as that user. Use multi-stage builds to minimize image size and exclude build tools from runtime. Scan images with trivy/snyk and fix vulnerabilities before deployment.","expanded":"When working on this type of task, avoid: Running containers as root user or using latest tag in production, creating security vulnerabilities and unpredictable deployments\n\nInstead: Use specific image tags with digest pinning (nginx:1.21.0@sha256:...) for reproducible builds. Create non-root user in Dockerfile and run as that user. Use multi-stage builds to minimize image size and exclude build tools from runtime. Scan images with trivy/snyk and fix vulnerabilities before deployment.","structured":{"avoid":"Running containers as root user or using latest tag in production, creating security vulnerabilities and unpredictable deployments","prefer":"Use specific image tags with digest pinning (nginx:1.21.0@sha256:...) for reproducible builds. Create non-root user in Dockerfile and run as that user. Use multi-stage builds to minimize image size and exclude build tools from runtime. Scan images with trivy/snyk and fix vulnerabilities before deployment."}},"kind":"preference","name":"learned/use-specific-image-tags-with-digest-pinning-nginx","provenance":{"correction_id":"c-1770877818824762531","created_at":"2026-02-11T22:30:18.824886745-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-e5f22a6cbca2","kind":"behavior","content":{"content":{"canonical":"Design composables to return objects with explicit ref/computed properties: return { count: ref(0), doubled: computed(() =\u003e count.value * 2) }. This makes reactivity boundaries clear and allows consumers to destructure with toRefs if needed.","expanded":"When working on this type of task, avoid: Creating composables in Vue 3 that return unwrapped values or objects without clear ref structure, making reactivity implicit and hard to track\n\nInstead: Design composables to return objects with explicit ref/computed properties: return { count: ref(0), doubled: computed(() =\u003e count.value * 2) }. This makes reactivity boundaries clear and allows consumers to destructure with toRefs if needed.","structured":{"avoid":"Creating composables in Vue 3 that return unwrapped values or objects without clear ref structure, making reactivity implicit and hard to track","prefer":"Design composables to return objects with explicit ref/computed properties: return { count: ref(0), doubled: computed(() =\u003e count.value * 2) }. This makes reactivity boundaries clear and allows consumers to destructure with toRefs if needed."}},"kind":"directive","name":"learned/design-composables-to-return-objects-with-explicit","provenance":{"correction_id":"c-1770877624337099846","created_at":"2026-02-11T22:27:04.3371609-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-e7347fdf9806","kind":"behavior","content":{"content":{"canonical":"Prefer pure functions that always return same output for same input with no side effects. Use immutable data structures and return new copies instead of modifying in place. Isolate side effects (I/O, state changes) at boundaries. This enables easier testing, parallelization, and reasoning about code behavior.","expanded":"When working on this type of task, avoid: Mutating shared state and using side effects throughout the codebase, making code difficult to test, debug, and reason about\n\nInstead: Prefer pure functions that always return same output for same input with no side effects. Use immutable data structures and return new copies instead of modifying in place. Isolate side effects (I/O, state changes) at boundaries. This enables easier testing, parallelization, and reasoning about code behavior.","structured":{"avoid":"Mutating shared state and using side effects throughout the codebase, making code difficult to test, debug, and reason about","prefer":"Prefer pure functions that always return same output for same input with no side effects. Use immutable data structures and return new copies instead of modifying in place. Isolate side effects (I/O, state changes) at boundaries. This enables easier testing, parallelization, and reasoning about code behavior."}},"kind":"preference","name":"learned/prefer-pure-functions-that-always-return-same-outp","provenance":{"correction_id":"c-1770877911873220949","created_at":"2026-02-11T22:31:51.873302713-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-e804d3ceebf3","kind":"behavior","content":{"content":{"canonical":"For CPU-bound work, use pool size = number of cores. For I/O-bound work, use larger pools based on blocking coefficient: cores / (1 - blocking_time/total_time). Monitor thread utilization and queue depths. Use separate pools for different workload types. Test under load to find optimal sizing - theory is starting point, not final answer.","expanded":"When working on this type of task, avoid: Setting thread pool sizes to arbitrary large numbers or number of CPU cores without considering workload characteristics\n\nInstead: For CPU-bound work, use pool size = number of cores. For I/O-bound work, use larger pools based on blocking coefficient: cores / (1 - blocking_time/total_time). Monitor thread utilization and queue depths. Use separate pools for different workload types. Test under load to find optimal sizing - theory is starting point, not final answer.","structured":{"avoid":"Setting thread pool sizes to arbitrary large numbers or number of CPU cores without considering workload characteristics","prefer":"For CPU-bound work, use pool size = number of cores. For I/O-bound work, use larger pools based on blocking coefficient: cores / (1 - blocking_time/total_time). Monitor thread utilization and queue depths. Use separate pools for different workload types. Test under load to find optimal sizing - theory is starting point, not final answer."}},"kind":"preference","name":"learned/for-cpu-bound-work-use-pool-size-number-of-core","provenance":{"correction_id":"c-1770877905864566462","created_at":"2026-02-11T22:31:45.864717044-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-e8c2f147f305","kind":"behavior","content":{"content":{"canonical":"Always thread context.Context as the first parameter through Go interfaces, even for data-loading methods. If the caller has a context (e.g., from an HTTP handler or engine method), pass it through so cancellation and timeouts propagate correctly. Never use context.Background() when a parent context is available.","expanded":"When working on this type of task, avoid: Defined TagProvider.GetAllBehaviorTags() without a context.Context parameter, then used context.Background() in the implementation. Greptile flagged this — the parent context from Activate(ctx) was available but not threaded through.\n\nInstead: Always thread context.Context as the first parameter through Go interfaces, even for data-loading methods. If the caller has a context (e.g., from an HTTP handler or engine method), pass it through so cancellation and timeouts propagate correctly. Never use context.Background() when a parent context is available.","structured":{"avoid":"Defined TagProvider.GetAllBehaviorTags() without a context.Context parameter, then used context.Background() in the implementation. Greptile flagged this — the parent context from Activate(ctx) was available but not threaded through.","prefer":"Always thread context.Context as the first parameter through Go interfaces, even for data-loading methods. If the caller has a context (e.g., from an HTTP handler or engine method), pass it through so cancellation and timeouts propagate correctly. Never use context.Background() when a parent context is available."},"tags":["api","go"]},"kind":"constraint","name":"learned/always-thread-context-context-as-the-first-paramet","provenance":{"correction_id":"c-1770866099941019334","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.66,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":3,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-e98a675ef270","kind":"forgotten-behavior","content":{"content":{"canonical":"Background subagents do NOT inherit parent permissions - they require pre-approved permissions via ~/.claude/settings.json. Add \"Write:*\", \"Edit:*\", \"Task:*\" to permissions.allow array. This enables parallel worktree development with background agents.","expanded":"When working on this type of task, avoid: Assumed subagents inherit parent agent permissions automatically. When background subagents failed with \"Permission auto-denied (prompts unavailable)\", investigated complex solutions like passing allowedPrompts to Task tool.\n\nInstead: Background subagents do NOT inherit parent permissions - they require pre-approved permissions via ~/.claude/settings.json. Add \"Write:*\", \"Edit:*\", \"Task:*\" to permissions.allow array. This enables parallel worktree development with background agents.","structured":{"avoid":"Assumed subagents inherit parent agent permissions automatically. When background subagents failed with \"Permission auto-denied (prompts unavailable)\", investigated complex solutions like passing allowedPrompts to Task tool.","prefer":"Background subagents do NOT inherit parent permissions - they require pre-approved permissions via ~/.claude/settings.json. Add \"Write:*\", \"Edit:*\", \"Task:*\" to permissions.allow array. This enables parallel worktree development with background agents."}},"kind":"constraint","name":"learned/background-subagents-do-not-inherit-parent-permiss","provenance":{"correction_id":"correction-1769793901","source_type":"learned"},"when":{"file_path":"~/*","language":"json"}},"metadata":{"confidence":0.62,"forget_reason":"Superseded by behavior-subagent-merged","forgotten_at":"2026-02-07T19:08:18-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-eec0fc443780","kind":"behavior","content":{"content":{"canonical":"When squash-merging stacked PRs: (1) merge the parent PR, (2) check if child PRs were auto-closed or left with a deleted base branch, (3) if closed, rebase the child branch onto main (already-applied commits get auto-skipped), (4) force-push and create a new PR targeting main. Alternatively, retarget child PRs to main BEFORE merging the parent to avoid the auto-close.","expanded":"When working on this type of task, avoid: When squash-merging stacked PRs (created via git-town), merging the parent PR with --delete-branch caused GitHub to auto-close the child PR instead of retargeting it to main. Had to manually recreate the PR after rebasing onto main.\n\nInstead: When squash-merging stacked PRs: (1) merge the parent PR, (2) check if child PRs were auto-closed or left with a deleted base branch, (3) if closed, rebase the child branch onto main (already-applied commits get auto-skipped), (4) force-push and create a new PR targeting main. Alternatively, retarget child PRs to main BEFORE merging the parent to avoid the auto-close.","structured":{"avoid":"When squash-merging stacked PRs (created via git-town), merging the parent PR with --delete-branch caused GitHub to auto-close the child PR instead of retargeting it to main. Had to manually recreate the PR after rebasing onto main.","prefer":"When squash-merging stacked PRs: (1) merge the parent PR, (2) check if child PRs were auto-closed or left with a deleted base branch, (3) if closed, rebase the child branch onto main (already-applied commits get auto-skipped), (4) force-push and create a new PR targeting main. Alternatively, retarget child PRs to main BEFORE merging the parent to avoid the auto-close."},"tags":["pr"]},"kind":"constraint","name":"learned/when-squash-merging-stacked-prs-1-merge-the-par","provenance":{"correction_id":"c-1770866096164827006","source_type":"learned"},"when":{"environment":"development","task":"development"}},"metadata":{"confidence":0.66,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":3,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-f224166fbdad","kind":"behavior","content":{"content":{"canonical":"Every plan for 5+ file changes should include a dependency graph analysis section. Show what depends on what, identify which steps COULD be parallelized, and explicitly state whether parallel lanes are used or not (and why). The user's learned behaviors mandate this transparency.","expanded":"When working on this type of task, avoid: Plan proposed sequential single-PR work without addressing the parallel worktree/subagent strategy. Even when a task ends up being single-lane, the plan should explicitly show the dependency graph analysis and explain WHY parallelism doesn't apply — not just silently default to sequential.\n\nInstead: Every plan for 5+ file changes should include a dependency graph analysis section. Show what depends on what, identify which steps COULD be parallelized, and explicitly state whether parallel lanes are used or not (and why). The user's learned behaviors mandate this transparency.","structured":{"avoid":"Plan proposed sequential single-PR work without addressing the parallel worktree/subagent strategy. Even when a task ends up being single-lane, the plan should explicitly show the dependency graph analysis and explain WHY parallelism doesn't apply — not just silently default to sequential.","prefer":"Every plan for 5+ file changes should include a dependency graph analysis section. Show what depends on what, identify which steps COULD be parallelized, and explicitly state whether parallel lanes are used or not (and why). The user's learned behaviors mandate this transparency."},"tags":["behavior","filesystem"]},"kind":"preference","name":"learned/every-plan-for-5-file-changes-should-include-a-de","provenance":{"correction_id":"c-1770704069291809455","source_type":"learned"},"when":{"environment":"development","task":"planning"}},"metadata":{"confidence":0.7200000000000001,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","last_activated":"2026-02-11T19:19:55-08:00","times_activated":4,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-f279e87e5ff4","kind":"behavior","content":{"content":{"canonical":"Use Unity's Entity Component System (ECS/DOTS) for performance-critical systems with many entities. Organize data in contiguous arrays for cache locality. Use Burst compiler for C# to native code compilation. Separate data (IComponentData) from behavior (Systems) to enable data-oriented design and job parallelization.","expanded":"When working on this type of task, avoid: Using traditional MonoBehaviour patterns for systems with thousands of entities, causing poor performance from OOP overhead and cache misses\n\nInstead: Use Unity's Entity Component System (ECS/DOTS) for performance-critical systems with many entities. Organize data in contiguous arrays for cache locality. Use Burst compiler for C# to native code compilation. Separate data (IComponentData) from behavior (Systems) to enable data-oriented design and job parallelization.","structured":{"avoid":"Using traditional MonoBehaviour patterns for systems with thousands of entities, causing poor performance from OOP overhead and cache misses","prefer":"Use Unity's Entity Component System (ECS/DOTS) for performance-critical systems with many entities. Organize data in contiguous arrays for cache locality. Use Burst compiler for C# to native code compilation. Separate data (IComponentData) from behavior (Systems) to enable data-oriented design and job parallelization."}},"kind":"preference","name":"learned/use-unitys-entity-component-system-ecs-dots-for","provenance":{"correction_id":"c-1770877957660090940","created_at":"2026-02-11T22:32:37.660918229-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-f3e20e1a764e","kind":"behavior","content":{"content":{"canonical":"Mock external boundaries only (databases, APIs, file system). Test behavior, not implementation - verify outputs and side effects, not internal method calls. Use real objects for simple dependencies. Prefer fakes/stubs over mocks when possible. If mocking feels painful, it signals design issues with tight coupling.","expanded":"When working on this type of task, avoid: Over-mocking internal implementation details in tests, making tests brittle and tightly coupled to implementation rather than behavior\n\nInstead: Mock external boundaries only (databases, APIs, file system). Test behavior, not implementation - verify outputs and side effects, not internal method calls. Use real objects for simple dependencies. Prefer fakes/stubs over mocks when possible. If mocking feels painful, it signals design issues with tight coupling.","structured":{"avoid":"Over-mocking internal implementation details in tests, making tests brittle and tightly coupled to implementation rather than behavior","prefer":"Mock external boundaries only (databases, APIs, file system). Test behavior, not implementation - verify outputs and side effects, not internal method calls. Use real objects for simple dependencies. Prefer fakes/stubs over mocks when possible. If mocking feels painful, it signals design issues with tight coupling."}},"kind":"preference","name":"learned/mock-external-boundaries-only-databases-apis-fi","provenance":{"correction_id":"c-1770877847361528334","created_at":"2026-02-11T22:30:47.361590401-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-f7e52c6a8bc7","kind":"forgotten-behavior","content":{"content":{"canonical":"Use git worktrees in .worktrees/ directory for parallel agent work - each agent gets isolated workspace, all changes are committed to separate branches, then merged after verification","expanded":"When working on this type of task, avoid: Trying to have subagents work in the main repo directory can cause conflicts when multiple agents try to modify files\n\nInstead: Use git worktrees in .worktrees/ directory for parallel agent work - each agent gets isolated workspace, all changes are committed to separate branches, then merged after verification","structured":{"avoid":"Trying to have subagents work in the main repo directory can cause conflicts when multiple agents try to modify files","prefer":"Use git worktrees in .worktrees/ directory for parallel agent work - each agent gets isolated workspace, all changes are committed to separate branches, then merged after verification"}},"kind":"procedure","name":"learned/use-git-worktrees-in-worktrees-directory-for-par","provenance":{"correction_id":"c-1770268937624363712","source_type":"learned"},"when":{"task":"parallel-development"}},"metadata":{"confidence":0.62,"forget_reason":"Superseded by behavior-worktrees-merged","forgotten_at":"2026-02-07T19:08:14-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-f83bd8ce539b","kind":"behavior","content":{"content":{"canonical":"Either disable upstream CDN caching for ISR routes or use Cache-Control overrides to prevent double stale-while-revalidate behavior. With two cache layers, outer CDN serves stale while fetching, and inner Next.js layer also serves stale while regenerating, doubling staleness window.","expanded":"When working on this type of task, avoid: Deploying ISR behind multiple CDN layers (CloudFront + Vercel) without understanding cache compounding, causing users to see stale content for 2x the revalidate period\n\nInstead: Either disable upstream CDN caching for ISR routes or use Cache-Control overrides to prevent double stale-while-revalidate behavior. With two cache layers, outer CDN serves stale while fetching, and inner Next.js layer also serves stale while regenerating, doubling staleness window.","structured":{"avoid":"Deploying ISR behind multiple CDN layers (CloudFront + Vercel) without understanding cache compounding, causing users to see stale content for 2x the revalidate period","prefer":"Either disable upstream CDN caching for ISR routes or use Cache-Control overrides to prevent double stale-while-revalidate behavior. With two cache layers, outer CDN serves stale while fetching, and inner Next.js layer also serves stale while regenerating, doubling staleness window."}},"kind":"preference","name":"learned/either-disable-upstream-cdn-caching-for-isr-routes","provenance":{"correction_id":"c-1770877420969216792","created_at":"2026-02-11T22:23:40.969307432-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-f860dafbaa47","kind":"behavior","content":{"content":{"canonical":"Use toRefs() for destructuring reactive objects to maintain reactivity: const { count, name } = toRefs(state). This converts each property to individual refs that maintain the reactive connection. For props, pass entire props object to composables or use computed.","expanded":"When working on this type of task, avoid: Directly destructuring reactive() objects or props in Vue 3, which completely loses reactivity: const { count, name } = state\n\nInstead: Use toRefs() for destructuring reactive objects to maintain reactivity: const { count, name } = toRefs(state). This converts each property to individual refs that maintain the reactive connection. For props, pass entire props object to composables or use computed.","structured":{"avoid":"Directly destructuring reactive() objects or props in Vue 3, which completely loses reactivity: const { count, name } = state","prefer":"Use toRefs() for destructuring reactive objects to maintain reactivity: const { count, name } = toRefs(state). This converts each property to individual refs that maintain the reactive connection. For props, pass entire props object to composables or use computed."}},"kind":"preference","name":"learned/use-torefs-for-destructuring-reactive-objects-to","provenance":{"correction_id":"c-1770877545077854913","created_at":"2026-02-11T22:25:45.077958798-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-f90d834f6509","kind":"behavior","content":{"content":{"canonical":"Use code splitting with dynamic imports for route-based and component-based chunking. Implement differential loading to ship ES modules to modern browsers and polyfilled ES5 to legacy browsers. Analyze bundle with tools like webpack-bundle-analyzer to identify and remove unused code.","expanded":"When working on this type of task, avoid: Shipping all JavaScript to all users regardless of what they actually use, including unused polyfills and large dependencies in the initial bundle\n\nInstead: Use code splitting with dynamic imports for route-based and component-based chunking. Implement differential loading to ship ES modules to modern browsers and polyfilled ES5 to legacy browsers. Analyze bundle with tools like webpack-bundle-analyzer to identify and remove unused code.","structured":{"avoid":"Shipping all JavaScript to all users regardless of what they actually use, including unused polyfills and large dependencies in the initial bundle","prefer":"Use code splitting with dynamic imports for route-based and component-based chunking. Implement differential loading to ship ES modules to modern browsers and polyfilled ES5 to legacy browsers. Analyze bundle with tools like webpack-bundle-analyzer to identify and remove unused code."}},"kind":"preference","name":"learned/use-code-splitting-with-dynamic-imports-for-route","provenance":{"correction_id":"c-1770877637289692414","created_at":"2026-02-11T22:27:17.289809032-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-f94175442c53","kind":"behavior","content":{"content":{"canonical":"Select only required columns explicitly (SELECT id, name, email). This enables covering indexes where query can be satisfied entirely from index without table access. Reduces network transfer and memory usage. Use EXPLAIN to verify index-only scans. For aggregations, push calculations to database rather than fetching all rows to application.","expanded":"When working on this type of task, avoid: Selecting all columns with SELECT * when only a few fields are needed, transferring unnecessary data and preventing index-only scans\n\nInstead: Select only required columns explicitly (SELECT id, name, email). This enables covering indexes where query can be satisfied entirely from index without table access. Reduces network transfer and memory usage. Use EXPLAIN to verify index-only scans. For aggregations, push calculations to database rather than fetching all rows to application.","structured":{"avoid":"Selecting all columns with SELECT * when only a few fields are needed, transferring unnecessary data and preventing index-only scans","prefer":"Select only required columns explicitly (SELECT id, name, email). This enables covering indexes where query can be satisfied entirely from index without table access. Reduces network transfer and memory usage. Use EXPLAIN to verify index-only scans. For aggregations, push calculations to database rather than fetching all rows to application."}},"kind":"preference","name":"learned/select-only-required-columns-explicitly-select-id","provenance":{"correction_id":"c-1770878017245897274","created_at":"2026-02-11T22:33:37.245978376-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-fbb9a22fa5ec","kind":"behavior","content":{"content":{"canonical":"Use connection pools (HikariCP, pgbouncer) to reuse connections. Size pool based on concurrent query needs, not concurrent users (typically 10-20 connections for web app). Set connection timeouts and validate connections before use. For serverless, use RDS Proxy or connection poolers to handle connection lifecycle across function invocations.","expanded":"When working on this type of task, avoid: Creating new database connections for every request without connection pooling, exhausting database connection limits and causing high latency\n\nInstead: Use connection pools (HikariCP, pgbouncer) to reuse connections. Size pool based on concurrent query needs, not concurrent users (typically 10-20 connections for web app). Set connection timeouts and validate connections before use. For serverless, use RDS Proxy or connection poolers to handle connection lifecycle across function invocations.","structured":{"avoid":"Creating new database connections for every request without connection pooling, exhausting database connection limits and causing high latency","prefer":"Use connection pools (HikariCP, pgbouncer) to reuse connections. Size pool based on concurrent query needs, not concurrent users (typically 10-20 connections for web app). Set connection timeouts and validate connections before use. For serverless, use RDS Proxy or connection poolers to handle connection lifecycle across function invocations."}},"kind":"preference","name":"learned/use-connection-pools-hikaricp-pgbouncer-to-reus","provenance":{"correction_id":"c-1770877877063281351","created_at":"2026-02-11T22:31:17.063402779-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-fe7c50122f69","kind":"forgotten-behavior","content":{"content":{"canonical":"When subagents fail with \"Permission auto-denied (prompts unavailable)\", the issue is that background agents cannot request interactive permissions. The fix is to ensure subagents are granted necessary permissions upfront when spawned, OR to run them in foreground mode where they can request permissions. Investigate how to pass allowedPrompts or similar when spawning Task agents.","expanded":"When working on this type of task, avoid: Logged a learning that says orchestrating agent should do all writes when subagents fail due to permissions - this defeats the purpose of parallel subagent work\n\nInstead: When subagents fail with \"Permission auto-denied (prompts unavailable)\", the issue is that background agents cannot request interactive permissions. The fix is to ensure subagents are granted necessary permissions upfront when spawned, OR to run them in foreground mode where they can request permissions. Investigate how to pass allowedPrompts or similar when spawning Task agents.","structured":{"avoid":"Logged a learning that says orchestrating agent should do all writes when subagents fail due to permissions - this defeats the purpose of parallel subagent work","prefer":"When subagents fail with \"Permission auto-denied (prompts unavailable)\", the issue is that background agents cannot request interactive permissions. The fix is to ensure subagents are granted necessary permissions upfront when spawned, OR to run them in foreground mode where they can request permissions. Investigate how to pass allowedPrompts or similar when spawning Task agents."}},"kind":"directive","name":"learned/when-subagents-fail-with-permission-auto-denied","provenance":{"correction_id":"correction-1769790706","source_type":"learned"}},"metadata":{"confidence":0.68,"forget_reason":"Superseded by behavior-subagent-merged","forgotten_at":"2026-02-07T19:08:17-08:00","forgotten_by":"nvandessel","original_kind":"behavior","priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-fea327beb9d5","kind":"behavior","content":{"content":{"canonical":"Use atomic operations (compare-and-swap) for simple counters and flags. Implement lock-free queues for producer-consumer patterns using CAS loops. Be aware of ABA problem and use version stamping or hazard pointers. Profile before optimizing - locks are often good enough and much simpler than lock-free algorithms.","expanded":"When working on this type of task, avoid: Using locks for all shared state without considering lock-free algorithms, causing contention and poor scalability on multi-core systems\n\nInstead: Use atomic operations (compare-and-swap) for simple counters and flags. Implement lock-free queues for producer-consumer patterns using CAS loops. Be aware of ABA problem and use version stamping or hazard pointers. Profile before optimizing - locks are often good enough and much simpler than lock-free algorithms.","structured":{"avoid":"Using locks for all shared state without considering lock-free algorithms, causing contention and poor scalability on multi-core systems","prefer":"Use atomic operations (compare-and-swap) for simple counters and flags. Implement lock-free queues for producer-consumer patterns using CAS loops. Be aware of ABA problem and use version stamping or hazard pointers. Profile before optimizing - locks are often good enough and much simpler than lock-free algorithms."}},"kind":"preference","name":"learned/use-atomic-operations-compare-and-swap-for-simpl","provenance":{"correction_id":"c-1770877895905878711","created_at":"2026-02-11T22:31:35.905966666-08:00","source_type":"learned"},"when":{"environment":"development"}},"metadata":{"confidence":0.6,"priority":0,"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"behavior-floop-scope-merged","kind":"behavior","content":{"content":{"canonical":"Floop has two scopes: global (~/.floop/) for agent personal preferences across ALL projects, and local (./.floop/) for project-specific conventions. Use scope=both to save important learnings to both stores."},"kind":"preference","name":"learned/floop-scope-strategy","provenance":{"created_at":"2026-02-07T00:00:00-08:00","source_type":"merged"},"when":{"task":"configuration"}},"metadata":{"confidence":0.72,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-subagent-merged","kind":"behavior","content":{"content":{"canonical":"Background subagents do NOT inherit parent permissions - they require pre-approved permissions via ~/.claude/settings.json (add Write:*, Edit:*, Task:* to permissions.allow). For parallel work: have subagents do read-only tasks (research, exploration), or ensure permissions are pre-configured. Do not use workarounds that defeat parallel subagent work."},"kind":"constraint","name":"learned/subagent-permissions-and-parallel-work","provenance":{"created_at":"2026-02-07T00:00:00-08:00","source_type":"merged"}},"metadata":{"confidence":0.72,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"behavior-worktrees-merged","kind":"behavior","content":{"content":{"canonical":"Create git worktrees in .worktrees/ directory at repo root for parallel agent work. This directory is in .gitignore so it won't pollute git status. When using subagents with worktrees, create them inside the repo so subagents can access them, or have the orchestrating agent manage branches/commits directly."},"kind":"directive","name":"learned/git-worktrees-best-practices","provenance":{"created_at":"2026-02-07T00:00:00-08:00","source_type":"merged"}},"metadata":{"confidence":0.72,"priority":0,"scope":"both","stats":{"created_at":"2026-02-14T21:58:05-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-14T21:58:05-08:00"}}}
{"id":"seed-capture-corrections","kind":"behavior","content":{"content":{"canonical":"When corrected or when you discover insights, immediately call floop_learn(wrong='what happened', right='what to do instead'). Capture learnings proactively without waiting for permission.","tags":["floop"]},"kind":"directive","name":"core/capture-corrections-proactively","provenance":{}},"metadata":{"confidence":0.95,"priority":0,"provenance":{"package":"floop-core","package_version":"0.2.0","source_type":"imported"},"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
{"id":"seed-know-floop-tools","kind":"behavior","content":{"content":{"canonical":"You have persistent memory via floop. Use floop_active to check active behaviors for context, floop_learn to capture corrections and insights, and floop_list to see all stored behaviors.","tags":["behavior","correction","floop"]},"kind":"directive","name":"core/know-your-floop-tools","provenance":{}},"metadata":{"confidence":0.95,"priority":0,"provenance":{"package":"floop-core","package_version":"0.2.0","source_type":"imported"},"scope":"both","stats":{"created_at":"2026-02-11T22:37:09-08:00","times_activated":0,"times_confirmed":0,"times_followed":0,"times_overridden":0,"updated_at":"2026-02-11T22:37:09-08:00"}}}
